{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test preparation\" : \"0\" \"Calendar\":\"1\" \"Comics, graphics & novels\" :\"2\" \"Romance\" :\"3\"\"Computer\" \"&\" \"technology \":\"4\"\")\n",
      "\"Cookbooks, food & wine\":5\"Children books\" :6\"Science fiction and fantasy\":7\n",
      "\"Mystery, thriller & suspense \":8\"Travel \":9\"Law\" :10\"Reference\" :11\n",
      "\"Parenting and relationship \":12\"Engineering & transportation \":13\"Medical books \":14\n",
      "\"Crafts ,hobbies & home \":15\"Religion and spirituality \":16\"Sports and outdoors \":18\"Self  help \":19\n",
      "\"Science & math \":20\"Biography & memoirs\" :21\"Business & money \":22\n",
      "\"Health fitness & dieting \":23\"History \":24\"Teen & young Adults \":2 \"Arts & photography\":26\"Literature & Fiction\":27\n",
      "\"Humor & Entertainment \":28\"Christian books & Bible \":29 \"Political & social sciences\":30\n"
     ]
    }
   ],
   "source": [
    "number_of_crossvalidation_run=10\n",
    "cwd=os.getcwd()\n",
    "visual_m_path = os.path.join(cwd,'data/book-cover_mean_feature_vectors.pkl')\n",
    "textual_m_path = os.path.join(cwd,'data/docs_extracted_features.pkl')\n",
    "models_folder_name = os.path.join(cwd,'models','artificial_net_visual_modality')\n",
    "path_to_save_test_results=os.path.join(models_folder_name, 'test_results.pkl')\n",
    "model_checkpoint_path = os.path.join(cwd,models_folder_name,'gmu.ckpt')\n",
    "models_folder_name_test = os.path.join(cwd,os.pardir,'text_feature_extraction','models','test',str(number_of_crossvalidation_run))\n",
    "path_to_preprocessed_texts_test=os.path.join(models_folder_name_test,'book-cover_test_dataset.pkl')\n",
    "summaries_folder_name=os.path.join(cwd,'summaries','artificial_net_visual_modality')\n",
    "\n",
    "\n",
    "df_visual_m = pd.read_pickle(visual_m_path)\n",
    "df_textual_m = pd.read_pickle(textual_m_path)\n",
    "\n",
    "number_of_recipes=len(df_visual_m)\n",
    "unique_labels=sorted(set(df_visual_m.mean_vector_labels.values))\n",
    "number_of_classes=len(unique_labels)\n",
    "possible_class_indices=list(range(0,number_of_classes))\n",
    "labels2class_indices=dict(zip(unique_labels,possible_class_indices))\n",
    "print(labels2class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_normalized_features_visual=df_visual_m.mean_feature_vectors.tolist()\n",
    "mean_visual=np.mean(not_normalized_features_visual)\n",
    "print(\"Mean value: visual features \", mean_visual)\n",
    "subtracted_mean_visual= (not_normalized_features_visual-mean_visual)\n",
    "max_visual=np.max(subtracted_mean_visual)\n",
    "print(\"Max value in visual feature vectors after subraction of mean value: \", max_visual)\n",
    "normalized_features_visual = (subtracted_mean_visual/max_visual).tolist()\n",
    "df_visual_m.mean_feature_vectors=normalized_features_visual\n",
    "\n",
    "not_normalized_features_textual=df_textual_m.doc_embeddings.tolist()\n",
    "mean_textual=np.mean(not_normalized_features_textual)\n",
    "print(\"Mean value: textual features \", mean_textual)\n",
    "subtracted_mean_textual= (not_normalized_features_textual-mean_textual)\n",
    "max_textual=np.max(subtracted_mean_textual)\n",
    "print(\"Max value in textual feature vectors after subraction of mean value: \", max_textual)\n",
    "normalized_features_textual = (subtracted_mean_textual/max_textual).tolist()\n",
    "df_textual_m.doc_embeddings=normalized_features_textual\n",
    "\n",
    "\n",
    "df_textual_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples:  [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 158, 159, 160, 161, 162, 164, 165, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 234, 235, 236, 237, 238, 240, 241, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 272, 273, 275, 276, 278, 279, 280, 281, 282, 283, 285, 286, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314]\n",
      "test samples:  [0, 4, 8, 20, 22, 28, 38, 39, 47, 54, 60, 67, 73, 85, 93, 99, 105, 111, 123, 125, 133, 138, 147, 152, 157, 163, 166, 172, 177, 182, 190, 197, 202, 210, 217, 229, 233, 239, 242, 244, 246, 256, 267, 274, 277, 284, 287, 293, 302, 303, 313, 315]\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed_texts_test = pd.read_pickle(path_to_preprocessed_texts_test)\n",
    "test_samples=df_preprocessed_texts_test.index.values.tolist()\n",
    "\n",
    "all_samples=set(range(0,number_of_recipes))\n",
    "train_samples=list(all_samples.difference(test_samples))\n",
    "print(\"train samples: \", train_samples)\n",
    "print(\"test samples: \", test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2, 3, 3, 3, 4, 4, 5, 6, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 13, 13, 14, 14, 15, 15, 15, 16, 17, 17, 18, 19, 20, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 25, 25, 26, 26]\n",
      "(264, 202)\n",
      "(264, 27)\n"
     ]
    }
   ],
   "source": [
    "df_visual_m_train=df_visual_m.iloc[train_samples]\n",
    "df_visual_m_test=df_visual_m.iloc[test_samples]\n",
    "df_textual_m_train=df_textual_m.iloc[train_samples]\n",
    "df_textual_m_test=df_textual_m.iloc[test_samples]\n",
    "\n",
    "visual_m_train_inputs=list(df_visual_m_train.mean_feature_vectors.values)\n",
    "visual_m_test_inputs=list(df_visual_m_test.mean_feature_vectors.values)\n",
    "\n",
    "textual_m_train_inputs=list(df_textual_m_train.doc_embeddings.values)\n",
    "textual_m_test_inputs=list(df_textual_m_test.doc_embeddings.values)\n",
    "\n",
    "train_correct_class_ids=[labels2class_indices[l] for l in df_visual_m_train.mean_vector_labels]\n",
    "test_correct_class_ids=[labels2class_indices[l] for l in df_visual_m_test.mean_vector_labels]\n",
    "\n",
    "number_of_training_samples=len(visual_m_train_inputs)\n",
    "number_of_test_samples=len(visual_m_test_inputs)\n",
    "len_of_visual_features_vec=len(visual_m_train_inputs[0])\n",
    "len_of_textual_features_vec=len(textual_m_train_inputs[0])\n",
    "\n",
    "print(test_correct_class_ids)\n",
    "print(np.shape(visual_m_train_inputs))\n",
    "print(np.shape(textual_m_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "learning_rate=0.001\n",
    "hidden_state_dim = 27\n",
    "number_of_training_iterations=50000\n",
    "print_valid_every=20\n",
    "num_repeat_training=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textual_m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24  0  5 20]\n",
      "(4, 202)\n",
      "(4, 27)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "def remove_directory_content(path):\n",
    "    files = glob.glob(path+\"/*\")\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "def create_training_batch():\n",
    "    inputs_visual=[]\n",
    "    inputs_textual=[]\n",
    "    correct_classes=[]\n",
    "    for i in range(batch_size):\n",
    "        train_sample_index=np.random.choice(range(0,number_of_training_samples),1)[0]\n",
    "        inputs_visual.append(visual_m_train_inputs[train_sample_index])\n",
    "        inputs_textual.append(textual_m_train_inputs[train_sample_index])\n",
    "        correct_classes.append(train_correct_class_ids[train_sample_index])\n",
    "    return np.array(inputs_visual),np.array(inputs_textual),np.array(correct_classes)\n",
    "\n",
    "inputs_visual,inputs_textual,correct_classes=create_training_batch()\n",
    "print(np.array(correct_classes))\n",
    "print(np.shape(inputs_visual))\n",
    "print(np.shape(inputs_textual))\n",
    "print(np.shape(correct_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = tf.placeholder(tf.float32, shape=[None,len_of_visual_features_vec],name=\"input_visual_modality\")\n",
    "target = tf.placeholder(tf.int32, shape=[None],name=\"input_correct_labels\")\n",
    "\n",
    "h_v = tf.layers.dense(visual, hidden_state_dim, activation=tf.nn.tanh, name=\"h_v\")\n",
    "\n",
    "logits = tf.layers.dense(h_v, number_of_classes, name=\"h\")\n",
    "scores = tf.nn.sigmoid(logits)\n",
    "\n",
    "multi_class_labels=tf.one_hot(target, depth=number_of_classes)\n",
    "loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(multi_class_labels=multi_class_labels,\n",
    "                                       logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(logits, axis=1), tf.argmax(multi_class_labels,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "    accuracy_ph = tf.placeholder(tf.float32,shape=None,name='accuracy_summary')\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy_ph)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy=0\n",
    "\n",
    "def show_validation_result(session,accuracy_res, loss_res, scores_res):\n",
    "    print(\"Validation: Loss: \", loss_res,\" Accuracy: \", accuracy_res)\n",
    "    global best_accuracy\n",
    "    if accuracy_res > best_accuracy:\n",
    "        best_accuracy=accuracy_res\n",
    "        df_performance=pd.DataFrame(data={'Accuracy':[accuracy_res], 'Loss':[loss_res]})\n",
    "        class_scores=list(np.around(scores_res,4))\n",
    "        predicted_labels = [np.argmax(one_recipe_class_scores) for one_recipe_class_scores in class_scores]\n",
    "        df_scores=pd.DataFrame(data={'Class scores':class_scores,\n",
    "                               'Predicted labels':predicted_labels,\n",
    "                               'Correct labels':test_correct_class_ids,\n",
    "                               })\n",
    "        df_res=pd.concat([df_scores, df_performance], axis=1)\n",
    "        df_res.to_pickle(path_to_save_test_results)\n",
    "        saver.save(session, model_checkpoint_path)\n",
    "        display(df_res)\n",
    "        \n",
    "    \n",
    "\n",
    "def train():\n",
    "        session=tf.InteractiveSession()\n",
    "        summ_writer = tf.summary.FileWriter(summaries_folder_name, session.graph)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    \n",
    "        print(\"Start model training\")\n",
    "    \n",
    "        for train_iter in range(number_of_training_iterations):           \n",
    "            inputs_visual,inputs_textual,correct_classes=create_training_batch()\n",
    "            _, l = session.run([train_op, loss], {visual: inputs_visual,\n",
    "                                           target: correct_classes})\n",
    "            print(train_iter, \": Training: loss: \", l)\n",
    "            summ_loss = session.run(loss_summary, feed_dict={loss_ph:l})\n",
    "            summ_writer.add_summary(summ_loss, train_iter)\n",
    "        \n",
    "            if (train_iter)%print_valid_every==0:                        \n",
    "                accuracy_res, loss_res, scores_res = session.run([accuracy, loss, scores],\n",
    "                                                                          {visual: visual_m_test_inputs,\n",
    "                                                                           target: test_correct_class_ids})        \n",
    "                \n",
    "                show_validation_result(session,accuracy_res, loss_res, scores_res)\n",
    "                summ_accuracy = session.run(accuracy_summary, feed_dict={accuracy_ph:accuracy_res})\n",
    "                summ_writer.add_summary(summ_accuracy, train_iter)\n",
    "                \n",
    "                if accuracy_res==1.0:\n",
    "                    return 0\n",
    "                \n",
    "        session.close()\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "0 : Training: loss:  0.6908216\n",
      "Validation: Loss:  0.6893656  Accuracy:  0.03846154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.5019, 0.5058, 0.4965, 0.5058, 0.496, 0.5028...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.689366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4977, 0.4866, 0.4928, 0.5094, 0.5, 0.5112, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.4943, 0.496, 0.4974, 0.5058, 0.491, 0.4979,...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.486, 0.4971, 0.4981, 0.5208, 0.4837, 0.5069...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.4975, 0.5027, 0.4989, 0.5023, 0.494, 0.4983...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.5004, 0.5145, 0.4991, 0.5061, 0.4928, 0.496...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.5136, 0.5072, 0.4842, 0.502, 0.4907, 0.4928...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.5098, 0.5118, 0.489, 0.4871, 0.4909, 0.4854...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.515, 0.4946, 0.4952, 0.5021, 0.5059, 0.5035...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.4803, 0.4934, 0.5001, 0.5137, 0.4989, 0.508...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.4745, 0.4823, 0.4946, 0.5117, 0.4915, 0.498...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.4944, 0.4822, 0.5003, 0.5104, 0.4844, 0.51,...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.4779, 0.4537, 0.5406, 0.4791, 0.4692, 0.485...</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.4885, 0.5336, 0.4988, 0.4753, 0.5357, 0.487...</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.4938, 0.486, 0.4974, 0.5187, 0.493, 0.5076,...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.493, 0.4845, 0.5133, 0.498, 0.4935, 0.5059,...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.4965, 0.5041, 0.4921, 0.5201, 0.495, 0.503,...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.4903, 0.5097, 0.4891, 0.5095, 0.4942, 0.498...</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.4847, 0.489, 0.4905, 0.5079, 0.4851, 0.5155...</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.4847, 0.4851, 0.4917, 0.515, 0.4833, 0.52, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.4948, 0.5004, 0.5015, 0.5095, 0.485, 0.5168...</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.4893, 0.5051, 0.4989, 0.5021, 0.4849, 0.498...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.4862, 0.5052, 0.4988, 0.5061, 0.4821, 0.505...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.4752, 0.4836, 0.4962, 0.5026, 0.4863, 0.518...</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.5053, 0.5009, 0.4872, 0.515, 0.4948, 0.5106...</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.5017, 0.5064, 0.4854, 0.5195, 0.512, 0.4993...</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.4968, 0.5024, 0.4926, 0.5087, 0.5018, 0.500...</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.5007, 0.4959, 0.4875, 0.5215, 0.5092, 0.508...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.4984, 0.4983, 0.5001, 0.4807, 0.533, 0.5027...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.4907, 0.4792, 0.4924, 0.5146, 0.5147, 0.508...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4194, 0.4278, 0.5594, 0.4937, 0.5057, 0.539...</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.5049, 0.4957, 0.501, 0.5101, 0.4961, 0.4996...</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.5067, 0.4988, 0.4973, 0.5129, 0.4977, 0.505...</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.4978, 0.5018, 0.5025, 0.5041, 0.492, 0.5019...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.4927, 0.4881, 0.5053, 0.5062, 0.4881, 0.506...</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.5102, 0.4998, 0.492, 0.5191, 0.4927, 0.5101...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.503, 0.5004, 0.4888, 0.5112, 0.4827, 0.4979...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.497, 0.4944, 0.4848, 0.5252, 0.4558, 0.491,...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.4877, 0.51, 0.4802, 0.522, 0.4918, 0.4935, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.4903, 0.5103, 0.4867, 0.5119, 0.4831, 0.495...</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.486, 0.516, 0.4889, 0.5105, 0.4909, 0.488, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.5007, 0.5073, 0.4897, 0.5096, 0.4885, 0.495...</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.4929, 0.5061, 0.5042, 0.5002, 0.4749, 0.488...</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.477, 0.5079, 0.5156, 0.4878, 0.4511, 0.4623...</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.4798, 0.4971, 0.5079, 0.4934, 0.4651, 0.457...</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.4938, 0.5082, 0.5054, 0.4948, 0.4719, 0.483...</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.5031, 0.5001, 0.4977, 0.5069, 0.4983, 0.497...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.51, 0.5044, 0.494, 0.5069, 0.4962, 0.508, 0...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.4846, 0.49, 0.5009, 0.5161, 0.4986, 0.506, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.4886, 0.471, 0.5108, 0.4909, 0.4867, 0.5084...</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.4963, 0.4929, 0.5033, 0.5129, 0.51, 0.5098,...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.5019, 0.495, 0.4985, 0.5156, 0.4963, 0.5122...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.5019, 0.5058, 0.4965, 0.5058, 0.496, 0.5028...               0   \n",
       "1   [0.4977, 0.4866, 0.4928, 0.5094, 0.5, 0.5112, ...               0   \n",
       "2   [0.4943, 0.496, 0.4974, 0.5058, 0.491, 0.4979,...               0   \n",
       "3   [0.486, 0.4971, 0.4981, 0.5208, 0.4837, 0.5069...               1   \n",
       "4   [0.4975, 0.5027, 0.4989, 0.5023, 0.494, 0.4983...               1   \n",
       "5   [0.5004, 0.5145, 0.4991, 0.5061, 0.4928, 0.496...               2   \n",
       "6   [0.5136, 0.5072, 0.4842, 0.502, 0.4907, 0.4928...               3   \n",
       "7   [0.5098, 0.5118, 0.489, 0.4871, 0.4909, 0.4854...               3   \n",
       "8   [0.515, 0.4946, 0.4952, 0.5021, 0.5059, 0.5035...               3   \n",
       "9   [0.4803, 0.4934, 0.5001, 0.5137, 0.4989, 0.508...               4   \n",
       "10  [0.4745, 0.4823, 0.4946, 0.5117, 0.4915, 0.498...               4   \n",
       "11  [0.4944, 0.4822, 0.5003, 0.5104, 0.4844, 0.51,...               5   \n",
       "12  [0.4779, 0.4537, 0.5406, 0.4791, 0.4692, 0.485...               6   \n",
       "13  [0.4885, 0.5336, 0.4988, 0.4753, 0.5357, 0.487...               7   \n",
       "14  [0.4938, 0.486, 0.4974, 0.5187, 0.493, 0.5076,...               8   \n",
       "15  [0.493, 0.4845, 0.5133, 0.498, 0.4935, 0.5059,...               8   \n",
       "16  [0.4965, 0.5041, 0.4921, 0.5201, 0.495, 0.503,...               9   \n",
       "17  [0.4903, 0.5097, 0.4891, 0.5095, 0.4942, 0.498...               9   \n",
       "18  [0.4847, 0.489, 0.4905, 0.5079, 0.4851, 0.5155...              10   \n",
       "19  [0.4847, 0.4851, 0.4917, 0.515, 0.4833, 0.52, ...              10   \n",
       "20  [0.4948, 0.5004, 0.5015, 0.5095, 0.485, 0.5168...              11   \n",
       "21  [0.4893, 0.5051, 0.4989, 0.5021, 0.4849, 0.498...              11   \n",
       "22  [0.4862, 0.5052, 0.4988, 0.5061, 0.4821, 0.505...              12   \n",
       "23  [0.4752, 0.4836, 0.4962, 0.5026, 0.4863, 0.518...              13   \n",
       "24  [0.5053, 0.5009, 0.4872, 0.515, 0.4948, 0.5106...              13   \n",
       "25  [0.5017, 0.5064, 0.4854, 0.5195, 0.512, 0.4993...              14   \n",
       "26  [0.4968, 0.5024, 0.4926, 0.5087, 0.5018, 0.500...              14   \n",
       "27  [0.5007, 0.4959, 0.4875, 0.5215, 0.5092, 0.508...              15   \n",
       "28  [0.4984, 0.4983, 0.5001, 0.4807, 0.533, 0.5027...              15   \n",
       "29  [0.4907, 0.4792, 0.4924, 0.5146, 0.5147, 0.508...              15   \n",
       "30  [0.4194, 0.4278, 0.5594, 0.4937, 0.5057, 0.539...              16   \n",
       "31  [0.5049, 0.4957, 0.501, 0.5101, 0.4961, 0.4996...              17   \n",
       "32  [0.5067, 0.4988, 0.4973, 0.5129, 0.4977, 0.505...              17   \n",
       "33  [0.4978, 0.5018, 0.5025, 0.5041, 0.492, 0.5019...              18   \n",
       "34  [0.4927, 0.4881, 0.5053, 0.5062, 0.4881, 0.506...              19   \n",
       "35  [0.5102, 0.4998, 0.492, 0.5191, 0.4927, 0.5101...              20   \n",
       "36  [0.503, 0.5004, 0.4888, 0.5112, 0.4827, 0.4979...              21   \n",
       "37  [0.497, 0.4944, 0.4848, 0.5252, 0.4558, 0.491,...              21   \n",
       "38  [0.4877, 0.51, 0.4802, 0.522, 0.4918, 0.4935, ...              22   \n",
       "39  [0.4903, 0.5103, 0.4867, 0.5119, 0.4831, 0.495...              22   \n",
       "40  [0.486, 0.516, 0.4889, 0.5105, 0.4909, 0.488, ...              22   \n",
       "41  [0.5007, 0.5073, 0.4897, 0.5096, 0.4885, 0.495...              22   \n",
       "42  [0.4929, 0.5061, 0.5042, 0.5002, 0.4749, 0.488...              23   \n",
       "43  [0.477, 0.5079, 0.5156, 0.4878, 0.4511, 0.4623...              23   \n",
       "44  [0.4798, 0.4971, 0.5079, 0.4934, 0.4651, 0.457...              23   \n",
       "45  [0.4938, 0.5082, 0.5054, 0.4948, 0.4719, 0.483...              23   \n",
       "46  [0.5031, 0.5001, 0.4977, 0.5069, 0.4983, 0.497...              24   \n",
       "47  [0.51, 0.5044, 0.494, 0.5069, 0.4962, 0.508, 0...              24   \n",
       "48  [0.4846, 0.49, 0.5009, 0.5161, 0.4986, 0.506, ...              25   \n",
       "49  [0.4886, 0.471, 0.5108, 0.4909, 0.4867, 0.5084...              25   \n",
       "50  [0.4963, 0.4929, 0.5033, 0.5129, 0.51, 0.5098,...              26   \n",
       "51  [0.5019, 0.495, 0.4985, 0.5156, 0.4963, 0.5122...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  9  0.038462  0.689366  \n",
       "1                  5       NaN       NaN  \n",
       "2                 11       NaN       NaN  \n",
       "3                 11       NaN       NaN  \n",
       "4                 18       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                  0       NaN       NaN  \n",
       "7                  1       NaN       NaN  \n",
       "8                  0       NaN       NaN  \n",
       "9                 26       NaN       NaN  \n",
       "10                11       NaN       NaN  \n",
       "11                11       NaN       NaN  \n",
       "12                26       NaN       NaN  \n",
       "13                13       NaN       NaN  \n",
       "14                 3       NaN       NaN  \n",
       "15                11       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                20       NaN       NaN  \n",
       "18                19       NaN       NaN  \n",
       "19                19       NaN       NaN  \n",
       "20                19       NaN       NaN  \n",
       "21                11       NaN       NaN  \n",
       "22                11       NaN       NaN  \n",
       "23                19       NaN       NaN  \n",
       "24                 3       NaN       NaN  \n",
       "25                 3       NaN       NaN  \n",
       "26                17       NaN       NaN  \n",
       "27                 3       NaN       NaN  \n",
       "28                 4       NaN       NaN  \n",
       "29                14       NaN       NaN  \n",
       "30                11       NaN       NaN  \n",
       "31                 9       NaN       NaN  \n",
       "32                 9       NaN       NaN  \n",
       "33                20       NaN       NaN  \n",
       "34                11       NaN       NaN  \n",
       "35                 3       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                 6       NaN       NaN  \n",
       "38                 6       NaN       NaN  \n",
       "39                 3       NaN       NaN  \n",
       "40                11       NaN       NaN  \n",
       "41                 3       NaN       NaN  \n",
       "42                11       NaN       NaN  \n",
       "43                26       NaN       NaN  \n",
       "44                 6       NaN       NaN  \n",
       "45                 6       NaN       NaN  \n",
       "46                 9       NaN       NaN  \n",
       "47                 9       NaN       NaN  \n",
       "48                11       NaN       NaN  \n",
       "49                19       NaN       NaN  \n",
       "50                 3       NaN       NaN  \n",
       "51                26       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Training: loss:  0.6898213\n",
      "2 : Training: loss:  0.68347794\n",
      "3 : Training: loss:  0.6836042\n",
      "4 : Training: loss:  0.68499714\n",
      "5 : Training: loss:  0.6798108\n",
      "6 : Training: loss:  0.6784371\n",
      "7 : Training: loss:  0.68074363\n",
      "8 : Training: loss:  0.6757576\n",
      "9 : Training: loss:  0.671802\n",
      "10 : Training: loss:  0.67115885\n",
      "11 : Training: loss:  0.67638326\n",
      "12 : Training: loss:  0.6653495\n",
      "13 : Training: loss:  0.66949534\n",
      "14 : Training: loss:  0.6709995\n",
      "15 : Training: loss:  0.6651725\n",
      "16 : Training: loss:  0.65573657\n",
      "17 : Training: loss:  0.6561366\n",
      "18 : Training: loss:  0.64592195\n",
      "19 : Training: loss:  0.6469886\n",
      "20 : Training: loss:  0.64787096\n",
      "Validation: Loss:  0.6476101  Accuracy:  0.01923077\n",
      "21 : Training: loss:  0.6604279\n",
      "22 : Training: loss:  0.64713234\n",
      "23 : Training: loss:  0.6511235\n",
      "24 : Training: loss:  0.64668494\n",
      "25 : Training: loss:  0.6263825\n",
      "26 : Training: loss:  0.6353888\n",
      "27 : Training: loss:  0.62696904\n",
      "28 : Training: loss:  0.60639334\n",
      "29 : Training: loss:  0.6292714\n",
      "30 : Training: loss:  0.6367607\n",
      "31 : Training: loss:  0.6351607\n",
      "32 : Training: loss:  0.62047344\n",
      "33 : Training: loss:  0.6296837\n",
      "34 : Training: loss:  0.61305964\n",
      "35 : Training: loss:  0.6232108\n",
      "36 : Training: loss:  0.6243809\n",
      "37 : Training: loss:  0.59316754\n",
      "38 : Training: loss:  0.6089515\n",
      "39 : Training: loss:  0.60518533\n",
      "40 : Training: loss:  0.6008912\n",
      "Validation: Loss:  0.587878  Accuracy:  0.01923077\n",
      "41 : Training: loss:  0.6008712\n",
      "42 : Training: loss:  0.5845261\n",
      "43 : Training: loss:  0.5953487\n",
      "44 : Training: loss:  0.5476704\n",
      "45 : Training: loss:  0.5652848\n",
      "46 : Training: loss:  0.57977915\n",
      "47 : Training: loss:  0.5830165\n",
      "48 : Training: loss:  0.5684552\n",
      "49 : Training: loss:  0.56034416\n",
      "50 : Training: loss:  0.5446983\n",
      "51 : Training: loss:  0.5585587\n",
      "52 : Training: loss:  0.5362338\n",
      "53 : Training: loss:  0.5526358\n",
      "54 : Training: loss:  0.503203\n",
      "55 : Training: loss:  0.49765116\n",
      "56 : Training: loss:  0.49872476\n",
      "57 : Training: loss:  0.489263\n",
      "58 : Training: loss:  0.51661384\n",
      "59 : Training: loss:  0.51946187\n",
      "60 : Training: loss:  0.49704093\n",
      "Validation: Loss:  0.5055605  Accuracy:  0.01923077\n",
      "61 : Training: loss:  0.48178127\n",
      "62 : Training: loss:  0.51611745\n",
      "63 : Training: loss:  0.5183137\n",
      "64 : Training: loss:  0.5032117\n",
      "65 : Training: loss:  0.4650658\n",
      "66 : Training: loss:  0.47344035\n",
      "67 : Training: loss:  0.46038458\n",
      "68 : Training: loss:  0.45635065\n",
      "69 : Training: loss:  0.48694974\n",
      "70 : Training: loss:  0.43468\n",
      "71 : Training: loss:  0.49090752\n",
      "72 : Training: loss:  0.45089403\n",
      "73 : Training: loss:  0.39271262\n",
      "74 : Training: loss:  0.44818425\n",
      "75 : Training: loss:  0.46431106\n",
      "76 : Training: loss:  0.47778696\n",
      "77 : Training: loss:  0.42352536\n",
      "78 : Training: loss:  0.43792006\n",
      "79 : Training: loss:  0.45284674\n",
      "80 : Training: loss:  0.3913776\n",
      "Validation: Loss:  0.41337824  Accuracy:  0.0\n",
      "81 : Training: loss:  0.41290775\n",
      "82 : Training: loss:  0.38916507\n",
      "83 : Training: loss:  0.40427634\n",
      "84 : Training: loss:  0.36880118\n",
      "85 : Training: loss:  0.40206414\n",
      "86 : Training: loss:  0.36733592\n",
      "87 : Training: loss:  0.34604418\n",
      "88 : Training: loss:  0.38775572\n",
      "89 : Training: loss:  0.32548082\n",
      "90 : Training: loss:  0.32259312\n",
      "91 : Training: loss:  0.3602095\n",
      "92 : Training: loss:  0.38004774\n",
      "93 : Training: loss:  0.36297286\n",
      "94 : Training: loss:  0.3955092\n",
      "95 : Training: loss:  0.3539158\n",
      "96 : Training: loss:  0.32315463\n",
      "97 : Training: loss:  0.3617096\n",
      "98 : Training: loss:  0.31921947\n",
      "99 : Training: loss:  0.37584347\n",
      "100 : Training: loss:  0.3006146\n",
      "Validation: Loss:  0.33090994  Accuracy:  0.0\n",
      "101 : Training: loss:  0.34202695\n",
      "102 : Training: loss:  0.34276274\n",
      "103 : Training: loss:  0.32810205\n",
      "104 : Training: loss:  0.35753995\n",
      "105 : Training: loss:  0.33304963\n",
      "106 : Training: loss:  0.31032372\n",
      "107 : Training: loss:  0.35766986\n",
      "108 : Training: loss:  0.29714888\n",
      "109 : Training: loss:  0.32138428\n",
      "110 : Training: loss:  0.2845703\n",
      "111 : Training: loss:  0.25015342\n",
      "112 : Training: loss:  0.24471562\n",
      "113 : Training: loss:  0.29168212\n",
      "114 : Training: loss:  0.2729966\n",
      "115 : Training: loss:  0.2703852\n",
      "116 : Training: loss:  0.27827513\n",
      "117 : Training: loss:  0.2821329\n",
      "118 : Training: loss:  0.30148873\n",
      "119 : Training: loss:  0.27227607\n",
      "120 : Training: loss:  0.28219563\n",
      "Validation: Loss:  0.26930052  Accuracy:  0.0\n",
      "121 : Training: loss:  0.2976505\n",
      "122 : Training: loss:  0.21847628\n",
      "123 : Training: loss:  0.27935982\n",
      "124 : Training: loss:  0.27626672\n",
      "125 : Training: loss:  0.18629348\n",
      "126 : Training: loss:  0.25264\n",
      "127 : Training: loss:  0.2563599\n",
      "128 : Training: loss:  0.26414678\n",
      "129 : Training: loss:  0.22957201\n",
      "130 : Training: loss:  0.2446484\n",
      "131 : Training: loss:  0.2520807\n",
      "132 : Training: loss:  0.20362714\n",
      "133 : Training: loss:  0.23402402\n",
      "134 : Training: loss:  0.17987776\n",
      "135 : Training: loss:  0.22755855\n",
      "136 : Training: loss:  0.20420648\n",
      "137 : Training: loss:  0.25572\n",
      "138 : Training: loss:  0.25378922\n",
      "139 : Training: loss:  0.19585104\n",
      "140 : Training: loss:  0.19422902\n",
      "Validation: Loss:  0.229222  Accuracy:  0.0\n",
      "141 : Training: loss:  0.24375631\n",
      "142 : Training: loss:  0.2375867\n",
      "143 : Training: loss:  0.1652909\n",
      "144 : Training: loss:  0.22669458\n",
      "145 : Training: loss:  0.23971875\n",
      "146 : Training: loss:  0.23090082\n",
      "147 : Training: loss:  0.217162\n",
      "148 : Training: loss:  0.2038746\n",
      "149 : Training: loss:  0.22978646\n",
      "150 : Training: loss:  0.22647817\n",
      "151 : Training: loss:  0.23401375\n",
      "152 : Training: loss:  0.2255873\n",
      "153 : Training: loss:  0.19878092\n",
      "154 : Training: loss:  0.1953044\n",
      "155 : Training: loss:  0.20735854\n",
      "156 : Training: loss:  0.21409465\n",
      "157 : Training: loss:  0.21259125\n",
      "158 : Training: loss:  0.21310423\n",
      "159 : Training: loss:  0.22741888\n",
      "160 : Training: loss:  0.22050132\n",
      "Validation: Loss:  0.2036844  Accuracy:  0.01923077\n",
      "161 : Training: loss:  0.20533617\n",
      "162 : Training: loss:  0.19458307\n",
      "163 : Training: loss:  0.20381749\n",
      "164 : Training: loss:  0.21201293\n",
      "165 : Training: loss:  0.19174278\n",
      "166 : Training: loss:  0.1782315\n",
      "167 : Training: loss:  0.18265711\n",
      "168 : Training: loss:  0.19026473\n",
      "169 : Training: loss:  0.1847572\n",
      "170 : Training: loss:  0.18350802\n",
      "171 : Training: loss:  0.17858993\n",
      "172 : Training: loss:  0.19183157\n",
      "173 : Training: loss:  0.18596394\n",
      "174 : Training: loss:  0.16572154\n",
      "175 : Training: loss:  0.18643786\n",
      "176 : Training: loss:  0.18744057\n",
      "177 : Training: loss:  0.20394547\n",
      "178 : Training: loss:  0.17785943\n",
      "179 : Training: loss:  0.1953274\n",
      "180 : Training: loss:  0.18699026\n",
      "Validation: Loss:  0.18782157  Accuracy:  0.03846154\n",
      "181 : Training: loss:  0.17273167\n",
      "182 : Training: loss:  0.17354597\n",
      "183 : Training: loss:  0.18533346\n",
      "184 : Training: loss:  0.18666294\n",
      "185 : Training: loss:  0.18944941\n",
      "186 : Training: loss:  0.20584525\n",
      "187 : Training: loss:  0.18313092\n",
      "188 : Training: loss:  0.15602884\n",
      "189 : Training: loss:  0.18094537\n",
      "190 : Training: loss:  0.19125645\n",
      "191 : Training: loss:  0.18981723\n",
      "192 : Training: loss:  0.19110674\n",
      "193 : Training: loss:  0.16954105\n",
      "194 : Training: loss:  0.18741216\n",
      "195 : Training: loss:  0.17007448\n",
      "196 : Training: loss:  0.15393716\n",
      "197 : Training: loss:  0.19023116\n",
      "198 : Training: loss:  0.16978838\n",
      "199 : Training: loss:  0.17182106\n",
      "200 : Training: loss:  0.18033676\n",
      "Validation: Loss:  0.1781699  Accuracy:  0.057692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0963, 0.099, 0.0839, 0.1144, 0.1422, 0.1102...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.17817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0713, 0.0713, 0.0607, 0.089, 0.115, 0.0876,...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0856, 0.0888, 0.0758, 0.1048, 0.1276, 0.099...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0461, 0.0495, 0.0401, 0.0634, 0.0798, 0.059...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0727, 0.0761, 0.0632, 0.0892, 0.1139, 0.085...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.1124, 0.1215, 0.101, 0.1365, 0.1586, 0.1274...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0866, 0.0891, 0.0688, 0.1069, 0.1249, 0.096...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.101, 0.1067, 0.0832, 0.1182, 0.1408, 0.1104...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1606, 0.1526, 0.1385, 0.1732, 0.2058, 0.170...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0449, 0.0482, 0.0397, 0.0616, 0.0848, 0.060...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0254, 0.0284, 0.0216, 0.0376, 0.0524, 0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0449, 0.0465, 0.0388, 0.0603, 0.0767, 0.058...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0448, 0.0474, 0.0468, 0.0568, 0.0734, 0.056...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0892, 0.1193, 0.0887, 0.1039, 0.1418, 0.104...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0432, 0.0444, 0.0363, 0.0577, 0.0759, 0.055...</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0563, 0.0569, 0.0509, 0.0672, 0.0909, 0.069...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.1006, 0.1069, 0.0891, 0.1299, 0.1492, 0.118...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.084, 0.0938, 0.0749, 0.108, 0.1305, 0.1008,...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0658, 0.0688, 0.0575, 0.0857, 0.1072, 0.086...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.059, 0.0609, 0.051, 0.0783, 0.0971, 0.079, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0831, 0.0856, 0.0746, 0.1023, 0.1246, 0.103...</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0523, 0.0579, 0.0459, 0.0701, 0.0874, 0.066...</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0651, 0.0691, 0.0581, 0.0816, 0.1033, 0.082...</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0492, 0.0527, 0.0444, 0.0655, 0.0863, 0.068...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0935, 0.0941, 0.0776, 0.1143, 0.1369, 0.109...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0957, 0.0996, 0.0807, 0.1192, 0.1478, 0.108...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0812, 0.0844, 0.0698, 0.1001, 0.1285, 0.095...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0698, 0.0724, 0.0584, 0.0914, 0.1155, 0.085...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0649, 0.0733, 0.0588, 0.0764, 0.1127, 0.078...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0476, 0.0512, 0.0418, 0.0644, 0.0871, 0.061...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0049, 0.0055, 0.0046, 0.0081, 0.0136, 0.008...</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0992, 0.0998, 0.088, 0.1198, 0.1441, 0.1123...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0716, 0.073, 0.0611, 0.0907, 0.1139, 0.0848...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0639, 0.0672, 0.0558, 0.0814, 0.1039, 0.077...</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0706, 0.0714, 0.0635, 0.0868, 0.1092, 0.086...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0587, 0.059, 0.0474, 0.0756, 0.0938, 0.0706...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0758, 0.0791, 0.0635, 0.0974, 0.1126, 0.089...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0323, 0.0369, 0.0267, 0.0492, 0.0526, 0.042...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0941, 0.1037, 0.0792, 0.124, 0.1409, 0.1109...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0814, 0.0892, 0.0693, 0.1059, 0.1238, 0.097...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0748, 0.0853, 0.0659, 0.0954, 0.1167, 0.087...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0827, 0.0874, 0.0695, 0.1047, 0.1234, 0.096...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0493, 0.0558, 0.0438, 0.0667, 0.0797, 0.060...</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0179, 0.023, 0.0164, 0.0281, 0.0334, 0.0243...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0173, 0.0221, 0.0157, 0.0276, 0.0332, 0.023...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0403, 0.0469, 0.0357, 0.0562, 0.0666, 0.050...</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1265, 0.1271, 0.1137, 0.1434, 0.1733, 0.138...</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0992, 0.0985, 0.0838, 0.1146, 0.1414, 0.112...</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.042, 0.0451, 0.0368, 0.0574, 0.0776, 0.0548...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0541, 0.0548, 0.0492, 0.0666, 0.0886, 0.069...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1074, 0.1085, 0.0985, 0.1265, 0.1585, 0.125...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.098, 0.0982, 0.0862, 0.1193, 0.1433, 0.1166...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0963, 0.099, 0.0839, 0.1144, 0.1422, 0.1102...               0   \n",
       "1   [0.0713, 0.0713, 0.0607, 0.089, 0.115, 0.0876,...               0   \n",
       "2   [0.0856, 0.0888, 0.0758, 0.1048, 0.1276, 0.099...               0   \n",
       "3   [0.0461, 0.0495, 0.0401, 0.0634, 0.0798, 0.059...               1   \n",
       "4   [0.0727, 0.0761, 0.0632, 0.0892, 0.1139, 0.085...               1   \n",
       "5   [0.1124, 0.1215, 0.101, 0.1365, 0.1586, 0.1274...               2   \n",
       "6   [0.0866, 0.0891, 0.0688, 0.1069, 0.1249, 0.096...               3   \n",
       "7   [0.101, 0.1067, 0.0832, 0.1182, 0.1408, 0.1104...               3   \n",
       "8   [0.1606, 0.1526, 0.1385, 0.1732, 0.2058, 0.170...               3   \n",
       "9   [0.0449, 0.0482, 0.0397, 0.0616, 0.0848, 0.060...               4   \n",
       "10  [0.0254, 0.0284, 0.0216, 0.0376, 0.0524, 0.035...               4   \n",
       "11  [0.0449, 0.0465, 0.0388, 0.0603, 0.0767, 0.058...               5   \n",
       "12  [0.0448, 0.0474, 0.0468, 0.0568, 0.0734, 0.056...               6   \n",
       "13  [0.0892, 0.1193, 0.0887, 0.1039, 0.1418, 0.104...               7   \n",
       "14  [0.0432, 0.0444, 0.0363, 0.0577, 0.0759, 0.055...               8   \n",
       "15  [0.0563, 0.0569, 0.0509, 0.0672, 0.0909, 0.069...               8   \n",
       "16  [0.1006, 0.1069, 0.0891, 0.1299, 0.1492, 0.118...               9   \n",
       "17  [0.084, 0.0938, 0.0749, 0.108, 0.1305, 0.1008,...               9   \n",
       "18  [0.0658, 0.0688, 0.0575, 0.0857, 0.1072, 0.086...              10   \n",
       "19  [0.059, 0.0609, 0.051, 0.0783, 0.0971, 0.079, ...              10   \n",
       "20  [0.0831, 0.0856, 0.0746, 0.1023, 0.1246, 0.103...              11   \n",
       "21  [0.0523, 0.0579, 0.0459, 0.0701, 0.0874, 0.066...              11   \n",
       "22  [0.0651, 0.0691, 0.0581, 0.0816, 0.1033, 0.082...              12   \n",
       "23  [0.0492, 0.0527, 0.0444, 0.0655, 0.0863, 0.068...              13   \n",
       "24  [0.0935, 0.0941, 0.0776, 0.1143, 0.1369, 0.109...              13   \n",
       "25  [0.0957, 0.0996, 0.0807, 0.1192, 0.1478, 0.108...              14   \n",
       "26  [0.0812, 0.0844, 0.0698, 0.1001, 0.1285, 0.095...              14   \n",
       "27  [0.0698, 0.0724, 0.0584, 0.0914, 0.1155, 0.085...              15   \n",
       "28  [0.0649, 0.0733, 0.0588, 0.0764, 0.1127, 0.078...              15   \n",
       "29  [0.0476, 0.0512, 0.0418, 0.0644, 0.0871, 0.061...              15   \n",
       "30  [0.0049, 0.0055, 0.0046, 0.0081, 0.0136, 0.008...              16   \n",
       "31  [0.0992, 0.0998, 0.088, 0.1198, 0.1441, 0.1123...              17   \n",
       "32  [0.0716, 0.073, 0.0611, 0.0907, 0.1139, 0.0848...              17   \n",
       "33  [0.0639, 0.0672, 0.0558, 0.0814, 0.1039, 0.077...              18   \n",
       "34  [0.0706, 0.0714, 0.0635, 0.0868, 0.1092, 0.086...              19   \n",
       "35  [0.0587, 0.059, 0.0474, 0.0756, 0.0938, 0.0706...              20   \n",
       "36  [0.0758, 0.0791, 0.0635, 0.0974, 0.1126, 0.089...              21   \n",
       "37  [0.0323, 0.0369, 0.0267, 0.0492, 0.0526, 0.042...              21   \n",
       "38  [0.0941, 0.1037, 0.0792, 0.124, 0.1409, 0.1109...              22   \n",
       "39  [0.0814, 0.0892, 0.0693, 0.1059, 0.1238, 0.097...              22   \n",
       "40  [0.0748, 0.0853, 0.0659, 0.0954, 0.1167, 0.087...              22   \n",
       "41  [0.0827, 0.0874, 0.0695, 0.1047, 0.1234, 0.096...              22   \n",
       "42  [0.0493, 0.0558, 0.0438, 0.0667, 0.0797, 0.060...              23   \n",
       "43  [0.0179, 0.023, 0.0164, 0.0281, 0.0334, 0.0243...              23   \n",
       "44  [0.0173, 0.0221, 0.0157, 0.0276, 0.0332, 0.023...              23   \n",
       "45  [0.0403, 0.0469, 0.0357, 0.0562, 0.0666, 0.050...              23   \n",
       "46  [0.1265, 0.1271, 0.1137, 0.1434, 0.1733, 0.138...              24   \n",
       "47  [0.0992, 0.0985, 0.0838, 0.1146, 0.1414, 0.112...              24   \n",
       "48  [0.042, 0.0451, 0.0368, 0.0574, 0.0776, 0.0548...              25   \n",
       "49  [0.0541, 0.0548, 0.0492, 0.0666, 0.0886, 0.069...              25   \n",
       "50  [0.1074, 0.1085, 0.0985, 0.1265, 0.1585, 0.125...              26   \n",
       "51  [0.098, 0.0982, 0.0862, 0.1193, 0.1433, 0.1166...              26   \n",
       "\n",
       "    Predicted labels  Accuracy     Loss  \n",
       "0                  6  0.057692  0.17817  \n",
       "1                  6       NaN      NaN  \n",
       "2                  6       NaN      NaN  \n",
       "3                  6       NaN      NaN  \n",
       "4                  6       NaN      NaN  \n",
       "5                  6       NaN      NaN  \n",
       "6                  6       NaN      NaN  \n",
       "7                  6       NaN      NaN  \n",
       "8                  6       NaN      NaN  \n",
       "9                  6       NaN      NaN  \n",
       "10                 6       NaN      NaN  \n",
       "11                 6       NaN      NaN  \n",
       "12                 8       NaN      NaN  \n",
       "13                 8       NaN      NaN  \n",
       "14                 6       NaN      NaN  \n",
       "15                 8       NaN      NaN  \n",
       "16                 6       NaN      NaN  \n",
       "17                 6       NaN      NaN  \n",
       "18                 6       NaN      NaN  \n",
       "19                 6       NaN      NaN  \n",
       "20                 6       NaN      NaN  \n",
       "21                 6       NaN      NaN  \n",
       "22                 6       NaN      NaN  \n",
       "23                 6       NaN      NaN  \n",
       "24                 6       NaN      NaN  \n",
       "25                 6       NaN      NaN  \n",
       "26                 6       NaN      NaN  \n",
       "27                 6       NaN      NaN  \n",
       "28                 6       NaN      NaN  \n",
       "29                 6       NaN      NaN  \n",
       "30                 8       NaN      NaN  \n",
       "31                 6       NaN      NaN  \n",
       "32                 6       NaN      NaN  \n",
       "33                 6       NaN      NaN  \n",
       "34                 6       NaN      NaN  \n",
       "35                 6       NaN      NaN  \n",
       "36                 6       NaN      NaN  \n",
       "37                 6       NaN      NaN  \n",
       "38                 6       NaN      NaN  \n",
       "39                 6       NaN      NaN  \n",
       "40                 6       NaN      NaN  \n",
       "41                 6       NaN      NaN  \n",
       "42                 6       NaN      NaN  \n",
       "43                23       NaN      NaN  \n",
       "44                23       NaN      NaN  \n",
       "45                 6       NaN      NaN  \n",
       "46                 6       NaN      NaN  \n",
       "47                 6       NaN      NaN  \n",
       "48                 6       NaN      NaN  \n",
       "49                 8       NaN      NaN  \n",
       "50                 8       NaN      NaN  \n",
       "51                 6       NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 : Training: loss:  0.17021154\n",
      "202 : Training: loss:  0.17164552\n",
      "203 : Training: loss:  0.18251391\n",
      "204 : Training: loss:  0.16454652\n",
      "205 : Training: loss:  0.17363514\n",
      "206 : Training: loss:  0.16878739\n",
      "207 : Training: loss:  0.17801389\n",
      "208 : Training: loss:  0.17658672\n",
      "209 : Training: loss:  0.18726829\n",
      "210 : Training: loss:  0.1651471\n",
      "211 : Training: loss:  0.17784224\n",
      "212 : Training: loss:  0.19018745\n",
      "213 : Training: loss:  0.16956072\n",
      "214 : Training: loss:  0.18292426\n",
      "215 : Training: loss:  0.18153802\n",
      "216 : Training: loss:  0.16557309\n",
      "217 : Training: loss:  0.16421461\n",
      "218 : Training: loss:  0.16447726\n",
      "219 : Training: loss:  0.17014113\n",
      "220 : Training: loss:  0.18588293\n",
      "Validation: Loss:  0.17158408  Accuracy:  0.057692308\n",
      "221 : Training: loss:  0.19819134\n",
      "222 : Training: loss:  0.17334273\n",
      "223 : Training: loss:  0.16494499\n",
      "224 : Training: loss:  0.17096475\n",
      "225 : Training: loss:  0.17359728\n",
      "226 : Training: loss:  0.16665821\n",
      "227 : Training: loss:  0.18440226\n",
      "228 : Training: loss:  0.1634749\n",
      "229 : Training: loss:  0.17242701\n",
      "230 : Training: loss:  0.19333583\n",
      "231 : Training: loss:  0.15970859\n",
      "232 : Training: loss:  0.14381172\n",
      "233 : Training: loss:  0.18208472\n",
      "234 : Training: loss:  0.17342198\n",
      "235 : Training: loss:  0.17759915\n",
      "236 : Training: loss:  0.15761667\n",
      "237 : Training: loss:  0.16439027\n",
      "238 : Training: loss:  0.16370717\n",
      "239 : Training: loss:  0.1681984\n",
      "240 : Training: loss:  0.15302865\n",
      "Validation: Loss:  0.16704665  Accuracy:  0.07692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0838, 0.0736, 0.065, 0.0914, 0.107, 0.0831,...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.167047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0628, 0.0523, 0.0469, 0.0705, 0.0848, 0.065...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0759, 0.0669, 0.0597, 0.0847, 0.0964, 0.076...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.041, 0.0356, 0.0306, 0.0495, 0.0569, 0.0435...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0637, 0.0558, 0.0486, 0.0708, 0.0838, 0.063...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0982, 0.0926, 0.0797, 0.1115, 0.1221, 0.098...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0749, 0.0658, 0.0525, 0.0863, 0.0926, 0.072...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0883, 0.0807, 0.0648, 0.0972, 0.1071, 0.084...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1424, 0.1209, 0.1128, 0.1459, 0.1659, 0.137...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.041, 0.0354, 0.0311, 0.0492, 0.062, 0.0449,...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0238, 0.0207, 0.0169, 0.0298, 0.037, 0.0258...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0407, 0.034, 0.0303, 0.0477, 0.0553, 0.0433...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0424, 0.037, 0.0386, 0.0472, 0.0552, 0.0435...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0872, 0.1036, 0.0804, 0.0923, 0.1172, 0.088...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0391, 0.0323, 0.0281, 0.0457, 0.0546, 0.040...</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0519, 0.0429, 0.0408, 0.0542, 0.0676, 0.052...</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0878, 0.0806, 0.0698, 0.1052, 0.1136, 0.090...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0738, 0.0706, 0.0588, 0.0872, 0.0983, 0.076...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0592, 0.0514, 0.0453, 0.069, 0.0801, 0.0653...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0541, 0.0459, 0.0408, 0.0636, 0.0729, 0.060...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0739, 0.0645, 0.0589, 0.0827, 0.0942, 0.078...</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0468, 0.0427, 0.0356, 0.0562, 0.0635, 0.049...</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0581, 0.0511, 0.0454, 0.0653, 0.0765, 0.061...</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0453, 0.0394, 0.0353, 0.0526, 0.0639, 0.051...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0814, 0.0697, 0.06, 0.0914, 0.1027, 0.0827,...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0844, 0.0752, 0.0635, 0.0967, 0.1126, 0.082...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0716, 0.0631, 0.0544, 0.0805, 0.0965, 0.071...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0611, 0.053, 0.0449, 0.0723, 0.0847, 0.0629...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0597, 0.0571, 0.048, 0.063, 0.0857, 0.0608,...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0434, 0.0384, 0.0331, 0.0516, 0.0637, 0.045...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0054, 0.0043, 0.004, 0.0068, 0.0095, 0.0065...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0866, 0.0751, 0.0689, 0.0967, 0.1093, 0.085...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0618, 0.053, 0.0464, 0.0713, 0.0828, 0.0622...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0554, 0.0484, 0.0422, 0.0638, 0.0749, 0.056...</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0633, 0.0536, 0.0501, 0.07, 0.0816, 0.0656,...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0512, 0.0424, 0.0359, 0.0592, 0.0672, 0.051...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0662, 0.0583, 0.0489, 0.0778, 0.0831, 0.066...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0304, 0.028, 0.0217, 0.0404, 0.0381, 0.0318...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0862, 0.0819, 0.0644, 0.105, 0.1112, 0.0884...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0728, 0.0678, 0.0547, 0.0869, 0.0943, 0.075...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0675, 0.0652, 0.0526, 0.0781, 0.0885, 0.067...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0721, 0.0646, 0.0534, 0.084, 0.0918, 0.072,...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0435, 0.0405, 0.0335, 0.0528, 0.0568, 0.044...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0169, 0.0169, 0.013, 0.0227, 0.0229, 0.0176...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0164, 0.0163, 0.0125, 0.0223, 0.0228, 0.016...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0358, 0.034, 0.0273, 0.0446, 0.047, 0.0366,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1133, 0.1001, 0.0929, 0.119, 0.1377, 0.1095...</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0862, 0.0732, 0.0649, 0.0916, 0.1064, 0.085...</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0382, 0.033, 0.0287, 0.0454, 0.056, 0.0404,...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0505, 0.0419, 0.0399, 0.0545, 0.0666, 0.053...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0976, 0.0851, 0.0808, 0.1048, 0.125, 0.0997...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0878, 0.0753, 0.0692, 0.0975, 0.1107, 0.090...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0838, 0.0736, 0.065, 0.0914, 0.107, 0.0831,...               0   \n",
       "1   [0.0628, 0.0523, 0.0469, 0.0705, 0.0848, 0.065...               0   \n",
       "2   [0.0759, 0.0669, 0.0597, 0.0847, 0.0964, 0.076...               0   \n",
       "3   [0.041, 0.0356, 0.0306, 0.0495, 0.0569, 0.0435...               1   \n",
       "4   [0.0637, 0.0558, 0.0486, 0.0708, 0.0838, 0.063...               1   \n",
       "5   [0.0982, 0.0926, 0.0797, 0.1115, 0.1221, 0.098...               2   \n",
       "6   [0.0749, 0.0658, 0.0525, 0.0863, 0.0926, 0.072...               3   \n",
       "7   [0.0883, 0.0807, 0.0648, 0.0972, 0.1071, 0.084...               3   \n",
       "8   [0.1424, 0.1209, 0.1128, 0.1459, 0.1659, 0.137...               3   \n",
       "9   [0.041, 0.0354, 0.0311, 0.0492, 0.062, 0.0449,...               4   \n",
       "10  [0.0238, 0.0207, 0.0169, 0.0298, 0.037, 0.0258...               4   \n",
       "11  [0.0407, 0.034, 0.0303, 0.0477, 0.0553, 0.0433...               5   \n",
       "12  [0.0424, 0.037, 0.0386, 0.0472, 0.0552, 0.0435...               6   \n",
       "13  [0.0872, 0.1036, 0.0804, 0.0923, 0.1172, 0.088...               7   \n",
       "14  [0.0391, 0.0323, 0.0281, 0.0457, 0.0546, 0.040...               8   \n",
       "15  [0.0519, 0.0429, 0.0408, 0.0542, 0.0676, 0.052...               8   \n",
       "16  [0.0878, 0.0806, 0.0698, 0.1052, 0.1136, 0.090...               9   \n",
       "17  [0.0738, 0.0706, 0.0588, 0.0872, 0.0983, 0.076...               9   \n",
       "18  [0.0592, 0.0514, 0.0453, 0.069, 0.0801, 0.0653...              10   \n",
       "19  [0.0541, 0.0459, 0.0408, 0.0636, 0.0729, 0.060...              10   \n",
       "20  [0.0739, 0.0645, 0.0589, 0.0827, 0.0942, 0.078...              11   \n",
       "21  [0.0468, 0.0427, 0.0356, 0.0562, 0.0635, 0.049...              11   \n",
       "22  [0.0581, 0.0511, 0.0454, 0.0653, 0.0765, 0.061...              12   \n",
       "23  [0.0453, 0.0394, 0.0353, 0.0526, 0.0639, 0.051...              13   \n",
       "24  [0.0814, 0.0697, 0.06, 0.0914, 0.1027, 0.0827,...              13   \n",
       "25  [0.0844, 0.0752, 0.0635, 0.0967, 0.1126, 0.082...              14   \n",
       "26  [0.0716, 0.0631, 0.0544, 0.0805, 0.0965, 0.071...              14   \n",
       "27  [0.0611, 0.053, 0.0449, 0.0723, 0.0847, 0.0629...              15   \n",
       "28  [0.0597, 0.0571, 0.048, 0.063, 0.0857, 0.0608,...              15   \n",
       "29  [0.0434, 0.0384, 0.0331, 0.0516, 0.0637, 0.045...              15   \n",
       "30  [0.0054, 0.0043, 0.004, 0.0068, 0.0095, 0.0065...              16   \n",
       "31  [0.0866, 0.0751, 0.0689, 0.0967, 0.1093, 0.085...              17   \n",
       "32  [0.0618, 0.053, 0.0464, 0.0713, 0.0828, 0.0622...              17   \n",
       "33  [0.0554, 0.0484, 0.0422, 0.0638, 0.0749, 0.056...              18   \n",
       "34  [0.0633, 0.0536, 0.0501, 0.07, 0.0816, 0.0656,...              19   \n",
       "35  [0.0512, 0.0424, 0.0359, 0.0592, 0.0672, 0.051...              20   \n",
       "36  [0.0662, 0.0583, 0.0489, 0.0778, 0.0831, 0.066...              21   \n",
       "37  [0.0304, 0.028, 0.0217, 0.0404, 0.0381, 0.0318...              21   \n",
       "38  [0.0862, 0.0819, 0.0644, 0.105, 0.1112, 0.0884...              22   \n",
       "39  [0.0728, 0.0678, 0.0547, 0.0869, 0.0943, 0.075...              22   \n",
       "40  [0.0675, 0.0652, 0.0526, 0.0781, 0.0885, 0.067...              22   \n",
       "41  [0.0721, 0.0646, 0.0534, 0.084, 0.0918, 0.072,...              22   \n",
       "42  [0.0435, 0.0405, 0.0335, 0.0528, 0.0568, 0.044...              23   \n",
       "43  [0.0169, 0.0169, 0.013, 0.0227, 0.0229, 0.0176...              23   \n",
       "44  [0.0164, 0.0163, 0.0125, 0.0223, 0.0228, 0.016...              23   \n",
       "45  [0.0358, 0.034, 0.0273, 0.0446, 0.047, 0.0366,...              23   \n",
       "46  [0.1133, 0.1001, 0.0929, 0.119, 0.1377, 0.1095...              24   \n",
       "47  [0.0862, 0.0732, 0.0649, 0.0916, 0.1064, 0.085...              24   \n",
       "48  [0.0382, 0.033, 0.0287, 0.0454, 0.056, 0.0404,...              25   \n",
       "49  [0.0505, 0.0419, 0.0399, 0.0545, 0.0666, 0.053...              25   \n",
       "50  [0.0976, 0.0851, 0.0808, 0.1048, 0.125, 0.0997...              26   \n",
       "51  [0.0878, 0.0753, 0.0692, 0.0975, 0.1107, 0.090...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  6  0.076923  0.167047  \n",
       "1                  6       NaN       NaN  \n",
       "2                  6       NaN       NaN  \n",
       "3                  6       NaN       NaN  \n",
       "4                  6       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                  6       NaN       NaN  \n",
       "7                  6       NaN       NaN  \n",
       "8                  6       NaN       NaN  \n",
       "9                  6       NaN       NaN  \n",
       "10                 6       NaN       NaN  \n",
       "11                 6       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                 6       NaN       NaN  \n",
       "15                 6       NaN       NaN  \n",
       "16                 6       NaN       NaN  \n",
       "17                 6       NaN       NaN  \n",
       "18                 6       NaN       NaN  \n",
       "19                 6       NaN       NaN  \n",
       "20                 6       NaN       NaN  \n",
       "21                 6       NaN       NaN  \n",
       "22                 6       NaN       NaN  \n",
       "23                 6       NaN       NaN  \n",
       "24                 6       NaN       NaN  \n",
       "25                 6       NaN       NaN  \n",
       "26                 6       NaN       NaN  \n",
       "27                 6       NaN       NaN  \n",
       "28                 6       NaN       NaN  \n",
       "29                 6       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                 6       NaN       NaN  \n",
       "32                 6       NaN       NaN  \n",
       "33                 6       NaN       NaN  \n",
       "34                 6       NaN       NaN  \n",
       "35                 6       NaN       NaN  \n",
       "36                 6       NaN       NaN  \n",
       "37                 6       NaN       NaN  \n",
       "38                 6       NaN       NaN  \n",
       "39                 6       NaN       NaN  \n",
       "40                 6       NaN       NaN  \n",
       "41                 6       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 6       NaN       NaN  \n",
       "47                 6       NaN       NaN  \n",
       "48                 6       NaN       NaN  \n",
       "49                 6       NaN       NaN  \n",
       "50                 6       NaN       NaN  \n",
       "51                 6       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 : Training: loss:  0.16638939\n",
      "242 : Training: loss:  0.1675976\n",
      "243 : Training: loss:  0.15954281\n",
      "244 : Training: loss:  0.17784111\n",
      "245 : Training: loss:  0.16756976\n",
      "246 : Training: loss:  0.15339652\n",
      "247 : Training: loss:  0.17156725\n",
      "248 : Training: loss:  0.15999001\n",
      "249 : Training: loss:  0.16529316\n",
      "250 : Training: loss:  0.16148764\n",
      "251 : Training: loss:  0.16701649\n",
      "252 : Training: loss:  0.1605014\n",
      "253 : Training: loss:  0.16749337\n",
      "254 : Training: loss:  0.16221112\n",
      "255 : Training: loss:  0.16115074\n",
      "256 : Training: loss:  0.16445073\n",
      "257 : Training: loss:  0.1730815\n",
      "258 : Training: loss:  0.16583431\n",
      "259 : Training: loss:  0.17013651\n",
      "260 : Training: loss:  0.18222778\n",
      "Validation: Loss:  0.16408628  Accuracy:  0.07692308\n",
      "261 : Training: loss:  0.15986864\n",
      "262 : Training: loss:  0.15783818\n",
      "263 : Training: loss:  0.15792952\n",
      "264 : Training: loss:  0.15692471\n",
      "265 : Training: loss:  0.15889002\n",
      "266 : Training: loss:  0.166871\n",
      "267 : Training: loss:  0.16272658\n",
      "268 : Training: loss:  0.15875115\n",
      "269 : Training: loss:  0.155641\n",
      "270 : Training: loss:  0.15418488\n",
      "271 : Training: loss:  0.16913469\n",
      "272 : Training: loss:  0.15693322\n",
      "273 : Training: loss:  0.15605469\n",
      "274 : Training: loss:  0.16060181\n",
      "275 : Training: loss:  0.18272197\n",
      "276 : Training: loss:  0.15956345\n",
      "277 : Training: loss:  0.16573322\n",
      "278 : Training: loss:  0.15671873\n",
      "279 : Training: loss:  0.16570927\n",
      "280 : Training: loss:  0.16190013\n",
      "Validation: Loss:  0.16231126  Accuracy:  0.115384616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.089, 0.0641, 0.0552, 0.084, 0.0959, 0.0737,...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.162311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.069, 0.046, 0.0399, 0.0651, 0.0765, 0.0582,...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0823, 0.0594, 0.0515, 0.0792, 0.0874, 0.068...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0464, 0.0312, 0.0259, 0.0459, 0.0512, 0.038...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0695, 0.049, 0.0413, 0.0657, 0.0753, 0.0568...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.1023, 0.0812, 0.0679, 0.1033, 0.1095, 0.087...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0797, 0.0575, 0.0441, 0.0807, 0.0828, 0.064...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0935, 0.0716, 0.0553, 0.0919, 0.0969, 0.076...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1458, 0.107, 0.098, 0.1353, 0.1509, 0.1237,...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0477, 0.0321, 0.0271, 0.0467, 0.0576, 0.041...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0293, 0.0191, 0.0149, 0.029, 0.0346, 0.0241...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0476, 0.0312, 0.0268, 0.0459, 0.0512, 0.040...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0506, 0.0358, 0.036, 0.0473, 0.0528, 0.0419...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0993, 0.105, 0.0792, 0.0944, 0.1136, 0.0876...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.045, 0.0288, 0.0241, 0.043, 0.0499, 0.0368,...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0595, 0.039, 0.0359, 0.0514, 0.0624, 0.0481...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0924, 0.0705, 0.0594, 0.0969, 0.1018, 0.080...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0793, 0.0626, 0.0506, 0.0811, 0.0886, 0.068...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0667, 0.0466, 0.0398, 0.0654, 0.074, 0.0601...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0619, 0.0418, 0.0361, 0.0603, 0.0677, 0.055...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0803, 0.0572, 0.051, 0.0768, 0.0856, 0.0711...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0533, 0.0387, 0.031, 0.0541, 0.0582, 0.0455...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0644, 0.0453, 0.0391, 0.0609, 0.0696, 0.055...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0528, 0.0361, 0.0314, 0.0502, 0.0595, 0.047...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0861, 0.0603, 0.0505, 0.0835, 0.0915, 0.073...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0896, 0.0659, 0.0541, 0.0891, 0.1011, 0.073...</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0782, 0.0562, 0.0471, 0.0754, 0.0879, 0.064...</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0661, 0.046, 0.0377, 0.0661, 0.0754, 0.0554...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0661, 0.0522, 0.0422, 0.0597, 0.0783, 0.055...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0491, 0.0346, 0.0286, 0.0483, 0.0577, 0.041...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.008, 0.0043, 0.0039, 0.0071, 0.0094, 0.0065...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0921, 0.0664, 0.0594, 0.0899, 0.0987, 0.076...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0672, 0.0465, 0.0394, 0.0662, 0.0741, 0.055...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.061, 0.0424, 0.0357, 0.0592, 0.0673, 0.0503...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0703, 0.048, 0.0436, 0.0659, 0.0747, 0.0597...</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0564, 0.0368, 0.0301, 0.0546, 0.0598, 0.045...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0723, 0.052, 0.0421, 0.0735, 0.0751, 0.06, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0377, 0.0278, 0.0205, 0.0421, 0.0364, 0.031...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0934, 0.0742, 0.0564, 0.1001, 0.1032, 0.081...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0794, 0.0606, 0.0473, 0.0822, 0.0865, 0.068...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0742, 0.0586, 0.0458, 0.0738, 0.081, 0.0609...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0773, 0.0565, 0.0451, 0.078, 0.0823, 0.064,...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0494, 0.0367, 0.0291, 0.0508, 0.0516, 0.040...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0216, 0.0167, 0.0121, 0.0239, 0.0216, 0.017...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0212, 0.0163, 0.0118, 0.0236, 0.0215, 0.016...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0415, 0.0312, 0.0239, 0.0437, 0.0427, 0.033...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1206, 0.0912, 0.0834, 0.1121, 0.1275, 0.100...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0912, 0.0637, 0.0551, 0.0841, 0.0952, 0.075...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0444, 0.0296, 0.0248, 0.0428, 0.0514, 0.036...</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0593, 0.0394, 0.0362, 0.0531, 0.063, 0.0506...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1037, 0.0757, 0.0703, 0.097, 0.114, 0.0899,...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0948, 0.0675, 0.0605, 0.0913, 0.1015, 0.082...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.089, 0.0641, 0.0552, 0.084, 0.0959, 0.0737,...               0   \n",
       "1   [0.069, 0.046, 0.0399, 0.0651, 0.0765, 0.0582,...               0   \n",
       "2   [0.0823, 0.0594, 0.0515, 0.0792, 0.0874, 0.068...               0   \n",
       "3   [0.0464, 0.0312, 0.0259, 0.0459, 0.0512, 0.038...               1   \n",
       "4   [0.0695, 0.049, 0.0413, 0.0657, 0.0753, 0.0568...               1   \n",
       "5   [0.1023, 0.0812, 0.0679, 0.1033, 0.1095, 0.087...               2   \n",
       "6   [0.0797, 0.0575, 0.0441, 0.0807, 0.0828, 0.064...               3   \n",
       "7   [0.0935, 0.0716, 0.0553, 0.0919, 0.0969, 0.076...               3   \n",
       "8   [0.1458, 0.107, 0.098, 0.1353, 0.1509, 0.1237,...               3   \n",
       "9   [0.0477, 0.0321, 0.0271, 0.0467, 0.0576, 0.041...               4   \n",
       "10  [0.0293, 0.0191, 0.0149, 0.029, 0.0346, 0.0241...               4   \n",
       "11  [0.0476, 0.0312, 0.0268, 0.0459, 0.0512, 0.040...               5   \n",
       "12  [0.0506, 0.0358, 0.036, 0.0473, 0.0528, 0.0419...               6   \n",
       "13  [0.0993, 0.105, 0.0792, 0.0944, 0.1136, 0.0876...               7   \n",
       "14  [0.045, 0.0288, 0.0241, 0.043, 0.0499, 0.0368,...               8   \n",
       "15  [0.0595, 0.039, 0.0359, 0.0514, 0.0624, 0.0481...               8   \n",
       "16  [0.0924, 0.0705, 0.0594, 0.0969, 0.1018, 0.080...               9   \n",
       "17  [0.0793, 0.0626, 0.0506, 0.0811, 0.0886, 0.068...               9   \n",
       "18  [0.0667, 0.0466, 0.0398, 0.0654, 0.074, 0.0601...              10   \n",
       "19  [0.0619, 0.0418, 0.0361, 0.0603, 0.0677, 0.055...              10   \n",
       "20  [0.0803, 0.0572, 0.051, 0.0768, 0.0856, 0.0711...              11   \n",
       "21  [0.0533, 0.0387, 0.031, 0.0541, 0.0582, 0.0455...              11   \n",
       "22  [0.0644, 0.0453, 0.0391, 0.0609, 0.0696, 0.055...              12   \n",
       "23  [0.0528, 0.0361, 0.0314, 0.0502, 0.0595, 0.047...              13   \n",
       "24  [0.0861, 0.0603, 0.0505, 0.0835, 0.0915, 0.073...              13   \n",
       "25  [0.0896, 0.0659, 0.0541, 0.0891, 0.1011, 0.073...              14   \n",
       "26  [0.0782, 0.0562, 0.0471, 0.0754, 0.0879, 0.064...              14   \n",
       "27  [0.0661, 0.046, 0.0377, 0.0661, 0.0754, 0.0554...              15   \n",
       "28  [0.0661, 0.0522, 0.0422, 0.0597, 0.0783, 0.055...              15   \n",
       "29  [0.0491, 0.0346, 0.0286, 0.0483, 0.0577, 0.041...              15   \n",
       "30  [0.008, 0.0043, 0.0039, 0.0071, 0.0094, 0.0065...              16   \n",
       "31  [0.0921, 0.0664, 0.0594, 0.0899, 0.0987, 0.076...              17   \n",
       "32  [0.0672, 0.0465, 0.0394, 0.0662, 0.0741, 0.055...              17   \n",
       "33  [0.061, 0.0424, 0.0357, 0.0592, 0.0673, 0.0503...              18   \n",
       "34  [0.0703, 0.048, 0.0436, 0.0659, 0.0747, 0.0597...              19   \n",
       "35  [0.0564, 0.0368, 0.0301, 0.0546, 0.0598, 0.045...              20   \n",
       "36  [0.0723, 0.052, 0.0421, 0.0735, 0.0751, 0.06, ...              21   \n",
       "37  [0.0377, 0.0278, 0.0205, 0.0421, 0.0364, 0.031...              21   \n",
       "38  [0.0934, 0.0742, 0.0564, 0.1001, 0.1032, 0.081...              22   \n",
       "39  [0.0794, 0.0606, 0.0473, 0.0822, 0.0865, 0.068...              22   \n",
       "40  [0.0742, 0.0586, 0.0458, 0.0738, 0.081, 0.0609...              22   \n",
       "41  [0.0773, 0.0565, 0.0451, 0.078, 0.0823, 0.064,...              22   \n",
       "42  [0.0494, 0.0367, 0.0291, 0.0508, 0.0516, 0.040...              23   \n",
       "43  [0.0216, 0.0167, 0.0121, 0.0239, 0.0216, 0.017...              23   \n",
       "44  [0.0212, 0.0163, 0.0118, 0.0236, 0.0215, 0.016...              23   \n",
       "45  [0.0415, 0.0312, 0.0239, 0.0437, 0.0427, 0.033...              23   \n",
       "46  [0.1206, 0.0912, 0.0834, 0.1121, 0.1275, 0.100...              24   \n",
       "47  [0.0912, 0.0637, 0.0551, 0.0841, 0.0952, 0.075...              24   \n",
       "48  [0.0444, 0.0296, 0.0248, 0.0428, 0.0514, 0.036...              25   \n",
       "49  [0.0593, 0.0394, 0.0362, 0.0531, 0.063, 0.0506...              25   \n",
       "50  [0.1037, 0.0757, 0.0703, 0.097, 0.114, 0.0899,...              26   \n",
       "51  [0.0948, 0.0675, 0.0605, 0.0913, 0.1015, 0.082...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  4  0.115385  0.162311  \n",
       "1                  4       NaN       NaN  \n",
       "2                 23       NaN       NaN  \n",
       "3                 23       NaN       NaN  \n",
       "4                 23       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                 23       NaN       NaN  \n",
       "7                 23       NaN       NaN  \n",
       "8                  4       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                 4       NaN       NaN  \n",
       "15                23       NaN       NaN  \n",
       "16                23       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                 4       NaN       NaN  \n",
       "19                 4       NaN       NaN  \n",
       "20                23       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                 4       NaN       NaN  \n",
       "23                 4       NaN       NaN  \n",
       "24                23       NaN       NaN  \n",
       "25                 4       NaN       NaN  \n",
       "26                 4       NaN       NaN  \n",
       "27                 4       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                23       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                23       NaN       NaN  \n",
       "34                 4       NaN       NaN  \n",
       "35                23       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                 4       NaN       NaN  \n",
       "39                 4       NaN       NaN  \n",
       "40                23       NaN       NaN  \n",
       "41                23       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                23       NaN       NaN  \n",
       "47                23       NaN       NaN  \n",
       "48                 4       NaN       NaN  \n",
       "49                23       NaN       NaN  \n",
       "50                 4       NaN       NaN  \n",
       "51                23       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 : Training: loss:  0.1439303\n",
      "282 : Training: loss:  0.16201578\n",
      "283 : Training: loss:  0.15929945\n",
      "284 : Training: loss:  0.17268123\n",
      "285 : Training: loss:  0.16088073\n",
      "286 : Training: loss:  0.16189502\n",
      "287 : Training: loss:  0.17684104\n",
      "288 : Training: loss:  0.1635295\n",
      "289 : Training: loss:  0.1512535\n",
      "290 : Training: loss:  0.16801958\n",
      "291 : Training: loss:  0.16303135\n",
      "292 : Training: loss:  0.17253858\n",
      "293 : Training: loss:  0.15841377\n",
      "294 : Training: loss:  0.16934712\n",
      "295 : Training: loss:  0.16666208\n",
      "296 : Training: loss:  0.15455139\n",
      "297 : Training: loss:  0.16158283\n",
      "298 : Training: loss:  0.15475884\n",
      "299 : Training: loss:  0.16742317\n",
      "300 : Training: loss:  0.16043474\n",
      "Validation: Loss:  0.16096394  Accuracy:  0.09615385\n",
      "301 : Training: loss:  0.16542569\n",
      "302 : Training: loss:  0.17252016\n",
      "303 : Training: loss:  0.15872069\n",
      "304 : Training: loss:  0.16998282\n",
      "305 : Training: loss:  0.1569908\n",
      "306 : Training: loss:  0.15763807\n",
      "307 : Training: loss:  0.16953656\n",
      "308 : Training: loss:  0.15190874\n",
      "309 : Training: loss:  0.15971573\n",
      "310 : Training: loss:  0.15712823\n",
      "311 : Training: loss:  0.15881377\n",
      "312 : Training: loss:  0.15291217\n",
      "313 : Training: loss:  0.15852039\n",
      "314 : Training: loss:  0.15707186\n",
      "315 : Training: loss:  0.14747202\n",
      "316 : Training: loss:  0.15899058\n",
      "317 : Training: loss:  0.15646285\n",
      "318 : Training: loss:  0.16032943\n",
      "319 : Training: loss:  0.14237882\n",
      "320 : Training: loss:  0.16791357\n",
      "Validation: Loss:  0.15984415  Accuracy:  0.115384616\n",
      "321 : Training: loss:  0.15930445\n",
      "322 : Training: loss:  0.15374565\n",
      "323 : Training: loss:  0.15696928\n",
      "324 : Training: loss:  0.1579614\n",
      "325 : Training: loss:  0.16229273\n",
      "326 : Training: loss:  0.15798208\n",
      "327 : Training: loss:  0.14950697\n",
      "328 : Training: loss:  0.1606677\n",
      "329 : Training: loss:  0.15727083\n",
      "330 : Training: loss:  0.15290183\n",
      "331 : Training: loss:  0.1536494\n",
      "332 : Training: loss:  0.17714453\n",
      "333 : Training: loss:  0.1710331\n",
      "334 : Training: loss:  0.16157104\n",
      "335 : Training: loss:  0.16049822\n",
      "336 : Training: loss:  0.15920188\n",
      "337 : Training: loss:  0.15584043\n",
      "338 : Training: loss:  0.15834446\n",
      "339 : Training: loss:  0.15511008\n",
      "340 : Training: loss:  0.16588098\n",
      "Validation: Loss:  0.1587405  Accuracy:  0.07692308\n",
      "341 : Training: loss:  0.16249432\n",
      "342 : Training: loss:  0.14752363\n",
      "343 : Training: loss:  0.15689789\n",
      "344 : Training: loss:  0.14679287\n",
      "345 : Training: loss:  0.1688838\n",
      "346 : Training: loss:  0.15687172\n",
      "347 : Training: loss:  0.15280724\n",
      "348 : Training: loss:  0.15397948\n",
      "349 : Training: loss:  0.15898782\n",
      "350 : Training: loss:  0.15624343\n",
      "351 : Training: loss:  0.1643573\n",
      "352 : Training: loss:  0.16560985\n",
      "353 : Training: loss:  0.16361696\n",
      "354 : Training: loss:  0.15353471\n",
      "355 : Training: loss:  0.14938352\n",
      "356 : Training: loss:  0.14818498\n",
      "357 : Training: loss:  0.15818888\n",
      "358 : Training: loss:  0.1567037\n",
      "359 : Training: loss:  0.15760809\n",
      "360 : Training: loss:  0.16114002\n",
      "Validation: Loss:  0.15784179  Accuracy:  0.07692308\n",
      "361 : Training: loss:  0.15956347\n",
      "362 : Training: loss:  0.16347225\n",
      "363 : Training: loss:  0.1436643\n",
      "364 : Training: loss:  0.16392562\n",
      "365 : Training: loss:  0.14890526\n",
      "366 : Training: loss:  0.16243298\n",
      "367 : Training: loss:  0.15833701\n",
      "368 : Training: loss:  0.16335204\n",
      "369 : Training: loss:  0.15702914\n",
      "370 : Training: loss:  0.17386883\n",
      "371 : Training: loss:  0.15060082\n",
      "372 : Training: loss:  0.16672802\n",
      "373 : Training: loss:  0.1414795\n",
      "374 : Training: loss:  0.16404141\n",
      "375 : Training: loss:  0.15687066\n",
      "376 : Training: loss:  0.14643012\n",
      "377 : Training: loss:  0.1605791\n",
      "378 : Training: loss:  0.14353925\n",
      "379 : Training: loss:  0.15928975\n",
      "380 : Training: loss:  0.1625997\n",
      "Validation: Loss:  0.15692894  Accuracy:  0.07692308\n",
      "381 : Training: loss:  0.1639581\n",
      "382 : Training: loss:  0.13670474\n",
      "383 : Training: loss:  0.16453612\n",
      "384 : Training: loss:  0.1665521\n",
      "385 : Training: loss:  0.16103718\n",
      "386 : Training: loss:  0.14375009\n",
      "387 : Training: loss:  0.15709755\n",
      "388 : Training: loss:  0.15528081\n",
      "389 : Training: loss:  0.14778545\n",
      "390 : Training: loss:  0.1617813\n",
      "391 : Training: loss:  0.16586113\n",
      "392 : Training: loss:  0.14934452\n",
      "393 : Training: loss:  0.15464829\n",
      "394 : Training: loss:  0.14481437\n",
      "395 : Training: loss:  0.1627455\n",
      "396 : Training: loss:  0.16178605\n",
      "397 : Training: loss:  0.15450478\n",
      "398 : Training: loss:  0.15322088\n",
      "399 : Training: loss:  0.15570885\n",
      "400 : Training: loss:  0.16493289\n",
      "Validation: Loss:  0.15615414  Accuracy:  0.07692308\n",
      "401 : Training: loss:  0.14148115\n",
      "402 : Training: loss:  0.16439398\n",
      "403 : Training: loss:  0.16019727\n",
      "404 : Training: loss:  0.16274403\n",
      "405 : Training: loss:  0.1583285\n",
      "406 : Training: loss:  0.15145527\n",
      "407 : Training: loss:  0.16513075\n",
      "408 : Training: loss:  0.15486802\n",
      "409 : Training: loss:  0.15915082\n",
      "410 : Training: loss:  0.12949602\n",
      "411 : Training: loss:  0.14547037\n",
      "412 : Training: loss:  0.15990831\n",
      "413 : Training: loss:  0.15022755\n",
      "414 : Training: loss:  0.15055838\n",
      "415 : Training: loss:  0.157582\n",
      "416 : Training: loss:  0.15770514\n",
      "417 : Training: loss:  0.16299492\n",
      "418 : Training: loss:  0.14503935\n",
      "419 : Training: loss:  0.14840758\n",
      "420 : Training: loss:  0.1616257\n",
      "Validation: Loss:  0.1556532  Accuracy:  0.07692308\n",
      "421 : Training: loss:  0.15821181\n",
      "422 : Training: loss:  0.1589509\n",
      "423 : Training: loss:  0.1651193\n",
      "424 : Training: loss:  0.16800097\n",
      "425 : Training: loss:  0.124197155\n",
      "426 : Training: loss:  0.16410454\n",
      "427 : Training: loss:  0.15170595\n",
      "428 : Training: loss:  0.15231606\n",
      "429 : Training: loss:  0.16471834\n",
      "430 : Training: loss:  0.16181238\n",
      "431 : Training: loss:  0.16024284\n",
      "432 : Training: loss:  0.15733062\n",
      "433 : Training: loss:  0.14895327\n",
      "434 : Training: loss:  0.16973098\n",
      "435 : Training: loss:  0.15835427\n",
      "436 : Training: loss:  0.16474897\n",
      "437 : Training: loss:  0.14870721\n",
      "438 : Training: loss:  0.1520503\n",
      "439 : Training: loss:  0.15849437\n",
      "440 : Training: loss:  0.16359253\n",
      "Validation: Loss:  0.15532723  Accuracy:  0.07692308\n",
      "441 : Training: loss:  0.15455684\n",
      "442 : Training: loss:  0.16545762\n",
      "443 : Training: loss:  0.14192125\n",
      "444 : Training: loss:  0.15134828\n",
      "445 : Training: loss:  0.16076602\n",
      "446 : Training: loss:  0.15993446\n",
      "447 : Training: loss:  0.14386752\n",
      "448 : Training: loss:  0.14596434\n",
      "449 : Training: loss:  0.14190494\n",
      "450 : Training: loss:  0.14265853\n",
      "451 : Training: loss:  0.14713256\n",
      "452 : Training: loss:  0.15151154\n",
      "453 : Training: loss:  0.14259921\n",
      "454 : Training: loss:  0.15832952\n",
      "455 : Training: loss:  0.1615608\n",
      "456 : Training: loss:  0.16903685\n",
      "457 : Training: loss:  0.15989198\n",
      "458 : Training: loss:  0.15877649\n",
      "459 : Training: loss:  0.14392826\n",
      "460 : Training: loss:  0.14710759\n",
      "Validation: Loss:  0.15485114  Accuracy:  0.07692308\n",
      "461 : Training: loss:  0.15479542\n",
      "462 : Training: loss:  0.15541565\n",
      "463 : Training: loss:  0.15127079\n",
      "464 : Training: loss:  0.16300583\n",
      "465 : Training: loss:  0.15507616\n",
      "466 : Training: loss:  0.15302944\n",
      "467 : Training: loss:  0.14869934\n",
      "468 : Training: loss:  0.15677862\n",
      "469 : Training: loss:  0.14582863\n",
      "470 : Training: loss:  0.1521857\n",
      "471 : Training: loss:  0.15763251\n",
      "472 : Training: loss:  0.1596777\n",
      "473 : Training: loss:  0.14000538\n",
      "474 : Training: loss:  0.16068904\n",
      "475 : Training: loss:  0.14015566\n",
      "476 : Training: loss:  0.15671329\n",
      "477 : Training: loss:  0.15397836\n",
      "478 : Training: loss:  0.1569318\n",
      "479 : Training: loss:  0.15408948\n",
      "480 : Training: loss:  0.15922947\n",
      "Validation: Loss:  0.15431981  Accuracy:  0.07692308\n",
      "481 : Training: loss:  0.1637104\n",
      "482 : Training: loss:  0.15840536\n",
      "483 : Training: loss:  0.1705218\n",
      "484 : Training: loss:  0.16174002\n",
      "485 : Training: loss:  0.12251906\n",
      "486 : Training: loss:  0.15488654\n",
      "487 : Training: loss:  0.16525117\n",
      "488 : Training: loss:  0.12563315\n",
      "489 : Training: loss:  0.14817469\n",
      "490 : Training: loss:  0.14590698\n",
      "491 : Training: loss:  0.16151796\n",
      "492 : Training: loss:  0.15283033\n",
      "493 : Training: loss:  0.15953916\n",
      "494 : Training: loss:  0.15169637\n",
      "495 : Training: loss:  0.15936892\n",
      "496 : Training: loss:  0.15459016\n",
      "497 : Training: loss:  0.14621182\n",
      "498 : Training: loss:  0.16660765\n",
      "499 : Training: loss:  0.16449486\n",
      "500 : Training: loss:  0.16247208\n",
      "Validation: Loss:  0.15368766  Accuracy:  0.09615385\n",
      "501 : Training: loss:  0.13526487\n",
      "502 : Training: loss:  0.16394551\n",
      "503 : Training: loss:  0.16415179\n",
      "504 : Training: loss:  0.14735189\n",
      "505 : Training: loss:  0.15805694\n",
      "506 : Training: loss:  0.16198018\n",
      "507 : Training: loss:  0.1515772\n",
      "508 : Training: loss:  0.14098412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509 : Training: loss:  0.13925746\n",
      "510 : Training: loss:  0.1585345\n",
      "511 : Training: loss:  0.15233192\n",
      "512 : Training: loss:  0.16817407\n",
      "513 : Training: loss:  0.14401275\n",
      "514 : Training: loss:  0.13533871\n",
      "515 : Training: loss:  0.14712535\n",
      "516 : Training: loss:  0.16224365\n",
      "517 : Training: loss:  0.13484944\n",
      "518 : Training: loss:  0.1590653\n",
      "519 : Training: loss:  0.15157576\n",
      "520 : Training: loss:  0.15578763\n",
      "Validation: Loss:  0.15302725  Accuracy:  0.09615385\n",
      "521 : Training: loss:  0.15483208\n",
      "522 : Training: loss:  0.15592678\n",
      "523 : Training: loss:  0.15859188\n",
      "524 : Training: loss:  0.14398411\n",
      "525 : Training: loss:  0.15374897\n",
      "526 : Training: loss:  0.15181977\n",
      "527 : Training: loss:  0.14314026\n",
      "528 : Training: loss:  0.1512193\n",
      "529 : Training: loss:  0.13387387\n",
      "530 : Training: loss:  0.16111766\n",
      "531 : Training: loss:  0.1561799\n",
      "532 : Training: loss:  0.15235122\n",
      "533 : Training: loss:  0.16591132\n",
      "534 : Training: loss:  0.15450986\n",
      "535 : Training: loss:  0.15298678\n",
      "536 : Training: loss:  0.13936365\n",
      "537 : Training: loss:  0.1539306\n",
      "538 : Training: loss:  0.15721743\n",
      "539 : Training: loss:  0.16057426\n",
      "540 : Training: loss:  0.13100094\n",
      "Validation: Loss:  0.1524115  Accuracy:  0.09615385\n",
      "541 : Training: loss:  0.15417169\n",
      "542 : Training: loss:  0.16568111\n",
      "543 : Training: loss:  0.15576886\n",
      "544 : Training: loss:  0.1619939\n",
      "545 : Training: loss:  0.15235798\n",
      "546 : Training: loss:  0.17062527\n",
      "547 : Training: loss:  0.15751179\n",
      "548 : Training: loss:  0.1575139\n",
      "549 : Training: loss:  0.1554577\n",
      "550 : Training: loss:  0.15686847\n",
      "551 : Training: loss:  0.14781812\n",
      "552 : Training: loss:  0.15784541\n",
      "553 : Training: loss:  0.14796399\n",
      "554 : Training: loss:  0.15429735\n",
      "555 : Training: loss:  0.1682497\n",
      "556 : Training: loss:  0.15114415\n",
      "557 : Training: loss:  0.14809883\n",
      "558 : Training: loss:  0.1622079\n",
      "559 : Training: loss:  0.14715996\n",
      "560 : Training: loss:  0.12566073\n",
      "Validation: Loss:  0.15192708  Accuracy:  0.13461539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0703, 0.0478, 0.0245, 0.0543, 0.0499, 0.039...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.151927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0606, 0.0392, 0.0196, 0.0456, 0.043, 0.0342...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0695, 0.0495, 0.0252, 0.056, 0.048, 0.0401,...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0423, 0.0283, 0.0124, 0.0324, 0.0276, 0.022...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0589, 0.0406, 0.0192, 0.0457, 0.0403, 0.032...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0797, 0.0637, 0.0321, 0.0742, 0.0592, 0.049...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0668, 0.0512, 0.0213, 0.066, 0.046, 0.0391,...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0797, 0.065, 0.0285, 0.0781, 0.057, 0.049, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.117, 0.0834, 0.0508, 0.0991, 0.0905, 0.075,...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0468, 0.0309, 0.0143, 0.0346, 0.0351, 0.026...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0329, 0.0235, 0.0088, 0.0254, 0.0223, 0.017...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0479, 0.0334, 0.0155, 0.037, 0.0306, 0.0278...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0579, 0.053, 0.0288, 0.0476, 0.0369, 0.0349...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1003, 0.141, 0.077, 0.0963, 0.0799, 0.0771,...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0434, 0.0285, 0.0123, 0.0321, 0.0285, 0.023...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0581, 0.0378, 0.0203, 0.0375, 0.036, 0.0313...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0737, 0.056, 0.0283, 0.0682, 0.0558, 0.0451...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0657, 0.0533, 0.0253, 0.0585, 0.0494, 0.039...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0629, 0.0433, 0.0213, 0.048, 0.0445, 0.0384...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0636, 0.0425, 0.0213, 0.0465, 0.0432, 0.038...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0682, 0.0468, 0.0252, 0.0521, 0.0474, 0.041...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0493, 0.0385, 0.016, 0.043, 0.0324, 0.0291,...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0557, 0.0372, 0.0184, 0.0401, 0.0373, 0.031...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0541, 0.037, 0.0183, 0.0377, 0.0369, 0.0326...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0696, 0.0469, 0.023, 0.0566, 0.0486, 0.0402...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0721, 0.051, 0.025, 0.0594, 0.0543, 0.04, 0...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0672, 0.0475, 0.0229, 0.0528, 0.0496, 0.037...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0562, 0.0394, 0.018, 0.0467, 0.0412, 0.0316...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0598, 0.0549, 0.0257, 0.048, 0.0463, 0.0366...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.046, 0.0362, 0.0163, 0.0367, 0.0335, 0.026,...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.02, 0.0128, 0.0071, 0.009, 0.0088, 0.0109, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0743, 0.0543, 0.029, 0.0635, 0.0544, 0.0434...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.053, 0.0378, 0.0174, 0.0446, 0.037, 0.029, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0519, 0.0361, 0.0164, 0.0418, 0.0356, 0.028...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.063, 0.0422, 0.0224, 0.0467, 0.0423, 0.0367...</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0478, 0.0315, 0.0135, 0.0379, 0.0303, 0.024...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.061, 0.0468, 0.0209, 0.0567, 0.0408, 0.0357...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0438, 0.0455, 0.0173, 0.0502, 0.0233, 0.028...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0857, 0.0681, 0.0303, 0.0802, 0.0662, 0.054...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0702, 0.0534, 0.0236, 0.063, 0.0512, 0.0426...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0639, 0.0508, 0.0228, 0.0523, 0.0453, 0.035...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0638, 0.0471, 0.0209, 0.0573, 0.0444, 0.036...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0454, 0.0399, 0.016, 0.0445, 0.0282, 0.0263...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0319, 0.0404, 0.0134, 0.0391, 0.015, 0.0197...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0326, 0.0434, 0.0148, 0.0421, 0.0156, 0.019...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.041, 0.0394, 0.0146, 0.0433, 0.024, 0.024, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1003, 0.0747, 0.0454, 0.0765, 0.075, 0.0603...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0712, 0.0476, 0.0245, 0.0544, 0.0489, 0.04,...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0437, 0.0296, 0.0131, 0.0318, 0.0303, 0.023...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0649, 0.045, 0.0243, 0.0447, 0.0419, 0.0391...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0823, 0.0573, 0.0339, 0.0618, 0.061, 0.0501...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0799, 0.0556, 0.0307, 0.0629, 0.057, 0.0492...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0703, 0.0478, 0.0245, 0.0543, 0.0499, 0.039...               0   \n",
       "1   [0.0606, 0.0392, 0.0196, 0.0456, 0.043, 0.0342...               0   \n",
       "2   [0.0695, 0.0495, 0.0252, 0.056, 0.048, 0.0401,...               0   \n",
       "3   [0.0423, 0.0283, 0.0124, 0.0324, 0.0276, 0.022...               1   \n",
       "4   [0.0589, 0.0406, 0.0192, 0.0457, 0.0403, 0.032...               1   \n",
       "5   [0.0797, 0.0637, 0.0321, 0.0742, 0.0592, 0.049...               2   \n",
       "6   [0.0668, 0.0512, 0.0213, 0.066, 0.046, 0.0391,...               3   \n",
       "7   [0.0797, 0.065, 0.0285, 0.0781, 0.057, 0.049, ...               3   \n",
       "8   [0.117, 0.0834, 0.0508, 0.0991, 0.0905, 0.075,...               3   \n",
       "9   [0.0468, 0.0309, 0.0143, 0.0346, 0.0351, 0.026...               4   \n",
       "10  [0.0329, 0.0235, 0.0088, 0.0254, 0.0223, 0.017...               4   \n",
       "11  [0.0479, 0.0334, 0.0155, 0.037, 0.0306, 0.0278...               5   \n",
       "12  [0.0579, 0.053, 0.0288, 0.0476, 0.0369, 0.0349...               6   \n",
       "13  [0.1003, 0.141, 0.077, 0.0963, 0.0799, 0.0771,...               7   \n",
       "14  [0.0434, 0.0285, 0.0123, 0.0321, 0.0285, 0.023...               8   \n",
       "15  [0.0581, 0.0378, 0.0203, 0.0375, 0.036, 0.0313...               8   \n",
       "16  [0.0737, 0.056, 0.0283, 0.0682, 0.0558, 0.0451...               9   \n",
       "17  [0.0657, 0.0533, 0.0253, 0.0585, 0.0494, 0.039...               9   \n",
       "18  [0.0629, 0.0433, 0.0213, 0.048, 0.0445, 0.0384...              10   \n",
       "19  [0.0636, 0.0425, 0.0213, 0.0465, 0.0432, 0.038...              10   \n",
       "20  [0.0682, 0.0468, 0.0252, 0.0521, 0.0474, 0.041...              11   \n",
       "21  [0.0493, 0.0385, 0.016, 0.043, 0.0324, 0.0291,...              11   \n",
       "22  [0.0557, 0.0372, 0.0184, 0.0401, 0.0373, 0.031...              12   \n",
       "23  [0.0541, 0.037, 0.0183, 0.0377, 0.0369, 0.0326...              13   \n",
       "24  [0.0696, 0.0469, 0.023, 0.0566, 0.0486, 0.0402...              13   \n",
       "25  [0.0721, 0.051, 0.025, 0.0594, 0.0543, 0.04, 0...              14   \n",
       "26  [0.0672, 0.0475, 0.0229, 0.0528, 0.0496, 0.037...              14   \n",
       "27  [0.0562, 0.0394, 0.018, 0.0467, 0.0412, 0.0316...              15   \n",
       "28  [0.0598, 0.0549, 0.0257, 0.048, 0.0463, 0.0366...              15   \n",
       "29  [0.046, 0.0362, 0.0163, 0.0367, 0.0335, 0.026,...              15   \n",
       "30  [0.02, 0.0128, 0.0071, 0.009, 0.0088, 0.0109, ...              16   \n",
       "31  [0.0743, 0.0543, 0.029, 0.0635, 0.0544, 0.0434...              17   \n",
       "32  [0.053, 0.0378, 0.0174, 0.0446, 0.037, 0.029, ...              17   \n",
       "33  [0.0519, 0.0361, 0.0164, 0.0418, 0.0356, 0.028...              18   \n",
       "34  [0.063, 0.0422, 0.0224, 0.0467, 0.0423, 0.0367...              19   \n",
       "35  [0.0478, 0.0315, 0.0135, 0.0379, 0.0303, 0.024...              20   \n",
       "36  [0.061, 0.0468, 0.0209, 0.0567, 0.0408, 0.0357...              21   \n",
       "37  [0.0438, 0.0455, 0.0173, 0.0502, 0.0233, 0.028...              21   \n",
       "38  [0.0857, 0.0681, 0.0303, 0.0802, 0.0662, 0.054...              22   \n",
       "39  [0.0702, 0.0534, 0.0236, 0.063, 0.0512, 0.0426...              22   \n",
       "40  [0.0639, 0.0508, 0.0228, 0.0523, 0.0453, 0.035...              22   \n",
       "41  [0.0638, 0.0471, 0.0209, 0.0573, 0.0444, 0.036...              22   \n",
       "42  [0.0454, 0.0399, 0.016, 0.0445, 0.0282, 0.0263...              23   \n",
       "43  [0.0319, 0.0404, 0.0134, 0.0391, 0.015, 0.0197...              23   \n",
       "44  [0.0326, 0.0434, 0.0148, 0.0421, 0.0156, 0.019...              23   \n",
       "45  [0.041, 0.0394, 0.0146, 0.0433, 0.024, 0.024, ...              23   \n",
       "46  [0.1003, 0.0747, 0.0454, 0.0765, 0.075, 0.0603...              24   \n",
       "47  [0.0712, 0.0476, 0.0245, 0.0544, 0.0489, 0.04,...              24   \n",
       "48  [0.0437, 0.0296, 0.0131, 0.0318, 0.0303, 0.023...              25   \n",
       "49  [0.0649, 0.045, 0.0243, 0.0447, 0.0419, 0.0391...              25   \n",
       "50  [0.0823, 0.0573, 0.0339, 0.0618, 0.061, 0.0501...              26   \n",
       "51  [0.0799, 0.0556, 0.0307, 0.0629, 0.057, 0.0492...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.134615  0.151927  \n",
       "1                  0       NaN       NaN  \n",
       "2                 23       NaN       NaN  \n",
       "3                 23       NaN       NaN  \n",
       "4                 23       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                 23       NaN       NaN  \n",
       "7                 23       NaN       NaN  \n",
       "8                 23       NaN       NaN  \n",
       "9                 23       NaN       NaN  \n",
       "10                23       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                23       NaN       NaN  \n",
       "15                23       NaN       NaN  \n",
       "16                23       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                 0       NaN       NaN  \n",
       "19                 0       NaN       NaN  \n",
       "20                23       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                 0       NaN       NaN  \n",
       "23                23       NaN       NaN  \n",
       "24                23       NaN       NaN  \n",
       "25                23       NaN       NaN  \n",
       "26                23       NaN       NaN  \n",
       "27                23       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                23       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                23       NaN       NaN  \n",
       "34                23       NaN       NaN  \n",
       "35                23       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                23       NaN       NaN  \n",
       "40                23       NaN       NaN  \n",
       "41                23       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                23       NaN       NaN  \n",
       "47                23       NaN       NaN  \n",
       "48                23       NaN       NaN  \n",
       "49                23       NaN       NaN  \n",
       "50                23       NaN       NaN  \n",
       "51                23       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 : Training: loss:  0.1545736\n",
      "562 : Training: loss:  0.14968888\n",
      "563 : Training: loss:  0.13493527\n",
      "564 : Training: loss:  0.14827715\n",
      "565 : Training: loss:  0.14128143\n",
      "566 : Training: loss:  0.16215223\n",
      "567 : Training: loss:  0.13390613\n",
      "568 : Training: loss:  0.15617113\n",
      "569 : Training: loss:  0.15705502\n",
      "570 : Training: loss:  0.13627367\n",
      "571 : Training: loss:  0.12550852\n",
      "572 : Training: loss:  0.15389408\n",
      "573 : Training: loss:  0.1363938\n",
      "574 : Training: loss:  0.14800514\n",
      "575 : Training: loss:  0.16280052\n",
      "576 : Training: loss:  0.17294769\n",
      "577 : Training: loss:  0.13715406\n",
      "578 : Training: loss:  0.1647008\n",
      "579 : Training: loss:  0.1556441\n",
      "580 : Training: loss:  0.15697116\n",
      "Validation: Loss:  0.1514422  Accuracy:  0.09615385\n",
      "581 : Training: loss:  0.15860158\n",
      "582 : Training: loss:  0.13747069\n",
      "583 : Training: loss:  0.14991607\n",
      "584 : Training: loss:  0.15447569\n",
      "585 : Training: loss:  0.16094905\n",
      "586 : Training: loss:  0.15210448\n",
      "587 : Training: loss:  0.16257527\n",
      "588 : Training: loss:  0.16122106\n",
      "589 : Training: loss:  0.13593829\n",
      "590 : Training: loss:  0.14318646\n",
      "591 : Training: loss:  0.15792753\n",
      "592 : Training: loss:  0.116975136\n",
      "593 : Training: loss:  0.15654318\n",
      "594 : Training: loss:  0.15145601\n",
      "595 : Training: loss:  0.14071362\n",
      "596 : Training: loss:  0.15160884\n",
      "597 : Training: loss:  0.14553617\n",
      "598 : Training: loss:  0.13819465\n",
      "599 : Training: loss:  0.16362306\n",
      "600 : Training: loss:  0.15635166\n",
      "Validation: Loss:  0.15092367  Accuracy:  0.09615385\n",
      "601 : Training: loss:  0.15565273\n",
      "602 : Training: loss:  0.13916296\n",
      "603 : Training: loss:  0.16193597\n",
      "604 : Training: loss:  0.15949227\n",
      "605 : Training: loss:  0.16181926\n",
      "606 : Training: loss:  0.16096422\n",
      "607 : Training: loss:  0.13875212\n",
      "608 : Training: loss:  0.14988568\n",
      "609 : Training: loss:  0.153602\n",
      "610 : Training: loss:  0.1505721\n",
      "611 : Training: loss:  0.1469087\n",
      "612 : Training: loss:  0.16535617\n",
      "613 : Training: loss:  0.1300837\n",
      "614 : Training: loss:  0.15910202\n",
      "615 : Training: loss:  0.1594614\n",
      "616 : Training: loss:  0.16221602\n",
      "617 : Training: loss:  0.15623063\n",
      "618 : Training: loss:  0.15656415\n",
      "619 : Training: loss:  0.15029465\n",
      "620 : Training: loss:  0.14807075\n",
      "Validation: Loss:  0.15051164  Accuracy:  0.09615385\n",
      "621 : Training: loss:  0.15549225\n",
      "622 : Training: loss:  0.14647605\n",
      "623 : Training: loss:  0.13638738\n",
      "624 : Training: loss:  0.16277084\n",
      "625 : Training: loss:  0.15765263\n",
      "626 : Training: loss:  0.1612332\n",
      "627 : Training: loss:  0.16031912\n",
      "628 : Training: loss:  0.15310071\n",
      "629 : Training: loss:  0.14202802\n",
      "630 : Training: loss:  0.15618251\n",
      "631 : Training: loss:  0.14073429\n",
      "632 : Training: loss:  0.16133632\n",
      "633 : Training: loss:  0.1516127\n",
      "634 : Training: loss:  0.15763521\n",
      "635 : Training: loss:  0.1595391\n",
      "636 : Training: loss:  0.162848\n",
      "637 : Training: loss:  0.16156632\n",
      "638 : Training: loss:  0.14637841\n",
      "639 : Training: loss:  0.15173338\n",
      "640 : Training: loss:  0.14782587\n",
      "Validation: Loss:  0.14997788  Accuracy:  0.17307693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.066, 0.0461, 0.0263, 0.0474, 0.0496, 0.0376...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.149978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0575, 0.0389, 0.0216, 0.0401, 0.0439, 0.033...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0658, 0.0487, 0.0274, 0.0495, 0.0481, 0.039...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0403, 0.0282, 0.014, 0.028, 0.0285, 0.0226,...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0559, 0.04, 0.0211, 0.0401, 0.0408, 0.0316,...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0735, 0.0611, 0.0336, 0.0654, 0.0576, 0.046...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0625, 0.0508, 0.0228, 0.0602, 0.046, 0.0382...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0752, 0.0649, 0.0304, 0.0725, 0.0574, 0.048...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1104, 0.0807, 0.0533, 0.0897, 0.0896, 0.072...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0448, 0.031, 0.016, 0.0302, 0.0371, 0.0267,...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0318, 0.0248, 0.0103, 0.0224, 0.0241, 0.018...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0458, 0.0343, 0.0176, 0.0327, 0.0316, 0.027...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0538, 0.0551, 0.0313, 0.0419, 0.0363, 0.033...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0903, 0.1412, 0.0813, 0.0851, 0.0747, 0.071...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0415, 0.0288, 0.0139, 0.0278, 0.0297, 0.023...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0561, 0.038, 0.0229, 0.0324, 0.0369, 0.0311...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0685, 0.0541, 0.0301, 0.0601, 0.055, 0.0431...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.061, 0.0521, 0.0272, 0.0515, 0.049, 0.0381,...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.06, 0.0431, 0.0234, 0.0424, 0.0455, 0.038, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0619, 0.0432, 0.0238, 0.0412, 0.0449, 0.038...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0646, 0.0456, 0.0274, 0.0454, 0.0476, 0.040...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0465, 0.0386, 0.0177, 0.038, 0.0326, 0.0287...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0529, 0.0362, 0.0202, 0.0345, 0.0379, 0.031...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0525, 0.0377, 0.0209, 0.033, 0.0385, 0.0328...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0652, 0.0454, 0.0247, 0.0495, 0.0484, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0667, 0.0488, 0.0263, 0.0514, 0.0531, 0.037...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0637, 0.0466, 0.0248, 0.0464, 0.0501, 0.036...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0526, 0.0389, 0.0197, 0.0408, 0.0417, 0.030...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0532, 0.0539, 0.0269, 0.0409, 0.0443, 0.033...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0421, 0.0364, 0.0178, 0.0314, 0.0334, 0.024...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0231, 0.0164, 0.0121, 0.0084, 0.0109, 0.013...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0687, 0.0526, 0.0307, 0.0558, 0.0535, 0.041...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0478, 0.0361, 0.0181, 0.0379, 0.0356, 0.027...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.049, 0.0358, 0.0181, 0.0368, 0.0362, 0.0278...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0608, 0.0422, 0.0251, 0.0413, 0.0435, 0.036...</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0442, 0.0306, 0.0146, 0.0324, 0.03, 0.0239,...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0566, 0.0461, 0.0225, 0.0505, 0.0402, 0.034...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0407, 0.0484, 0.0196, 0.0463, 0.0229, 0.027...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0829, 0.0679, 0.0325, 0.0729, 0.0682, 0.053...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0672, 0.0529, 0.0256, 0.0566, 0.0524, 0.042...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0602, 0.0499, 0.0246, 0.0456, 0.0454, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0601, 0.0462, 0.0225, 0.0511, 0.0445, 0.035...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0418, 0.0403, 0.0175, 0.0398, 0.0277, 0.025...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0306, 0.0462, 0.0164, 0.0383, 0.0151, 0.020...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0315, 0.0501, 0.0184, 0.0418, 0.0159, 0.020...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.038, 0.0409, 0.0164, 0.0395, 0.0237, 0.0236...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0934, 0.0715, 0.0473, 0.0662, 0.0729, 0.057...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.066, 0.0455, 0.026, 0.047, 0.048, 0.0381, 0...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.042, 0.0301, 0.015, 0.0276, 0.0319, 0.0232,...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0636, 0.0468, 0.0277, 0.0401, 0.0438, 0.039...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0738, 0.0531, 0.0342, 0.0513, 0.0577, 0.045...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0735, 0.0531, 0.0319, 0.054, 0.0554, 0.0465...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.066, 0.0461, 0.0263, 0.0474, 0.0496, 0.0376...               0   \n",
       "1   [0.0575, 0.0389, 0.0216, 0.0401, 0.0439, 0.033...               0   \n",
       "2   [0.0658, 0.0487, 0.0274, 0.0495, 0.0481, 0.039...               0   \n",
       "3   [0.0403, 0.0282, 0.014, 0.028, 0.0285, 0.0226,...               1   \n",
       "4   [0.0559, 0.04, 0.0211, 0.0401, 0.0408, 0.0316,...               1   \n",
       "5   [0.0735, 0.0611, 0.0336, 0.0654, 0.0576, 0.046...               2   \n",
       "6   [0.0625, 0.0508, 0.0228, 0.0602, 0.046, 0.0382...               3   \n",
       "7   [0.0752, 0.0649, 0.0304, 0.0725, 0.0574, 0.048...               3   \n",
       "8   [0.1104, 0.0807, 0.0533, 0.0897, 0.0896, 0.072...               3   \n",
       "9   [0.0448, 0.031, 0.016, 0.0302, 0.0371, 0.0267,...               4   \n",
       "10  [0.0318, 0.0248, 0.0103, 0.0224, 0.0241, 0.018...               4   \n",
       "11  [0.0458, 0.0343, 0.0176, 0.0327, 0.0316, 0.027...               5   \n",
       "12  [0.0538, 0.0551, 0.0313, 0.0419, 0.0363, 0.033...               6   \n",
       "13  [0.0903, 0.1412, 0.0813, 0.0851, 0.0747, 0.071...               7   \n",
       "14  [0.0415, 0.0288, 0.0139, 0.0278, 0.0297, 0.023...               8   \n",
       "15  [0.0561, 0.038, 0.0229, 0.0324, 0.0369, 0.0311...               8   \n",
       "16  [0.0685, 0.0541, 0.0301, 0.0601, 0.055, 0.0431...               9   \n",
       "17  [0.061, 0.0521, 0.0272, 0.0515, 0.049, 0.0381,...               9   \n",
       "18  [0.06, 0.0431, 0.0234, 0.0424, 0.0455, 0.038, ...              10   \n",
       "19  [0.0619, 0.0432, 0.0238, 0.0412, 0.0449, 0.038...              10   \n",
       "20  [0.0646, 0.0456, 0.0274, 0.0454, 0.0476, 0.040...              11   \n",
       "21  [0.0465, 0.0386, 0.0177, 0.038, 0.0326, 0.0287...              11   \n",
       "22  [0.0529, 0.0362, 0.0202, 0.0345, 0.0379, 0.031...              12   \n",
       "23  [0.0525, 0.0377, 0.0209, 0.033, 0.0385, 0.0328...              13   \n",
       "24  [0.0652, 0.0454, 0.0247, 0.0495, 0.0484, 0.038...              13   \n",
       "25  [0.0667, 0.0488, 0.0263, 0.0514, 0.0531, 0.037...              14   \n",
       "26  [0.0637, 0.0466, 0.0248, 0.0464, 0.0501, 0.036...              14   \n",
       "27  [0.0526, 0.0389, 0.0197, 0.0408, 0.0417, 0.030...              15   \n",
       "28  [0.0532, 0.0539, 0.0269, 0.0409, 0.0443, 0.033...              15   \n",
       "29  [0.0421, 0.0364, 0.0178, 0.0314, 0.0334, 0.024...              15   \n",
       "30  [0.0231, 0.0164, 0.0121, 0.0084, 0.0109, 0.013...              16   \n",
       "31  [0.0687, 0.0526, 0.0307, 0.0558, 0.0535, 0.041...              17   \n",
       "32  [0.0478, 0.0361, 0.0181, 0.0379, 0.0356, 0.027...              17   \n",
       "33  [0.049, 0.0358, 0.0181, 0.0368, 0.0362, 0.0278...              18   \n",
       "34  [0.0608, 0.0422, 0.0251, 0.0413, 0.0435, 0.036...              19   \n",
       "35  [0.0442, 0.0306, 0.0146, 0.0324, 0.03, 0.0239,...              20   \n",
       "36  [0.0566, 0.0461, 0.0225, 0.0505, 0.0402, 0.034...              21   \n",
       "37  [0.0407, 0.0484, 0.0196, 0.0463, 0.0229, 0.027...              21   \n",
       "38  [0.0829, 0.0679, 0.0325, 0.0729, 0.0682, 0.053...              22   \n",
       "39  [0.0672, 0.0529, 0.0256, 0.0566, 0.0524, 0.042...              22   \n",
       "40  [0.0602, 0.0499, 0.0246, 0.0456, 0.0454, 0.034...              22   \n",
       "41  [0.0601, 0.0462, 0.0225, 0.0511, 0.0445, 0.035...              22   \n",
       "42  [0.0418, 0.0403, 0.0175, 0.0398, 0.0277, 0.025...              23   \n",
       "43  [0.0306, 0.0462, 0.0164, 0.0383, 0.0151, 0.020...              23   \n",
       "44  [0.0315, 0.0501, 0.0184, 0.0418, 0.0159, 0.020...              23   \n",
       "45  [0.038, 0.0409, 0.0164, 0.0395, 0.0237, 0.0236...              23   \n",
       "46  [0.0934, 0.0715, 0.0473, 0.0662, 0.0729, 0.057...              24   \n",
       "47  [0.066, 0.0455, 0.026, 0.047, 0.048, 0.0381, 0...              24   \n",
       "48  [0.042, 0.0301, 0.015, 0.0276, 0.0319, 0.0232,...              25   \n",
       "49  [0.0636, 0.0468, 0.0277, 0.0401, 0.0438, 0.039...              25   \n",
       "50  [0.0738, 0.0531, 0.0342, 0.0513, 0.0577, 0.045...              26   \n",
       "51  [0.0735, 0.0531, 0.0319, 0.054, 0.0554, 0.0465...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.173077  0.149978  \n",
       "1                  0       NaN       NaN  \n",
       "2                 23       NaN       NaN  \n",
       "3                 23       NaN       NaN  \n",
       "4                 23       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                 23       NaN       NaN  \n",
       "7                 23       NaN       NaN  \n",
       "8                 23       NaN       NaN  \n",
       "9                 10       NaN       NaN  \n",
       "10                23       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                23       NaN       NaN  \n",
       "15                23       NaN       NaN  \n",
       "16                23       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                 0       NaN       NaN  \n",
       "20                23       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                 0       NaN       NaN  \n",
       "23                 0       NaN       NaN  \n",
       "24                23       NaN       NaN  \n",
       "25                23       NaN       NaN  \n",
       "26                23       NaN       NaN  \n",
       "27                23       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                23       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                23       NaN       NaN  \n",
       "34                23       NaN       NaN  \n",
       "35                23       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                23       NaN       NaN  \n",
       "41                23       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                23       NaN       NaN  \n",
       "47                23       NaN       NaN  \n",
       "48                23       NaN       NaN  \n",
       "49                23       NaN       NaN  \n",
       "50                23       NaN       NaN  \n",
       "51                23       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641 : Training: loss:  0.14914548\n",
      "642 : Training: loss:  0.16289347\n",
      "643 : Training: loss:  0.15332033\n",
      "644 : Training: loss:  0.16590178\n",
      "645 : Training: loss:  0.1520436\n",
      "646 : Training: loss:  0.1594134\n",
      "647 : Training: loss:  0.15602134\n",
      "648 : Training: loss:  0.16070329\n",
      "649 : Training: loss:  0.1321141\n",
      "650 : Training: loss:  0.1601941\n",
      "651 : Training: loss:  0.15962334\n",
      "652 : Training: loss:  0.15194052\n",
      "653 : Training: loss:  0.14873053\n",
      "654 : Training: loss:  0.15814279\n",
      "655 : Training: loss:  0.16072996\n",
      "656 : Training: loss:  0.16110407\n",
      "657 : Training: loss:  0.16006488\n",
      "658 : Training: loss:  0.15437487\n",
      "659 : Training: loss:  0.15775804\n",
      "660 : Training: loss:  0.14669517\n",
      "Validation: Loss:  0.14949931  Accuracy:  0.23076923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0675, 0.0453, 0.0263, 0.049, 0.0482, 0.0392...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.149499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0589, 0.0382, 0.0216, 0.0416, 0.0427, 0.035...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0671, 0.0477, 0.0273, 0.0511, 0.0467, 0.040...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0415, 0.0278, 0.014, 0.0292, 0.0276, 0.0239...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0574, 0.0394, 0.0212, 0.0418, 0.0397, 0.033...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0745, 0.0598, 0.0334, 0.0674, 0.0558, 0.048...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0641, 0.0504, 0.0228, 0.0633, 0.0449, 0.040...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0769, 0.0644, 0.0304, 0.0759, 0.0562, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1124, 0.0799, 0.0534, 0.0926, 0.0881, 0.075...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0455, 0.0301, 0.0157, 0.0311, 0.0357, 0.027...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0323, 0.024, 0.0101, 0.0232, 0.0231, 0.019,...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0466, 0.0335, 0.0175, 0.0338, 0.0304, 0.029...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0536, 0.0535, 0.0306, 0.0424, 0.0345, 0.034...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0855, 0.1309, 0.0757, 0.0818, 0.0685, 0.068...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0426, 0.0283, 0.0138, 0.0289, 0.0287, 0.024...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0567, 0.0367, 0.0225, 0.033, 0.0352, 0.0321...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0699, 0.0533, 0.03, 0.0624, 0.0537, 0.0449,...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0621, 0.0513, 0.0272, 0.0533, 0.0477, 0.039...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0616, 0.0424, 0.0234, 0.0439, 0.0445, 0.039...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0635, 0.0425, 0.0239, 0.0426, 0.0438, 0.040...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0661, 0.0448, 0.0274, 0.0469, 0.0464, 0.042...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0477, 0.0381, 0.0177, 0.0398, 0.0317, 0.030...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0543, 0.0356, 0.0202, 0.0358, 0.0368, 0.032...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0538, 0.037, 0.0209, 0.0341, 0.0374, 0.0345...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0668, 0.0447, 0.0247, 0.0515, 0.0472, 0.040...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0676, 0.0476, 0.026, 0.0528, 0.0513, 0.0391...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0653, 0.046, 0.0249, 0.0481, 0.0489, 0.0382...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0537, 0.0382, 0.0196, 0.0424, 0.0405, 0.032...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0518, 0.0507, 0.0255, 0.0406, 0.0411, 0.033...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0421, 0.0352, 0.0173, 0.0319, 0.0317, 0.025...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0236, 0.0157, 0.0124, 0.0086, 0.0104, 0.014...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.07, 0.0517, 0.0307, 0.0577, 0.0521, 0.0431,...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0483, 0.0351, 0.0178, 0.039, 0.0341, 0.028,...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0505, 0.0353, 0.0182, 0.0385, 0.0353, 0.029...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0627, 0.0418, 0.0253, 0.0429, 0.0426, 0.038...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.045, 0.0299, 0.0145, 0.0336, 0.0288, 0.025,...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0576, 0.0452, 0.0224, 0.0525, 0.039, 0.0359...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0405, 0.0466, 0.0191, 0.0473, 0.0217, 0.028...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0852, 0.0675, 0.0326, 0.0757, 0.0671, 0.056...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0691, 0.0524, 0.0257, 0.059, 0.0514, 0.0443...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0614, 0.0489, 0.0245, 0.0469, 0.044, 0.0359...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0617, 0.0457, 0.0226, 0.0533, 0.0435, 0.037...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0423, 0.0393, 0.0173, 0.0413, 0.0266, 0.026...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0306, 0.0448, 0.016, 0.0396, 0.0143, 0.0212...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0314, 0.0486, 0.018, 0.0433, 0.015, 0.0213,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0384, 0.04, 0.0163, 0.0411, 0.0227, 0.0246,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0944, 0.0699, 0.047, 0.0672, 0.0708, 0.0585...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0672, 0.0445, 0.0259, 0.0486, 0.0465, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0429, 0.0294, 0.0148, 0.0284, 0.0307, 0.024...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0645, 0.0457, 0.0275, 0.041, 0.0423, 0.0414...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0722, 0.0499, 0.0324, 0.0509, 0.0537, 0.045...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0735, 0.0511, 0.0311, 0.0547, 0.0528, 0.047...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0675, 0.0453, 0.0263, 0.049, 0.0482, 0.0392...               0   \n",
       "1   [0.0589, 0.0382, 0.0216, 0.0416, 0.0427, 0.035...               0   \n",
       "2   [0.0671, 0.0477, 0.0273, 0.0511, 0.0467, 0.040...               0   \n",
       "3   [0.0415, 0.0278, 0.014, 0.0292, 0.0276, 0.0239...               1   \n",
       "4   [0.0574, 0.0394, 0.0212, 0.0418, 0.0397, 0.033...               1   \n",
       "5   [0.0745, 0.0598, 0.0334, 0.0674, 0.0558, 0.048...               2   \n",
       "6   [0.0641, 0.0504, 0.0228, 0.0633, 0.0449, 0.040...               3   \n",
       "7   [0.0769, 0.0644, 0.0304, 0.0759, 0.0562, 0.050...               3   \n",
       "8   [0.1124, 0.0799, 0.0534, 0.0926, 0.0881, 0.075...               3   \n",
       "9   [0.0455, 0.0301, 0.0157, 0.0311, 0.0357, 0.027...               4   \n",
       "10  [0.0323, 0.024, 0.0101, 0.0232, 0.0231, 0.019,...               4   \n",
       "11  [0.0466, 0.0335, 0.0175, 0.0338, 0.0304, 0.029...               5   \n",
       "12  [0.0536, 0.0535, 0.0306, 0.0424, 0.0345, 0.034...               6   \n",
       "13  [0.0855, 0.1309, 0.0757, 0.0818, 0.0685, 0.068...               7   \n",
       "14  [0.0426, 0.0283, 0.0138, 0.0289, 0.0287, 0.024...               8   \n",
       "15  [0.0567, 0.0367, 0.0225, 0.033, 0.0352, 0.0321...               8   \n",
       "16  [0.0699, 0.0533, 0.03, 0.0624, 0.0537, 0.0449,...               9   \n",
       "17  [0.0621, 0.0513, 0.0272, 0.0533, 0.0477, 0.039...               9   \n",
       "18  [0.0616, 0.0424, 0.0234, 0.0439, 0.0445, 0.039...              10   \n",
       "19  [0.0635, 0.0425, 0.0239, 0.0426, 0.0438, 0.040...              10   \n",
       "20  [0.0661, 0.0448, 0.0274, 0.0469, 0.0464, 0.042...              11   \n",
       "21  [0.0477, 0.0381, 0.0177, 0.0398, 0.0317, 0.030...              11   \n",
       "22  [0.0543, 0.0356, 0.0202, 0.0358, 0.0368, 0.032...              12   \n",
       "23  [0.0538, 0.037, 0.0209, 0.0341, 0.0374, 0.0345...              13   \n",
       "24  [0.0668, 0.0447, 0.0247, 0.0515, 0.0472, 0.040...              13   \n",
       "25  [0.0676, 0.0476, 0.026, 0.0528, 0.0513, 0.0391...              14   \n",
       "26  [0.0653, 0.046, 0.0249, 0.0481, 0.0489, 0.0382...              14   \n",
       "27  [0.0537, 0.0382, 0.0196, 0.0424, 0.0405, 0.032...              15   \n",
       "28  [0.0518, 0.0507, 0.0255, 0.0406, 0.0411, 0.033...              15   \n",
       "29  [0.0421, 0.0352, 0.0173, 0.0319, 0.0317, 0.025...              15   \n",
       "30  [0.0236, 0.0157, 0.0124, 0.0086, 0.0104, 0.014...              16   \n",
       "31  [0.07, 0.0517, 0.0307, 0.0577, 0.0521, 0.0431,...              17   \n",
       "32  [0.0483, 0.0351, 0.0178, 0.039, 0.0341, 0.028,...              17   \n",
       "33  [0.0505, 0.0353, 0.0182, 0.0385, 0.0353, 0.029...              18   \n",
       "34  [0.0627, 0.0418, 0.0253, 0.0429, 0.0426, 0.038...              19   \n",
       "35  [0.045, 0.0299, 0.0145, 0.0336, 0.0288, 0.025,...              20   \n",
       "36  [0.0576, 0.0452, 0.0224, 0.0525, 0.039, 0.0359...              21   \n",
       "37  [0.0405, 0.0466, 0.0191, 0.0473, 0.0217, 0.028...              21   \n",
       "38  [0.0852, 0.0675, 0.0326, 0.0757, 0.0671, 0.056...              22   \n",
       "39  [0.0691, 0.0524, 0.0257, 0.059, 0.0514, 0.0443...              22   \n",
       "40  [0.0614, 0.0489, 0.0245, 0.0469, 0.044, 0.0359...              22   \n",
       "41  [0.0617, 0.0457, 0.0226, 0.0533, 0.0435, 0.037...              22   \n",
       "42  [0.0423, 0.0393, 0.0173, 0.0413, 0.0266, 0.026...              23   \n",
       "43  [0.0306, 0.0448, 0.016, 0.0396, 0.0143, 0.0212...              23   \n",
       "44  [0.0314, 0.0486, 0.018, 0.0433, 0.015, 0.0213,...              23   \n",
       "45  [0.0384, 0.04, 0.0163, 0.0411, 0.0227, 0.0246,...              23   \n",
       "46  [0.0944, 0.0699, 0.047, 0.0672, 0.0708, 0.0585...              24   \n",
       "47  [0.0672, 0.0445, 0.0259, 0.0486, 0.0465, 0.039...              24   \n",
       "48  [0.0429, 0.0294, 0.0148, 0.0284, 0.0307, 0.024...              25   \n",
       "49  [0.0645, 0.0457, 0.0275, 0.041, 0.0423, 0.0414...              25   \n",
       "50  [0.0722, 0.0499, 0.0324, 0.0509, 0.0537, 0.045...              26   \n",
       "51  [0.0735, 0.0511, 0.0311, 0.0547, 0.0528, 0.047...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.230769  0.149499  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                 23       NaN       NaN  \n",
       "7                 23       NaN       NaN  \n",
       "8                  0       NaN       NaN  \n",
       "9                 10       NaN       NaN  \n",
       "10                23       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                 0       NaN       NaN  \n",
       "15                 0       NaN       NaN  \n",
       "16                23       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                 0       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                 0       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                 0       NaN       NaN  \n",
       "25                 0       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                23       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                23       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                 0       NaN       NaN  \n",
       "35                23       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 0       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                 0       NaN       NaN  \n",
       "49                 0       NaN       NaN  \n",
       "50                 0       NaN       NaN  \n",
       "51                 0       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 : Training: loss:  0.1452418\n",
      "662 : Training: loss:  0.1561561\n",
      "663 : Training: loss:  0.15905473\n",
      "664 : Training: loss:  0.15460873\n",
      "665 : Training: loss:  0.1476275\n",
      "666 : Training: loss:  0.1533534\n",
      "667 : Training: loss:  0.1484256\n",
      "668 : Training: loss:  0.15187076\n",
      "669 : Training: loss:  0.15361692\n",
      "670 : Training: loss:  0.14681868\n",
      "671 : Training: loss:  0.15879786\n",
      "672 : Training: loss:  0.15355161\n",
      "673 : Training: loss:  0.15415464\n",
      "674 : Training: loss:  0.15695626\n",
      "675 : Training: loss:  0.16593648\n",
      "676 : Training: loss:  0.16339624\n",
      "677 : Training: loss:  0.14753217\n",
      "678 : Training: loss:  0.13296178\n",
      "679 : Training: loss:  0.15980138\n",
      "680 : Training: loss:  0.1599012\n",
      "Validation: Loss:  0.14915496  Accuracy:  0.23076923\n",
      "681 : Training: loss:  0.1651871\n",
      "682 : Training: loss:  0.15857002\n",
      "683 : Training: loss:  0.14875916\n",
      "684 : Training: loss:  0.14482018\n",
      "685 : Training: loss:  0.15307795\n",
      "686 : Training: loss:  0.15853235\n",
      "687 : Training: loss:  0.15910684\n",
      "688 : Training: loss:  0.16041383\n",
      "689 : Training: loss:  0.15525983\n",
      "690 : Training: loss:  0.1231495\n",
      "691 : Training: loss:  0.15643235\n",
      "692 : Training: loss:  0.14676449\n",
      "693 : Training: loss:  0.14849578\n",
      "694 : Training: loss:  0.13154675\n",
      "695 : Training: loss:  0.146833\n",
      "696 : Training: loss:  0.1575432\n",
      "697 : Training: loss:  0.15762332\n",
      "698 : Training: loss:  0.15768784\n",
      "699 : Training: loss:  0.15008496\n",
      "700 : Training: loss:  0.11299534\n",
      "Validation: Loss:  0.14880352  Accuracy:  0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0621, 0.046, 0.026, 0.0482, 0.0466, 0.0414,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.148804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0545, 0.0395, 0.0217, 0.0413, 0.042, 0.0382...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0622, 0.049, 0.0273, 0.0507, 0.0455, 0.0436...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0385, 0.0291, 0.0142, 0.0292, 0.0274, 0.026...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0532, 0.0406, 0.0213, 0.0416, 0.0389, 0.035...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0681, 0.0602, 0.0326, 0.0664, 0.0535, 0.050...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0592, 0.0519, 0.0227, 0.0637, 0.0438, 0.042...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0714, 0.0661, 0.0302, 0.0764, 0.0549, 0.053...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.105, 0.0804, 0.0527, 0.0915, 0.0856, 0.078,...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0426, 0.0319, 0.0161, 0.0312, 0.0361, 0.031...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0304, 0.0263, 0.0105, 0.0237, 0.0237, 0.022...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0436, 0.0354, 0.0179, 0.034, 0.0303, 0.0325...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0505, 0.0578, 0.0315, 0.043, 0.0345, 0.0376...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0772, 0.1311, 0.0733, 0.0785, 0.0653, 0.069...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0396, 0.0299, 0.014, 0.0288, 0.0286, 0.027,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0533, 0.0385, 0.023, 0.0327, 0.0348, 0.0352...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0641, 0.054, 0.0296, 0.0616, 0.0519, 0.0472...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0569, 0.0523, 0.0269, 0.0526, 0.0463, 0.041...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0577, 0.0441, 0.0236, 0.044, 0.0442, 0.0435...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0604, 0.0449, 0.0245, 0.043, 0.0441, 0.0451...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0615, 0.0459, 0.0275, 0.0464, 0.0454, 0.045...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0441, 0.0395, 0.0177, 0.0398, 0.0311, 0.032...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0506, 0.0367, 0.0203, 0.0355, 0.0362, 0.035...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0508, 0.0391, 0.0214, 0.0342, 0.0376, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0616, 0.0455, 0.0245, 0.0509, 0.0458, 0.043...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.062, 0.0484, 0.0256, 0.0519, 0.0495, 0.0411...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0609, 0.0475, 0.0249, 0.0478, 0.0482, 0.040...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0494, 0.0396, 0.0196, 0.042, 0.0397, 0.0346...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0465, 0.0522, 0.0251, 0.0396, 0.0397, 0.035...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0382, 0.037, 0.0174, 0.0314, 0.031, 0.0275,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0251, 0.0196, 0.0159, 0.0094, 0.0124, 0.019...</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0643, 0.0526, 0.0303, 0.057, 0.0504, 0.0453...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0436, 0.0358, 0.0175, 0.0382, 0.0327, 0.029...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0467, 0.0366, 0.0183, 0.0384, 0.0346, 0.031...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0586, 0.0433, 0.0256, 0.0427, 0.042, 0.042,...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0412, 0.0309, 0.0144, 0.0333, 0.028, 0.027,...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0529, 0.0464, 0.0222, 0.0523, 0.0379, 0.038...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0372, 0.0493, 0.0193, 0.0479, 0.0213, 0.031...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0808, 0.0701, 0.0328, 0.0762, 0.0669, 0.060...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0647, 0.0541, 0.0257, 0.0592, 0.0508, 0.047...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0566, 0.0501, 0.0243, 0.0462, 0.0428, 0.038...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.057, 0.0468, 0.0225, 0.0531, 0.0424, 0.0399...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0385, 0.0408, 0.0173, 0.0414, 0.0258, 0.028...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0281, 0.048, 0.0164, 0.0409, 0.014, 0.0236,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0288, 0.052, 0.0185, 0.0446, 0.0148, 0.0235...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0349, 0.0417, 0.0163, 0.0414, 0.022, 0.0266...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0881, 0.0708, 0.0464, 0.0657, 0.0688, 0.061...</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0616, 0.045, 0.0255, 0.0476, 0.0447, 0.0417...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0403, 0.0314, 0.0153, 0.0285, 0.031, 0.0272...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0618, 0.0489, 0.0286, 0.0413, 0.0427, 0.046...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0659, 0.0503, 0.0317, 0.049, 0.0513, 0.0478...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0679, 0.0519, 0.0307, 0.0536, 0.0511, 0.050...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0621, 0.046, 0.026, 0.0482, 0.0466, 0.0414,...               0   \n",
       "1   [0.0545, 0.0395, 0.0217, 0.0413, 0.042, 0.0382...               0   \n",
       "2   [0.0622, 0.049, 0.0273, 0.0507, 0.0455, 0.0436...               0   \n",
       "3   [0.0385, 0.0291, 0.0142, 0.0292, 0.0274, 0.026...               1   \n",
       "4   [0.0532, 0.0406, 0.0213, 0.0416, 0.0389, 0.035...               1   \n",
       "5   [0.0681, 0.0602, 0.0326, 0.0664, 0.0535, 0.050...               2   \n",
       "6   [0.0592, 0.0519, 0.0227, 0.0637, 0.0438, 0.042...               3   \n",
       "7   [0.0714, 0.0661, 0.0302, 0.0764, 0.0549, 0.053...               3   \n",
       "8   [0.105, 0.0804, 0.0527, 0.0915, 0.0856, 0.078,...               3   \n",
       "9   [0.0426, 0.0319, 0.0161, 0.0312, 0.0361, 0.031...               4   \n",
       "10  [0.0304, 0.0263, 0.0105, 0.0237, 0.0237, 0.022...               4   \n",
       "11  [0.0436, 0.0354, 0.0179, 0.034, 0.0303, 0.0325...               5   \n",
       "12  [0.0505, 0.0578, 0.0315, 0.043, 0.0345, 0.0376...               6   \n",
       "13  [0.0772, 0.1311, 0.0733, 0.0785, 0.0653, 0.069...               7   \n",
       "14  [0.0396, 0.0299, 0.014, 0.0288, 0.0286, 0.027,...               8   \n",
       "15  [0.0533, 0.0385, 0.023, 0.0327, 0.0348, 0.0352...               8   \n",
       "16  [0.0641, 0.054, 0.0296, 0.0616, 0.0519, 0.0472...               9   \n",
       "17  [0.0569, 0.0523, 0.0269, 0.0526, 0.0463, 0.041...               9   \n",
       "18  [0.0577, 0.0441, 0.0236, 0.044, 0.0442, 0.0435...              10   \n",
       "19  [0.0604, 0.0449, 0.0245, 0.043, 0.0441, 0.0451...              10   \n",
       "20  [0.0615, 0.0459, 0.0275, 0.0464, 0.0454, 0.045...              11   \n",
       "21  [0.0441, 0.0395, 0.0177, 0.0398, 0.0311, 0.032...              11   \n",
       "22  [0.0506, 0.0367, 0.0203, 0.0355, 0.0362, 0.035...              12   \n",
       "23  [0.0508, 0.0391, 0.0214, 0.0342, 0.0376, 0.038...              13   \n",
       "24  [0.0616, 0.0455, 0.0245, 0.0509, 0.0458, 0.043...              13   \n",
       "25  [0.062, 0.0484, 0.0256, 0.0519, 0.0495, 0.0411...              14   \n",
       "26  [0.0609, 0.0475, 0.0249, 0.0478, 0.0482, 0.040...              14   \n",
       "27  [0.0494, 0.0396, 0.0196, 0.042, 0.0397, 0.0346...              15   \n",
       "28  [0.0465, 0.0522, 0.0251, 0.0396, 0.0397, 0.035...              15   \n",
       "29  [0.0382, 0.037, 0.0174, 0.0314, 0.031, 0.0275,...              15   \n",
       "30  [0.0251, 0.0196, 0.0159, 0.0094, 0.0124, 0.019...              16   \n",
       "31  [0.0643, 0.0526, 0.0303, 0.057, 0.0504, 0.0453...              17   \n",
       "32  [0.0436, 0.0358, 0.0175, 0.0382, 0.0327, 0.029...              17   \n",
       "33  [0.0467, 0.0366, 0.0183, 0.0384, 0.0346, 0.031...              18   \n",
       "34  [0.0586, 0.0433, 0.0256, 0.0427, 0.042, 0.042,...              19   \n",
       "35  [0.0412, 0.0309, 0.0144, 0.0333, 0.028, 0.027,...              20   \n",
       "36  [0.0529, 0.0464, 0.0222, 0.0523, 0.0379, 0.038...              21   \n",
       "37  [0.0372, 0.0493, 0.0193, 0.0479, 0.0213, 0.031...              21   \n",
       "38  [0.0808, 0.0701, 0.0328, 0.0762, 0.0669, 0.060...              22   \n",
       "39  [0.0647, 0.0541, 0.0257, 0.0592, 0.0508, 0.047...              22   \n",
       "40  [0.0566, 0.0501, 0.0243, 0.0462, 0.0428, 0.038...              22   \n",
       "41  [0.057, 0.0468, 0.0225, 0.0531, 0.0424, 0.0399...              22   \n",
       "42  [0.0385, 0.0408, 0.0173, 0.0414, 0.0258, 0.028...              23   \n",
       "43  [0.0281, 0.048, 0.0164, 0.0409, 0.014, 0.0236,...              23   \n",
       "44  [0.0288, 0.052, 0.0185, 0.0446, 0.0148, 0.0235...              23   \n",
       "45  [0.0349, 0.0417, 0.0163, 0.0414, 0.022, 0.0266...              23   \n",
       "46  [0.0881, 0.0708, 0.0464, 0.0657, 0.0688, 0.061...              24   \n",
       "47  [0.0616, 0.045, 0.0255, 0.0476, 0.0447, 0.0417...              24   \n",
       "48  [0.0403, 0.0314, 0.0153, 0.0285, 0.031, 0.0272...              25   \n",
       "49  [0.0618, 0.0489, 0.0286, 0.0413, 0.0427, 0.046...              25   \n",
       "50  [0.0659, 0.0503, 0.0317, 0.049, 0.0513, 0.0478...              26   \n",
       "51  [0.0679, 0.0519, 0.0307, 0.0536, 0.0511, 0.050...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0      0.25  0.148804  \n",
       "1                 10       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                 23       NaN       NaN  \n",
       "7                 23       NaN       NaN  \n",
       "8                  0       NaN       NaN  \n",
       "9                 10       NaN       NaN  \n",
       "10                10       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                 0       NaN       NaN  \n",
       "15                 0       NaN       NaN  \n",
       "16                23       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                 0       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                 0       NaN       NaN  \n",
       "25                 0       NaN       NaN  \n",
       "26                10       NaN       NaN  \n",
       "27                23       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                23       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                 0       NaN       NaN  \n",
       "35                23       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                10       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                10       NaN       NaN  \n",
       "49                 0       NaN       NaN  \n",
       "50                10       NaN       NaN  \n",
       "51                10       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 : Training: loss:  0.14664686\n",
      "702 : Training: loss:  0.14999157\n",
      "703 : Training: loss:  0.15702751\n",
      "704 : Training: loss:  0.13379072\n",
      "705 : Training: loss:  0.15334758\n",
      "706 : Training: loss:  0.14788713\n",
      "707 : Training: loss:  0.15365206\n",
      "708 : Training: loss:  0.14673002\n",
      "709 : Training: loss:  0.15872698\n",
      "710 : Training: loss:  0.15683767\n",
      "711 : Training: loss:  0.16140887\n",
      "712 : Training: loss:  0.16055214\n",
      "713 : Training: loss:  0.16247545\n",
      "714 : Training: loss:  0.13631983\n",
      "715 : Training: loss:  0.12658058\n",
      "716 : Training: loss:  0.15714277\n",
      "717 : Training: loss:  0.14006302\n",
      "718 : Training: loss:  0.15407124\n",
      "719 : Training: loss:  0.15758814\n",
      "720 : Training: loss:  0.14410445\n",
      "Validation: Loss:  0.14825407  Accuracy:  0.23076923\n",
      "721 : Training: loss:  0.13913764\n",
      "722 : Training: loss:  0.14739515\n",
      "723 : Training: loss:  0.15338443\n",
      "724 : Training: loss:  0.1470528\n",
      "725 : Training: loss:  0.1399285\n",
      "726 : Training: loss:  0.14156254\n",
      "727 : Training: loss:  0.15975216\n",
      "728 : Training: loss:  0.15347594\n",
      "729 : Training: loss:  0.15806167\n",
      "730 : Training: loss:  0.15654911\n",
      "731 : Training: loss:  0.13330743\n",
      "732 : Training: loss:  0.15295912\n",
      "733 : Training: loss:  0.15003327\n",
      "734 : Training: loss:  0.15248299\n",
      "735 : Training: loss:  0.14490741\n",
      "736 : Training: loss:  0.1400101\n",
      "737 : Training: loss:  0.1387542\n",
      "738 : Training: loss:  0.14584887\n",
      "739 : Training: loss:  0.14310092\n",
      "740 : Training: loss:  0.15879814\n",
      "Validation: Loss:  0.14772019  Accuracy:  0.21153846\n",
      "741 : Training: loss:  0.14062607\n",
      "742 : Training: loss:  0.13265951\n",
      "743 : Training: loss:  0.1481172\n",
      "744 : Training: loss:  0.15319397\n",
      "745 : Training: loss:  0.13782923\n",
      "746 : Training: loss:  0.15239936\n",
      "747 : Training: loss:  0.15318802\n",
      "748 : Training: loss:  0.15445973\n",
      "749 : Training: loss:  0.1509413\n",
      "750 : Training: loss:  0.15139025\n",
      "751 : Training: loss:  0.14483109\n",
      "752 : Training: loss:  0.13259004\n",
      "753 : Training: loss:  0.13795267\n",
      "754 : Training: loss:  0.15350665\n",
      "755 : Training: loss:  0.15063109\n",
      "756 : Training: loss:  0.14587952\n",
      "757 : Training: loss:  0.15534802\n",
      "758 : Training: loss:  0.16087538\n",
      "759 : Training: loss:  0.14330174\n",
      "760 : Training: loss:  0.15296726\n",
      "Validation: Loss:  0.14710486  Accuracy:  0.17307693\n",
      "761 : Training: loss:  0.14014651\n",
      "762 : Training: loss:  0.1523144\n",
      "763 : Training: loss:  0.15643121\n",
      "764 : Training: loss:  0.10749463\n",
      "765 : Training: loss:  0.13626392\n",
      "766 : Training: loss:  0.12779766\n",
      "767 : Training: loss:  0.15873171\n",
      "768 : Training: loss:  0.15549496\n",
      "769 : Training: loss:  0.12204752\n",
      "770 : Training: loss:  0.12994722\n",
      "771 : Training: loss:  0.147847\n",
      "772 : Training: loss:  0.14179385\n",
      "773 : Training: loss:  0.15150727\n",
      "774 : Training: loss:  0.14749838\n",
      "775 : Training: loss:  0.15946855\n",
      "776 : Training: loss:  0.1530193\n",
      "777 : Training: loss:  0.15619045\n",
      "778 : Training: loss:  0.121691294\n",
      "779 : Training: loss:  0.14920455\n",
      "780 : Training: loss:  0.14932692\n",
      "Validation: Loss:  0.1464875  Accuracy:  0.17307693\n",
      "781 : Training: loss:  0.11356567\n",
      "782 : Training: loss:  0.14700599\n",
      "783 : Training: loss:  0.1578091\n",
      "784 : Training: loss:  0.14806879\n",
      "785 : Training: loss:  0.1383258\n",
      "786 : Training: loss:  0.14435938\n",
      "787 : Training: loss:  0.14538576\n",
      "788 : Training: loss:  0.16835244\n",
      "789 : Training: loss:  0.13992625\n",
      "790 : Training: loss:  0.1476892\n",
      "791 : Training: loss:  0.15299346\n",
      "792 : Training: loss:  0.1677533\n",
      "793 : Training: loss:  0.15577605\n",
      "794 : Training: loss:  0.13658686\n",
      "795 : Training: loss:  0.14014046\n",
      "796 : Training: loss:  0.15098044\n",
      "797 : Training: loss:  0.13968527\n",
      "798 : Training: loss:  0.1394132\n",
      "799 : Training: loss:  0.11928874\n",
      "800 : Training: loss:  0.15217552\n",
      "Validation: Loss:  0.14603435  Accuracy:  0.17307693\n",
      "801 : Training: loss:  0.15008685\n",
      "802 : Training: loss:  0.14546493\n",
      "803 : Training: loss:  0.097836815\n",
      "804 : Training: loss:  0.15402631\n",
      "805 : Training: loss:  0.13044204\n",
      "806 : Training: loss:  0.14294572\n",
      "807 : Training: loss:  0.13567787\n",
      "808 : Training: loss:  0.16658717\n",
      "809 : Training: loss:  0.15658368\n",
      "810 : Training: loss:  0.15001501\n",
      "811 : Training: loss:  0.15567239\n",
      "812 : Training: loss:  0.15969928\n",
      "813 : Training: loss:  0.16004045\n",
      "814 : Training: loss:  0.15123479\n",
      "815 : Training: loss:  0.15992707\n",
      "816 : Training: loss:  0.12997653\n",
      "817 : Training: loss:  0.1496973\n",
      "818 : Training: loss:  0.15429522\n",
      "819 : Training: loss:  0.16225855\n",
      "820 : Training: loss:  0.1522513\n",
      "Validation: Loss:  0.14568622  Accuracy:  0.17307693\n",
      "821 : Training: loss:  0.12737697\n",
      "822 : Training: loss:  0.15701567\n",
      "823 : Training: loss:  0.14989221\n",
      "824 : Training: loss:  0.1530435\n",
      "825 : Training: loss:  0.15097435\n",
      "826 : Training: loss:  0.15405019\n",
      "827 : Training: loss:  0.13976353\n",
      "828 : Training: loss:  0.15222114\n",
      "829 : Training: loss:  0.14654586\n",
      "830 : Training: loss:  0.16227554\n",
      "831 : Training: loss:  0.14787436\n",
      "832 : Training: loss:  0.14328077\n",
      "833 : Training: loss:  0.15333039\n",
      "834 : Training: loss:  0.15472104\n",
      "835 : Training: loss:  0.1474664\n",
      "836 : Training: loss:  0.15344104\n",
      "837 : Training: loss:  0.1260183\n",
      "838 : Training: loss:  0.12364942\n",
      "839 : Training: loss:  0.15068124\n",
      "840 : Training: loss:  0.15523621\n",
      "Validation: Loss:  0.14516659  Accuracy:  0.17307693\n",
      "841 : Training: loss:  0.11710239\n",
      "842 : Training: loss:  0.1512306\n",
      "843 : Training: loss:  0.13221473\n",
      "844 : Training: loss:  0.15267274\n",
      "845 : Training: loss:  0.14207484\n",
      "846 : Training: loss:  0.16242073\n",
      "847 : Training: loss:  0.11135779\n",
      "848 : Training: loss:  0.15752643\n",
      "849 : Training: loss:  0.15263395\n",
      "850 : Training: loss:  0.12400998\n",
      "851 : Training: loss:  0.13211876\n",
      "852 : Training: loss:  0.14884646\n",
      "853 : Training: loss:  0.1453517\n",
      "854 : Training: loss:  0.1136401\n",
      "855 : Training: loss:  0.12111992\n",
      "856 : Training: loss:  0.15050733\n",
      "857 : Training: loss:  0.14590718\n",
      "858 : Training: loss:  0.13742952\n",
      "859 : Training: loss:  0.13154405\n",
      "860 : Training: loss:  0.13053569\n",
      "Validation: Loss:  0.14475185  Accuracy:  0.23076923\n",
      "861 : Training: loss:  0.15031444\n",
      "862 : Training: loss:  0.15497114\n",
      "863 : Training: loss:  0.12990062\n",
      "864 : Training: loss:  0.1422266\n",
      "865 : Training: loss:  0.13632624\n",
      "866 : Training: loss:  0.1623645\n",
      "867 : Training: loss:  0.15081705\n",
      "868 : Training: loss:  0.15473785\n",
      "869 : Training: loss:  0.14600252\n",
      "870 : Training: loss:  0.13298896\n",
      "871 : Training: loss:  0.14311066\n",
      "872 : Training: loss:  0.15435088\n",
      "873 : Training: loss:  0.15306883\n",
      "874 : Training: loss:  0.1442878\n",
      "875 : Training: loss:  0.13737619\n",
      "876 : Training: loss:  0.15711616\n",
      "877 : Training: loss:  0.15044525\n",
      "878 : Training: loss:  0.16228186\n",
      "879 : Training: loss:  0.14669116\n",
      "880 : Training: loss:  0.16818103\n",
      "Validation: Loss:  0.14447267  Accuracy:  0.23076923\n",
      "881 : Training: loss:  0.12737319\n",
      "882 : Training: loss:  0.14873621\n",
      "883 : Training: loss:  0.110306166\n",
      "884 : Training: loss:  0.1477798\n",
      "885 : Training: loss:  0.1351284\n",
      "886 : Training: loss:  0.1552255\n",
      "887 : Training: loss:  0.14206828\n",
      "888 : Training: loss:  0.14783317\n",
      "889 : Training: loss:  0.14189151\n",
      "890 : Training: loss:  0.16417636\n",
      "891 : Training: loss:  0.14728117\n",
      "892 : Training: loss:  0.15832758\n",
      "893 : Training: loss:  0.15075131\n",
      "894 : Training: loss:  0.16392858\n",
      "895 : Training: loss:  0.15323229\n",
      "896 : Training: loss:  0.1539225\n",
      "897 : Training: loss:  0.11929006\n",
      "898 : Training: loss:  0.15520866\n",
      "899 : Training: loss:  0.14626344\n",
      "900 : Training: loss:  0.14751948\n",
      "Validation: Loss:  0.1441574  Accuracy:  0.26923078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0625, 0.0466, 0.0314, 0.0627, 0.0385, 0.037...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.144157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0569, 0.0426, 0.0278, 0.0557, 0.0364, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0636, 0.0522, 0.0337, 0.0671, 0.0382, 0.040...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0399, 0.0314, 0.0185, 0.0397, 0.023, 0.0248...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0546, 0.0429, 0.0265, 0.0567, 0.0326, 0.033...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0627, 0.0582, 0.0365, 0.0859, 0.0415, 0.042...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0548, 0.0519, 0.025, 0.0931, 0.0341, 0.0373...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0661, 0.0666, 0.0327, 0.1105, 0.0438, 0.047...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0994, 0.077, 0.0581, 0.1133, 0.0709, 0.0695...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0463, 0.0357, 0.0211, 0.0423, 0.0336, 0.031...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0338, 0.0328, 0.0148, 0.0349, 0.0223, 0.023...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0464, 0.0412, 0.0245, 0.0465, 0.0265, 0.033...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.047, 0.0643, 0.0388, 0.0531, 0.028, 0.0333,...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0618, 0.1245, 0.0803, 0.0805, 0.0481, 0.055...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0407, 0.0326, 0.0177, 0.0385, 0.0242, 0.025...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0579, 0.043, 0.0311, 0.0405, 0.03, 0.0344, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0605, 0.0531, 0.0345, 0.08, 0.0416, 0.041, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0537, 0.0524, 0.0319, 0.068, 0.0376, 0.0366...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0607, 0.0479, 0.0299, 0.058, 0.0391, 0.0427...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0657, 0.05, 0.0321, 0.0563, 0.0401, 0.0459,...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0624, 0.0467, 0.0341, 0.0582, 0.0384, 0.041...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0425, 0.0412, 0.021, 0.0547, 0.0245, 0.0294...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0517, 0.0371, 0.0251, 0.0446, 0.0304, 0.033...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0565, 0.0453, 0.0299, 0.0449, 0.0348, 0.040...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0604, 0.0453, 0.0291, 0.0667, 0.0371, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0597, 0.0477, 0.0298, 0.0656, 0.0398, 0.035...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0611, 0.0487, 0.0297, 0.0617, 0.0406, 0.036...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0481, 0.0408, 0.0238, 0.0559, 0.0326, 0.031...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0398, 0.0514, 0.0286, 0.0468, 0.0301, 0.029...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0347, 0.0383, 0.021, 0.0381, 0.0242, 0.0232...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.046, 0.0427, 0.0516, 0.0166, 0.0192, 0.0377...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0607, 0.0519, 0.0354, 0.0724, 0.0406, 0.039...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0396, 0.0347, 0.02, 0.0498, 0.0246, 0.0245,...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0476, 0.0389, 0.023, 0.0538, 0.029, 0.0293,...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0611, 0.0463, 0.0326, 0.0547, 0.0362, 0.040...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0399, 0.031, 0.0173, 0.0447, 0.0217, 0.0234...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0501, 0.0478, 0.0262, 0.0725, 0.0297, 0.033...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0324, 0.0528, 0.0235, 0.0655, 0.0153, 0.026...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0811, 0.0723, 0.0363, 0.0995, 0.0583, 0.056...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0645, 0.0557, 0.0295, 0.08, 0.043, 0.0439, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0551, 0.0507, 0.0283, 0.0576, 0.0349, 0.033...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.056, 0.0478, 0.0264, 0.0739, 0.0344, 0.0359...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0355, 0.0428, 0.0208, 0.0593, 0.0192, 0.024...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0251, 0.0538, 0.0206, 0.0641, 0.0101, 0.020...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0257, 0.0574, 0.0234, 0.0682, 0.0107, 0.020...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.031, 0.0438, 0.0194, 0.0613, 0.0158, 0.0223...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0831, 0.0674, 0.0532, 0.0721, 0.0562, 0.052...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0599, 0.0439, 0.03, 0.0603, 0.0357, 0.0367,...</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0442, 0.0365, 0.0211, 0.0386, 0.0285, 0.027...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0691, 0.059, 0.0394, 0.0532, 0.0395, 0.049,...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0622, 0.0484, 0.0362, 0.0548, 0.0399, 0.041...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0663, 0.0521, 0.036, 0.0644, 0.0413, 0.0457...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0625, 0.0466, 0.0314, 0.0627, 0.0385, 0.037...               0   \n",
       "1   [0.0569, 0.0426, 0.0278, 0.0557, 0.0364, 0.036...               0   \n",
       "2   [0.0636, 0.0522, 0.0337, 0.0671, 0.0382, 0.040...               0   \n",
       "3   [0.0399, 0.0314, 0.0185, 0.0397, 0.023, 0.0248...               1   \n",
       "4   [0.0546, 0.0429, 0.0265, 0.0567, 0.0326, 0.033...               1   \n",
       "5   [0.0627, 0.0582, 0.0365, 0.0859, 0.0415, 0.042...               2   \n",
       "6   [0.0548, 0.0519, 0.025, 0.0931, 0.0341, 0.0373...               3   \n",
       "7   [0.0661, 0.0666, 0.0327, 0.1105, 0.0438, 0.047...               3   \n",
       "8   [0.0994, 0.077, 0.0581, 0.1133, 0.0709, 0.0695...               3   \n",
       "9   [0.0463, 0.0357, 0.0211, 0.0423, 0.0336, 0.031...               4   \n",
       "10  [0.0338, 0.0328, 0.0148, 0.0349, 0.0223, 0.023...               4   \n",
       "11  [0.0464, 0.0412, 0.0245, 0.0465, 0.0265, 0.033...               5   \n",
       "12  [0.047, 0.0643, 0.0388, 0.0531, 0.028, 0.0333,...               6   \n",
       "13  [0.0618, 0.1245, 0.0803, 0.0805, 0.0481, 0.055...               7   \n",
       "14  [0.0407, 0.0326, 0.0177, 0.0385, 0.0242, 0.025...               8   \n",
       "15  [0.0579, 0.043, 0.0311, 0.0405, 0.03, 0.0344, ...               8   \n",
       "16  [0.0605, 0.0531, 0.0345, 0.08, 0.0416, 0.041, ...               9   \n",
       "17  [0.0537, 0.0524, 0.0319, 0.068, 0.0376, 0.0366...               9   \n",
       "18  [0.0607, 0.0479, 0.0299, 0.058, 0.0391, 0.0427...              10   \n",
       "19  [0.0657, 0.05, 0.0321, 0.0563, 0.0401, 0.0459,...              10   \n",
       "20  [0.0624, 0.0467, 0.0341, 0.0582, 0.0384, 0.041...              11   \n",
       "21  [0.0425, 0.0412, 0.021, 0.0547, 0.0245, 0.0294...              11   \n",
       "22  [0.0517, 0.0371, 0.0251, 0.0446, 0.0304, 0.033...              12   \n",
       "23  [0.0565, 0.0453, 0.0299, 0.0449, 0.0348, 0.040...              13   \n",
       "24  [0.0604, 0.0453, 0.0291, 0.0667, 0.0371, 0.038...              13   \n",
       "25  [0.0597, 0.0477, 0.0298, 0.0656, 0.0398, 0.035...              14   \n",
       "26  [0.0611, 0.0487, 0.0297, 0.0617, 0.0406, 0.036...              14   \n",
       "27  [0.0481, 0.0408, 0.0238, 0.0559, 0.0326, 0.031...              15   \n",
       "28  [0.0398, 0.0514, 0.0286, 0.0468, 0.0301, 0.029...              15   \n",
       "29  [0.0347, 0.0383, 0.021, 0.0381, 0.0242, 0.0232...              15   \n",
       "30  [0.046, 0.0427, 0.0516, 0.0166, 0.0192, 0.0377...              16   \n",
       "31  [0.0607, 0.0519, 0.0354, 0.0724, 0.0406, 0.039...              17   \n",
       "32  [0.0396, 0.0347, 0.02, 0.0498, 0.0246, 0.0245,...              17   \n",
       "33  [0.0476, 0.0389, 0.023, 0.0538, 0.029, 0.0293,...              18   \n",
       "34  [0.0611, 0.0463, 0.0326, 0.0547, 0.0362, 0.040...              19   \n",
       "35  [0.0399, 0.031, 0.0173, 0.0447, 0.0217, 0.0234...              20   \n",
       "36  [0.0501, 0.0478, 0.0262, 0.0725, 0.0297, 0.033...              21   \n",
       "37  [0.0324, 0.0528, 0.0235, 0.0655, 0.0153, 0.026...              21   \n",
       "38  [0.0811, 0.0723, 0.0363, 0.0995, 0.0583, 0.056...              22   \n",
       "39  [0.0645, 0.0557, 0.0295, 0.08, 0.043, 0.0439, ...              22   \n",
       "40  [0.0551, 0.0507, 0.0283, 0.0576, 0.0349, 0.033...              22   \n",
       "41  [0.056, 0.0478, 0.0264, 0.0739, 0.0344, 0.0359...              22   \n",
       "42  [0.0355, 0.0428, 0.0208, 0.0593, 0.0192, 0.024...              23   \n",
       "43  [0.0251, 0.0538, 0.0206, 0.0641, 0.0101, 0.020...              23   \n",
       "44  [0.0257, 0.0574, 0.0234, 0.0682, 0.0107, 0.020...              23   \n",
       "45  [0.031, 0.0438, 0.0194, 0.0613, 0.0158, 0.0223...              23   \n",
       "46  [0.0831, 0.0674, 0.0532, 0.0721, 0.0562, 0.052...              24   \n",
       "47  [0.0599, 0.0439, 0.03, 0.0603, 0.0357, 0.0367,...              24   \n",
       "48  [0.0442, 0.0365, 0.0211, 0.0386, 0.0285, 0.027...              25   \n",
       "49  [0.0691, 0.059, 0.0394, 0.0532, 0.0395, 0.049,...              25   \n",
       "50  [0.0622, 0.0484, 0.0362, 0.0548, 0.0399, 0.041...              26   \n",
       "51  [0.0663, 0.0521, 0.036, 0.0644, 0.0413, 0.0457...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                 22  0.269231  0.144157  \n",
       "1                  0       NaN       NaN  \n",
       "2                 22       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                 22       NaN       NaN  \n",
       "5                 23       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                 22       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                23       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                22       NaN       NaN  \n",
       "20                22       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                22       NaN       NaN  \n",
       "23                22       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                22       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                23       NaN       NaN  \n",
       "29                23       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                23       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                22       NaN       NaN  \n",
       "35                 3       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 0       NaN       NaN  \n",
       "47                 3       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                 0       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 : Training: loss:  0.14227322\n",
      "902 : Training: loss:  0.15094255\n",
      "903 : Training: loss:  0.1584513\n",
      "904 : Training: loss:  0.108189546\n",
      "905 : Training: loss:  0.124908\n",
      "906 : Training: loss:  0.15150629\n",
      "907 : Training: loss:  0.12917428\n",
      "908 : Training: loss:  0.13922809\n",
      "909 : Training: loss:  0.14911382\n",
      "910 : Training: loss:  0.1525018\n",
      "911 : Training: loss:  0.1396805\n",
      "912 : Training: loss:  0.14765446\n",
      "913 : Training: loss:  0.12327971\n",
      "914 : Training: loss:  0.1297436\n",
      "915 : Training: loss:  0.15654469\n",
      "916 : Training: loss:  0.11360335\n",
      "917 : Training: loss:  0.1560026\n",
      "918 : Training: loss:  0.15987699\n",
      "919 : Training: loss:  0.13180788\n",
      "920 : Training: loss:  0.1646042\n",
      "Validation: Loss:  0.14368686  Accuracy:  0.26923078\n",
      "921 : Training: loss:  0.1547591\n",
      "922 : Training: loss:  0.15500127\n",
      "923 : Training: loss:  0.1505578\n",
      "924 : Training: loss:  0.14836903\n",
      "925 : Training: loss:  0.14374858\n",
      "926 : Training: loss:  0.120796315\n",
      "927 : Training: loss:  0.1106897\n",
      "928 : Training: loss:  0.15484974\n",
      "929 : Training: loss:  0.14091343\n",
      "930 : Training: loss:  0.14846565\n",
      "931 : Training: loss:  0.15099019\n",
      "932 : Training: loss:  0.16467282\n",
      "933 : Training: loss:  0.14874364\n",
      "934 : Training: loss:  0.10216736\n",
      "935 : Training: loss:  0.12861273\n",
      "936 : Training: loss:  0.13381302\n",
      "937 : Training: loss:  0.13240111\n",
      "938 : Training: loss:  0.123281844\n",
      "939 : Training: loss:  0.14481656\n",
      "940 : Training: loss:  0.14820254\n",
      "Validation: Loss:  0.14327694  Accuracy:  0.26923078\n",
      "941 : Training: loss:  0.1428388\n",
      "942 : Training: loss:  0.15338075\n",
      "943 : Training: loss:  0.10842961\n",
      "944 : Training: loss:  0.1568777\n",
      "945 : Training: loss:  0.11750242\n",
      "946 : Training: loss:  0.12683684\n",
      "947 : Training: loss:  0.16162115\n",
      "948 : Training: loss:  0.14398111\n",
      "949 : Training: loss:  0.14925793\n",
      "950 : Training: loss:  0.15162528\n",
      "951 : Training: loss:  0.16352597\n",
      "952 : Training: loss:  0.14556569\n",
      "953 : Training: loss:  0.1494015\n",
      "954 : Training: loss:  0.14966738\n",
      "955 : Training: loss:  0.15509129\n",
      "956 : Training: loss:  0.15090148\n",
      "957 : Training: loss:  0.14847162\n",
      "958 : Training: loss:  0.1488865\n",
      "959 : Training: loss:  0.14253919\n",
      "960 : Training: loss:  0.15279627\n",
      "Validation: Loss:  0.14287  Accuracy:  0.23076923\n",
      "961 : Training: loss:  0.14692834\n",
      "962 : Training: loss:  0.14867766\n",
      "963 : Training: loss:  0.11963498\n",
      "964 : Training: loss:  0.1569084\n",
      "965 : Training: loss:  0.14526045\n",
      "966 : Training: loss:  0.15455472\n",
      "967 : Training: loss:  0.13347048\n",
      "968 : Training: loss:  0.1448556\n",
      "969 : Training: loss:  0.15751396\n",
      "970 : Training: loss:  0.11686641\n",
      "971 : Training: loss:  0.111688904\n",
      "972 : Training: loss:  0.13788277\n",
      "973 : Training: loss:  0.1534373\n",
      "974 : Training: loss:  0.14900161\n",
      "975 : Training: loss:  0.14647286\n",
      "976 : Training: loss:  0.14831504\n",
      "977 : Training: loss:  0.1469798\n",
      "978 : Training: loss:  0.13689364\n",
      "979 : Training: loss:  0.15911359\n",
      "980 : Training: loss:  0.11586581\n",
      "Validation: Loss:  0.14238219  Accuracy:  0.26923078\n",
      "981 : Training: loss:  0.12397579\n",
      "982 : Training: loss:  0.14377253\n",
      "983 : Training: loss:  0.15722778\n",
      "984 : Training: loss:  0.13405496\n",
      "985 : Training: loss:  0.1451129\n",
      "986 : Training: loss:  0.1532196\n",
      "987 : Training: loss:  0.13633938\n",
      "988 : Training: loss:  0.12462281\n",
      "989 : Training: loss:  0.1605852\n",
      "990 : Training: loss:  0.15506127\n",
      "991 : Training: loss:  0.14287737\n",
      "992 : Training: loss:  0.14047311\n",
      "993 : Training: loss:  0.13677664\n",
      "994 : Training: loss:  0.1478056\n",
      "995 : Training: loss:  0.15568571\n",
      "996 : Training: loss:  0.14283574\n",
      "997 : Training: loss:  0.15450612\n",
      "998 : Training: loss:  0.121667944\n",
      "999 : Training: loss:  0.14268664\n",
      "1000 : Training: loss:  0.14961547\n",
      "Validation: Loss:  0.1420095  Accuracy:  0.28846154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.063, 0.0392, 0.0317, 0.0624, 0.0408, 0.0387...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.142009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0586, 0.0366, 0.0286, 0.0557, 0.04, 0.0398,...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0643, 0.045, 0.034, 0.0674, 0.0405, 0.043, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0405, 0.0258, 0.0187, 0.0386, 0.0252, 0.026...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0551, 0.0359, 0.0267, 0.0565, 0.0348, 0.034...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0603, 0.048, 0.0358, 0.0868, 0.0422, 0.043,...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0527, 0.0425, 0.024, 0.0981, 0.0353, 0.0376...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0634, 0.0555, 0.0312, 0.1168, 0.0452, 0.048...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0966, 0.0651, 0.0568, 0.1131, 0.0721, 0.069...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0487, 0.0305, 0.0215, 0.0412, 0.0392, 0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.036, 0.0287, 0.0152, 0.0351, 0.0269, 0.0267...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0474, 0.0355, 0.025, 0.0457, 0.0289, 0.0361...</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0453, 0.0544, 0.0385, 0.0499, 0.0294, 0.033...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0557, 0.104, 0.076, 0.0752, 0.0479, 0.0532,...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0413, 0.0269, 0.0176, 0.0372, 0.0271, 0.027...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0607, 0.0371, 0.0322, 0.0384, 0.0329, 0.037...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.059, 0.0439, 0.0343, 0.0802, 0.043, 0.0413,...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0522, 0.0432, 0.0317, 0.0667, 0.0391, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0615, 0.0409, 0.0299, 0.0566, 0.0421, 0.045...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0677, 0.0433, 0.0325, 0.0544, 0.044, 0.0496...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0625, 0.0388, 0.0342, 0.0559, 0.0408, 0.043...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0408, 0.033, 0.0199, 0.0529, 0.0253, 0.0298...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0518, 0.0299, 0.0246, 0.0417, 0.0325, 0.035...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0585, 0.0393, 0.0307, 0.0429, 0.0385, 0.044...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0598, 0.0375, 0.029, 0.0665, 0.0387, 0.0398...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0597, 0.0398, 0.0298, 0.065, 0.0422, 0.0365...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0613, 0.0405, 0.0293, 0.0602, 0.0434, 0.038...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0479, 0.0339, 0.024, 0.0556, 0.0353, 0.0323...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0383, 0.043, 0.0286, 0.0455, 0.0324, 0.0301...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0341, 0.032, 0.0213, 0.0365, 0.0265, 0.0243...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0553, 0.0433, 0.0608, 0.0155, 0.0265, 0.048...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0594, 0.0431, 0.0353, 0.0714, 0.0421, 0.039...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0383, 0.0279, 0.0196, 0.049, 0.0256, 0.0247...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.048, 0.0323, 0.0231, 0.054, 0.0313, 0.0308,...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0618, 0.0389, 0.0326, 0.0526, 0.0387, 0.042...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0394, 0.0248, 0.017, 0.0437, 0.023, 0.024, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0485, 0.0396, 0.0256, 0.0738, 0.0306, 0.034...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0291, 0.0422, 0.0217, 0.0639, 0.0149, 0.025...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0811, 0.0619, 0.0349, 0.0997, 0.0629, 0.057...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0638, 0.0465, 0.0284, 0.0802, 0.0456, 0.045...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0543, 0.0417, 0.0275, 0.0552, 0.0367, 0.033...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0553, 0.0397, 0.0259, 0.0754, 0.0361, 0.036...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0335, 0.0345, 0.02, 0.0598, 0.0194, 0.0246,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0221, 0.041, 0.0185, 0.0634, 0.0096, 0.0191...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.023, 0.0438, 0.0214, 0.0674, 0.0102, 0.0188...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0284, 0.0344, 0.0182, 0.0616, 0.0155, 0.021...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0798, 0.0555, 0.0515, 0.0663, 0.0567, 0.051...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0591, 0.036, 0.0297, 0.0589, 0.0369, 0.0375...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0467, 0.0316, 0.022, 0.0375, 0.033, 0.0303,...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0728, 0.0534, 0.0408, 0.0515, 0.0439, 0.054...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0622, 0.0409, 0.0364, 0.0519, 0.0421, 0.043...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.066, 0.0442, 0.0358, 0.0623, 0.0432, 0.0478...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.063, 0.0392, 0.0317, 0.0624, 0.0408, 0.0387...               0   \n",
       "1   [0.0586, 0.0366, 0.0286, 0.0557, 0.04, 0.0398,...               0   \n",
       "2   [0.0643, 0.045, 0.034, 0.0674, 0.0405, 0.043, ...               0   \n",
       "3   [0.0405, 0.0258, 0.0187, 0.0386, 0.0252, 0.026...               1   \n",
       "4   [0.0551, 0.0359, 0.0267, 0.0565, 0.0348, 0.034...               1   \n",
       "5   [0.0603, 0.048, 0.0358, 0.0868, 0.0422, 0.043,...               2   \n",
       "6   [0.0527, 0.0425, 0.024, 0.0981, 0.0353, 0.0376...               3   \n",
       "7   [0.0634, 0.0555, 0.0312, 0.1168, 0.0452, 0.048...               3   \n",
       "8   [0.0966, 0.0651, 0.0568, 0.1131, 0.0721, 0.069...               3   \n",
       "9   [0.0487, 0.0305, 0.0215, 0.0412, 0.0392, 0.035...               4   \n",
       "10  [0.036, 0.0287, 0.0152, 0.0351, 0.0269, 0.0267...               4   \n",
       "11  [0.0474, 0.0355, 0.025, 0.0457, 0.0289, 0.0361...               5   \n",
       "12  [0.0453, 0.0544, 0.0385, 0.0499, 0.0294, 0.033...               6   \n",
       "13  [0.0557, 0.104, 0.076, 0.0752, 0.0479, 0.0532,...               7   \n",
       "14  [0.0413, 0.0269, 0.0176, 0.0372, 0.0271, 0.027...               8   \n",
       "15  [0.0607, 0.0371, 0.0322, 0.0384, 0.0329, 0.037...               8   \n",
       "16  [0.059, 0.0439, 0.0343, 0.0802, 0.043, 0.0413,...               9   \n",
       "17  [0.0522, 0.0432, 0.0317, 0.0667, 0.0391, 0.036...               9   \n",
       "18  [0.0615, 0.0409, 0.0299, 0.0566, 0.0421, 0.045...              10   \n",
       "19  [0.0677, 0.0433, 0.0325, 0.0544, 0.044, 0.0496...              10   \n",
       "20  [0.0625, 0.0388, 0.0342, 0.0559, 0.0408, 0.043...              11   \n",
       "21  [0.0408, 0.033, 0.0199, 0.0529, 0.0253, 0.0298...              11   \n",
       "22  [0.0518, 0.0299, 0.0246, 0.0417, 0.0325, 0.035...              12   \n",
       "23  [0.0585, 0.0393, 0.0307, 0.0429, 0.0385, 0.044...              13   \n",
       "24  [0.0598, 0.0375, 0.029, 0.0665, 0.0387, 0.0398...              13   \n",
       "25  [0.0597, 0.0398, 0.0298, 0.065, 0.0422, 0.0365...              14   \n",
       "26  [0.0613, 0.0405, 0.0293, 0.0602, 0.0434, 0.038...              14   \n",
       "27  [0.0479, 0.0339, 0.024, 0.0556, 0.0353, 0.0323...              15   \n",
       "28  [0.0383, 0.043, 0.0286, 0.0455, 0.0324, 0.0301...              15   \n",
       "29  [0.0341, 0.032, 0.0213, 0.0365, 0.0265, 0.0243...              15   \n",
       "30  [0.0553, 0.0433, 0.0608, 0.0155, 0.0265, 0.048...              16   \n",
       "31  [0.0594, 0.0431, 0.0353, 0.0714, 0.0421, 0.039...              17   \n",
       "32  [0.0383, 0.0279, 0.0196, 0.049, 0.0256, 0.0247...              17   \n",
       "33  [0.048, 0.0323, 0.0231, 0.054, 0.0313, 0.0308,...              18   \n",
       "34  [0.0618, 0.0389, 0.0326, 0.0526, 0.0387, 0.042...              19   \n",
       "35  [0.0394, 0.0248, 0.017, 0.0437, 0.023, 0.024, ...              20   \n",
       "36  [0.0485, 0.0396, 0.0256, 0.0738, 0.0306, 0.034...              21   \n",
       "37  [0.0291, 0.0422, 0.0217, 0.0639, 0.0149, 0.025...              21   \n",
       "38  [0.0811, 0.0619, 0.0349, 0.0997, 0.0629, 0.057...              22   \n",
       "39  [0.0638, 0.0465, 0.0284, 0.0802, 0.0456, 0.045...              22   \n",
       "40  [0.0543, 0.0417, 0.0275, 0.0552, 0.0367, 0.033...              22   \n",
       "41  [0.0553, 0.0397, 0.0259, 0.0754, 0.0361, 0.036...              22   \n",
       "42  [0.0335, 0.0345, 0.02, 0.0598, 0.0194, 0.0246,...              23   \n",
       "43  [0.0221, 0.041, 0.0185, 0.0634, 0.0096, 0.0191...              23   \n",
       "44  [0.023, 0.0438, 0.0214, 0.0674, 0.0102, 0.0188...              23   \n",
       "45  [0.0284, 0.0344, 0.0182, 0.0616, 0.0155, 0.021...              23   \n",
       "46  [0.0798, 0.0555, 0.0515, 0.0663, 0.0567, 0.051...              24   \n",
       "47  [0.0591, 0.036, 0.0297, 0.0589, 0.0369, 0.0375...              24   \n",
       "48  [0.0467, 0.0316, 0.022, 0.0375, 0.033, 0.0303,...              25   \n",
       "49  [0.0728, 0.0534, 0.0408, 0.0515, 0.0439, 0.054...              25   \n",
       "50  [0.0622, 0.0409, 0.0364, 0.0519, 0.0421, 0.043...              26   \n",
       "51  [0.066, 0.0442, 0.0358, 0.0623, 0.0432, 0.0478...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                 22  0.288462  0.142009  \n",
       "1                 22       NaN       NaN  \n",
       "2                 22       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                 22       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                 22       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                22       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                23       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                22       NaN       NaN  \n",
       "20                22       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                22       NaN       NaN  \n",
       "23                22       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                22       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                 3       NaN       NaN  \n",
       "32                23       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                22       NaN       NaN  \n",
       "35                22       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                22       NaN       NaN  \n",
       "47                22       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                22       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 : Training: loss:  0.14902237\n",
      "1002 : Training: loss:  0.13596964\n",
      "1003 : Training: loss:  0.1391956\n",
      "1004 : Training: loss:  0.14585616\n",
      "1005 : Training: loss:  0.11485752\n",
      "1006 : Training: loss:  0.1429099\n",
      "1007 : Training: loss:  0.14276357\n",
      "1008 : Training: loss:  0.14042486\n",
      "1009 : Training: loss:  0.1534585\n",
      "1010 : Training: loss:  0.14391077\n",
      "1011 : Training: loss:  0.16670033\n",
      "1012 : Training: loss:  0.13540727\n",
      "1013 : Training: loss:  0.10242587\n",
      "1014 : Training: loss:  0.14611353\n",
      "1015 : Training: loss:  0.14944401\n",
      "1016 : Training: loss:  0.10714387\n",
      "1017 : Training: loss:  0.12436062\n",
      "1018 : Training: loss:  0.15979424\n",
      "1019 : Training: loss:  0.15595472\n",
      "1020 : Training: loss:  0.14730906\n",
      "Validation: Loss:  0.14160861  Accuracy:  0.28846154\n",
      "1021 : Training: loss:  0.1608415\n",
      "1022 : Training: loss:  0.12195076\n",
      "1023 : Training: loss:  0.15272345\n",
      "1024 : Training: loss:  0.13302976\n",
      "1025 : Training: loss:  0.15217486\n",
      "1026 : Training: loss:  0.15704317\n",
      "1027 : Training: loss:  0.1428469\n",
      "1028 : Training: loss:  0.12667811\n",
      "1029 : Training: loss:  0.14980467\n",
      "1030 : Training: loss:  0.15117829\n",
      "1031 : Training: loss:  0.14095093\n",
      "1032 : Training: loss:  0.13078785\n",
      "1033 : Training: loss:  0.14220013\n",
      "1034 : Training: loss:  0.13053867\n",
      "1035 : Training: loss:  0.15056314\n",
      "1036 : Training: loss:  0.11771511\n",
      "1037 : Training: loss:  0.13122337\n",
      "1038 : Training: loss:  0.14723028\n",
      "1039 : Training: loss:  0.15963039\n",
      "1040 : Training: loss:  0.15497948\n",
      "Validation: Loss:  0.14110078  Accuracy:  0.25\n",
      "1041 : Training: loss:  0.14722301\n",
      "1042 : Training: loss:  0.15503596\n",
      "1043 : Training: loss:  0.1280895\n",
      "1044 : Training: loss:  0.15441924\n",
      "1045 : Training: loss:  0.13018024\n",
      "1046 : Training: loss:  0.14888385\n",
      "1047 : Training: loss:  0.1352446\n",
      "1048 : Training: loss:  0.14308204\n",
      "1049 : Training: loss:  0.15423481\n",
      "1050 : Training: loss:  0.16135938\n",
      "1051 : Training: loss:  0.13828659\n",
      "1052 : Training: loss:  0.15443575\n",
      "1053 : Training: loss:  0.15551898\n",
      "1054 : Training: loss:  0.13352153\n",
      "1055 : Training: loss:  0.14441182\n",
      "1056 : Training: loss:  0.1459145\n",
      "1057 : Training: loss:  0.15858084\n",
      "1058 : Training: loss:  0.14505884\n",
      "1059 : Training: loss:  0.13625444\n",
      "1060 : Training: loss:  0.10199687\n",
      "Validation: Loss:  0.14079782  Accuracy:  0.23076923\n",
      "1061 : Training: loss:  0.14468242\n",
      "1062 : Training: loss:  0.14818268\n",
      "1063 : Training: loss:  0.08612655\n",
      "1064 : Training: loss:  0.1448102\n",
      "1065 : Training: loss:  0.12911813\n",
      "1066 : Training: loss:  0.14798057\n",
      "1067 : Training: loss:  0.15396903\n",
      "1068 : Training: loss:  0.14034915\n",
      "1069 : Training: loss:  0.13860062\n",
      "1070 : Training: loss:  0.13103268\n",
      "1071 : Training: loss:  0.13016318\n",
      "1072 : Training: loss:  0.13726185\n",
      "1073 : Training: loss:  0.15362504\n",
      "1074 : Training: loss:  0.16192053\n",
      "1075 : Training: loss:  0.14065133\n",
      "1076 : Training: loss:  0.15554354\n",
      "1077 : Training: loss:  0.12958978\n",
      "1078 : Training: loss:  0.1124446\n",
      "1079 : Training: loss:  0.1266361\n",
      "1080 : Training: loss:  0.14558542\n",
      "Validation: Loss:  0.14050914  Accuracy:  0.23076923\n",
      "1081 : Training: loss:  0.1485369\n",
      "1082 : Training: loss:  0.11141032\n",
      "1083 : Training: loss:  0.14090021\n",
      "1084 : Training: loss:  0.11695637\n",
      "1085 : Training: loss:  0.1444075\n",
      "1086 : Training: loss:  0.1545478\n",
      "1087 : Training: loss:  0.14925116\n",
      "1088 : Training: loss:  0.14727543\n",
      "1089 : Training: loss:  0.14910364\n",
      "1090 : Training: loss:  0.13363948\n",
      "1091 : Training: loss:  0.14447014\n",
      "1092 : Training: loss:  0.13160679\n",
      "1093 : Training: loss:  0.14407708\n",
      "1094 : Training: loss:  0.15278384\n",
      "1095 : Training: loss:  0.1246648\n",
      "1096 : Training: loss:  0.110535115\n",
      "1097 : Training: loss:  0.14555629\n",
      "1098 : Training: loss:  0.14212675\n",
      "1099 : Training: loss:  0.13264431\n",
      "1100 : Training: loss:  0.1254473\n",
      "Validation: Loss:  0.14007033  Accuracy:  0.23076923\n",
      "1101 : Training: loss:  0.15012972\n",
      "1102 : Training: loss:  0.120214745\n",
      "1103 : Training: loss:  0.13474561\n",
      "1104 : Training: loss:  0.14757864\n",
      "1105 : Training: loss:  0.11540808\n",
      "1106 : Training: loss:  0.15032282\n",
      "1107 : Training: loss:  0.14165121\n",
      "1108 : Training: loss:  0.16444133\n",
      "1109 : Training: loss:  0.13446897\n",
      "1110 : Training: loss:  0.14904872\n",
      "1111 : Training: loss:  0.14188811\n",
      "1112 : Training: loss:  0.13995369\n",
      "1113 : Training: loss:  0.1200988\n",
      "1114 : Training: loss:  0.13969089\n",
      "1115 : Training: loss:  0.12529196\n",
      "1116 : Training: loss:  0.13326557\n",
      "1117 : Training: loss:  0.15705596\n",
      "1118 : Training: loss:  0.14364426\n",
      "1119 : Training: loss:  0.1349229\n",
      "1120 : Training: loss:  0.1497338\n",
      "Validation: Loss:  0.13937932  Accuracy:  0.23076923\n",
      "1121 : Training: loss:  0.159074\n",
      "1122 : Training: loss:  0.1458763\n",
      "1123 : Training: loss:  0.14792876\n",
      "1124 : Training: loss:  0.15908076\n",
      "1125 : Training: loss:  0.16169369\n",
      "1126 : Training: loss:  0.13444284\n",
      "1127 : Training: loss:  0.13730466\n",
      "1128 : Training: loss:  0.13047177\n",
      "1129 : Training: loss:  0.1596939\n",
      "1130 : Training: loss:  0.14238627\n",
      "1131 : Training: loss:  0.1477031\n",
      "1132 : Training: loss:  0.14884779\n",
      "1133 : Training: loss:  0.15943466\n",
      "1134 : Training: loss:  0.16028832\n",
      "1135 : Training: loss:  0.15373583\n",
      "1136 : Training: loss:  0.1508454\n",
      "1137 : Training: loss:  0.13503088\n",
      "1138 : Training: loss:  0.14627117\n",
      "1139 : Training: loss:  0.11655947\n",
      "1140 : Training: loss:  0.16068697\n",
      "Validation: Loss:  0.13896598  Accuracy:  0.23076923\n",
      "1141 : Training: loss:  0.14643145\n",
      "1142 : Training: loss:  0.12150393\n",
      "1143 : Training: loss:  0.16076508\n",
      "1144 : Training: loss:  0.13442494\n",
      "1145 : Training: loss:  0.14747058\n",
      "1146 : Training: loss:  0.14984034\n",
      "1147 : Training: loss:  0.15136977\n",
      "1148 : Training: loss:  0.1411847\n",
      "1149 : Training: loss:  0.15527956\n",
      "1150 : Training: loss:  0.13031088\n",
      "1151 : Training: loss:  0.14595903\n",
      "1152 : Training: loss:  0.15031844\n",
      "1153 : Training: loss:  0.114834644\n",
      "1154 : Training: loss:  0.1037531\n",
      "1155 : Training: loss:  0.13836853\n",
      "1156 : Training: loss:  0.15564406\n",
      "1157 : Training: loss:  0.12295226\n",
      "1158 : Training: loss:  0.1604853\n",
      "1159 : Training: loss:  0.14588039\n",
      "1160 : Training: loss:  0.14412937\n",
      "Validation: Loss:  0.13857938  Accuracy:  0.23076923\n",
      "1161 : Training: loss:  0.14117762\n",
      "1162 : Training: loss:  0.1297862\n",
      "1163 : Training: loss:  0.11807426\n",
      "1164 : Training: loss:  0.09834606\n",
      "1165 : Training: loss:  0.1536337\n",
      "1166 : Training: loss:  0.14538437\n",
      "1167 : Training: loss:  0.15025407\n",
      "1168 : Training: loss:  0.11851421\n",
      "1169 : Training: loss:  0.13238165\n",
      "1170 : Training: loss:  0.15207168\n",
      "1171 : Training: loss:  0.15103014\n",
      "1172 : Training: loss:  0.1356888\n",
      "1173 : Training: loss:  0.14659937\n",
      "1174 : Training: loss:  0.11950169\n",
      "1175 : Training: loss:  0.121473245\n",
      "1176 : Training: loss:  0.13963805\n",
      "1177 : Training: loss:  0.14483435\n",
      "1178 : Training: loss:  0.13980095\n",
      "1179 : Training: loss:  0.13754664\n",
      "1180 : Training: loss:  0.16278413\n",
      "Validation: Loss:  0.13803719  Accuracy:  0.25\n",
      "1181 : Training: loss:  0.15151165\n",
      "1182 : Training: loss:  0.15533772\n",
      "1183 : Training: loss:  0.14061525\n",
      "1184 : Training: loss:  0.12843458\n",
      "1185 : Training: loss:  0.16063678\n",
      "1186 : Training: loss:  0.11722307\n",
      "1187 : Training: loss:  0.1436341\n",
      "1188 : Training: loss:  0.12015921\n",
      "1189 : Training: loss:  0.12107912\n",
      "1190 : Training: loss:  0.14084734\n",
      "1191 : Training: loss:  0.14226717\n",
      "1192 : Training: loss:  0.14516698\n",
      "1193 : Training: loss:  0.1472555\n",
      "1194 : Training: loss:  0.13593364\n",
      "1195 : Training: loss:  0.124449536\n",
      "1196 : Training: loss:  0.1522132\n",
      "1197 : Training: loss:  0.1224054\n",
      "1198 : Training: loss:  0.15404193\n",
      "1199 : Training: loss:  0.15251009\n",
      "1200 : Training: loss:  0.16221313\n",
      "Validation: Loss:  0.1376925  Accuracy:  0.28846154\n",
      "1201 : Training: loss:  0.14171049\n",
      "1202 : Training: loss:  0.14600818\n",
      "1203 : Training: loss:  0.150778\n",
      "1204 : Training: loss:  0.13991931\n",
      "1205 : Training: loss:  0.1578396\n",
      "1206 : Training: loss:  0.14406691\n",
      "1207 : Training: loss:  0.14503458\n",
      "1208 : Training: loss:  0.109675646\n",
      "1209 : Training: loss:  0.14525376\n",
      "1210 : Training: loss:  0.13672188\n",
      "1211 : Training: loss:  0.10906495\n",
      "1212 : Training: loss:  0.14571995\n",
      "1213 : Training: loss:  0.11017328\n",
      "1214 : Training: loss:  0.13961431\n",
      "1215 : Training: loss:  0.13737823\n",
      "1216 : Training: loss:  0.14375482\n",
      "1217 : Training: loss:  0.13828701\n",
      "1218 : Training: loss:  0.13815552\n",
      "1219 : Training: loss:  0.13535763\n",
      "1220 : Training: loss:  0.115386724\n",
      "Validation: Loss:  0.13743746  Accuracy:  0.34615386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0577, 0.049, 0.0314, 0.0516, 0.0355, 0.04, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.137437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0551, 0.0497, 0.0291, 0.045, 0.0373, 0.0445...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0591, 0.0586, 0.0338, 0.0558, 0.0353, 0.045...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0371, 0.0356, 0.0184, 0.0297, 0.0228, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0507, 0.0469, 0.0265, 0.0469, 0.0306, 0.037...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0513, 0.0568, 0.0346, 0.0779, 0.0349, 0.042...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0451, 0.0529, 0.0227, 0.0974, 0.03, 0.0377,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0542, 0.0679, 0.0291, 0.1158, 0.0387, 0.048...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0866, 0.0745, 0.0555, 0.1024, 0.0634, 0.070...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0467, 0.0438, 0.0209, 0.0308, 0.0403, 0.040...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0347, 0.0477, 0.0152, 0.0271, 0.0286, 0.032...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0444, 0.052, 0.0256, 0.035, 0.0266, 0.0423,...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0373, 0.0733, 0.0366, 0.0385, 0.026, 0.0326...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0424, 0.1205, 0.0684, 0.0574, 0.0414, 0.045...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0368, 0.0377, 0.0162, 0.0277, 0.0249, 0.029...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0594, 0.0527, 0.0325, 0.0267, 0.03, 0.0424,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0512, 0.0525, 0.0338, 0.0703, 0.0368, 0.041...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0449, 0.0527, 0.0312, 0.0564, 0.0339, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0575, 0.0551, 0.0295, 0.0452, 0.0394, 0.050...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0675, 0.0617, 0.0337, 0.0432, 0.0438, 0.058...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0577, 0.0487, 0.0344, 0.0441, 0.0367, 0.046...</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0342, 0.0425, 0.0182, 0.0439, 0.0208, 0.030...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0474, 0.0382, 0.0237, 0.0316, 0.0292, 0.037...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0582, 0.058, 0.0329, 0.0325, 0.0384, 0.0533...</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0542, 0.0469, 0.0288, 0.0569, 0.0338, 0.041...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0531, 0.0496, 0.0289, 0.0539, 0.0371, 0.036...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0545, 0.0503, 0.0272, 0.0483, 0.0382, 0.038...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0424, 0.0445, 0.0234, 0.0456, 0.0321, 0.033...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0324, 0.0587, 0.0286, 0.0361, 0.0312, 0.031...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.029, 0.0453, 0.0206, 0.0272, 0.0249, 0.0251...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0944, 0.1209, 0.1072, 0.0137, 0.0519, 0.091...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0515, 0.052, 0.0349, 0.0609, 0.0361, 0.0398...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0321, 0.0351, 0.0187, 0.0412, 0.0215, 0.024...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0437, 0.0427, 0.023, 0.0455, 0.0278, 0.0327...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0567, 0.0505, 0.0319, 0.0404, 0.0342, 0.045...</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0352, 0.0329, 0.0165, 0.0358, 0.0199, 0.025...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0412, 0.0498, 0.0246, 0.0655, 0.0251, 0.034...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0211, 0.0512, 0.0188, 0.0515, 0.0107, 0.023...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0717, 0.0765, 0.0304, 0.0872, 0.0576, 0.058...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0556, 0.0575, 0.0256, 0.0693, 0.0401, 0.045...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0462, 0.0517, 0.0246, 0.0426, 0.0311, 0.032...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0487, 0.0497, 0.0247, 0.0676, 0.031, 0.0375...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0272, 0.0446, 0.0191, 0.0535, 0.0152, 0.024...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0163, 0.0513, 0.0166, 0.0587, 0.0069, 0.017...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0168, 0.0521, 0.0188, 0.0595, 0.0073, 0.016...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0221, 0.044, 0.0171, 0.0573, 0.0118, 0.0205...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0695, 0.0638, 0.049, 0.0494, 0.049, 0.0496,...</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.053, 0.044, 0.0292, 0.0483, 0.0315, 0.0385,...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0451, 0.0477, 0.0224, 0.0277, 0.0334, 0.034...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0722, 0.0797, 0.042, 0.0377, 0.0423, 0.0648...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.056, 0.0521, 0.0348, 0.0372, 0.0368, 0.0452...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0598, 0.0564, 0.0346, 0.0481, 0.0377, 0.051...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0577, 0.049, 0.0314, 0.0516, 0.0355, 0.04, ...               0   \n",
       "1   [0.0551, 0.0497, 0.0291, 0.045, 0.0373, 0.0445...               0   \n",
       "2   [0.0591, 0.0586, 0.0338, 0.0558, 0.0353, 0.045...               0   \n",
       "3   [0.0371, 0.0356, 0.0184, 0.0297, 0.0228, 0.029...               1   \n",
       "4   [0.0507, 0.0469, 0.0265, 0.0469, 0.0306, 0.037...               1   \n",
       "5   [0.0513, 0.0568, 0.0346, 0.0779, 0.0349, 0.042...               2   \n",
       "6   [0.0451, 0.0529, 0.0227, 0.0974, 0.03, 0.0377,...               3   \n",
       "7   [0.0542, 0.0679, 0.0291, 0.1158, 0.0387, 0.048...               3   \n",
       "8   [0.0866, 0.0745, 0.0555, 0.1024, 0.0634, 0.070...               3   \n",
       "9   [0.0467, 0.0438, 0.0209, 0.0308, 0.0403, 0.040...               4   \n",
       "10  [0.0347, 0.0477, 0.0152, 0.0271, 0.0286, 0.032...               4   \n",
       "11  [0.0444, 0.052, 0.0256, 0.035, 0.0266, 0.0423,...               5   \n",
       "12  [0.0373, 0.0733, 0.0366, 0.0385, 0.026, 0.0326...               6   \n",
       "13  [0.0424, 0.1205, 0.0684, 0.0574, 0.0414, 0.045...               7   \n",
       "14  [0.0368, 0.0377, 0.0162, 0.0277, 0.0249, 0.029...               8   \n",
       "15  [0.0594, 0.0527, 0.0325, 0.0267, 0.03, 0.0424,...               8   \n",
       "16  [0.0512, 0.0525, 0.0338, 0.0703, 0.0368, 0.041...               9   \n",
       "17  [0.0449, 0.0527, 0.0312, 0.0564, 0.0339, 0.036...               9   \n",
       "18  [0.0575, 0.0551, 0.0295, 0.0452, 0.0394, 0.050...              10   \n",
       "19  [0.0675, 0.0617, 0.0337, 0.0432, 0.0438, 0.058...              10   \n",
       "20  [0.0577, 0.0487, 0.0344, 0.0441, 0.0367, 0.046...              11   \n",
       "21  [0.0342, 0.0425, 0.0182, 0.0439, 0.0208, 0.030...              11   \n",
       "22  [0.0474, 0.0382, 0.0237, 0.0316, 0.0292, 0.037...              12   \n",
       "23  [0.0582, 0.058, 0.0329, 0.0325, 0.0384, 0.0533...              13   \n",
       "24  [0.0542, 0.0469, 0.0288, 0.0569, 0.0338, 0.041...              13   \n",
       "25  [0.0531, 0.0496, 0.0289, 0.0539, 0.0371, 0.036...              14   \n",
       "26  [0.0545, 0.0503, 0.0272, 0.0483, 0.0382, 0.038...              14   \n",
       "27  [0.0424, 0.0445, 0.0234, 0.0456, 0.0321, 0.033...              15   \n",
       "28  [0.0324, 0.0587, 0.0286, 0.0361, 0.0312, 0.031...              15   \n",
       "29  [0.029, 0.0453, 0.0206, 0.0272, 0.0249, 0.0251...              15   \n",
       "30  [0.0944, 0.1209, 0.1072, 0.0137, 0.0519, 0.091...              16   \n",
       "31  [0.0515, 0.052, 0.0349, 0.0609, 0.0361, 0.0398...              17   \n",
       "32  [0.0321, 0.0351, 0.0187, 0.0412, 0.0215, 0.024...              17   \n",
       "33  [0.0437, 0.0427, 0.023, 0.0455, 0.0278, 0.0327...              18   \n",
       "34  [0.0567, 0.0505, 0.0319, 0.0404, 0.0342, 0.045...              19   \n",
       "35  [0.0352, 0.0329, 0.0165, 0.0358, 0.0199, 0.025...              20   \n",
       "36  [0.0412, 0.0498, 0.0246, 0.0655, 0.0251, 0.034...              21   \n",
       "37  [0.0211, 0.0512, 0.0188, 0.0515, 0.0107, 0.023...              21   \n",
       "38  [0.0717, 0.0765, 0.0304, 0.0872, 0.0576, 0.058...              22   \n",
       "39  [0.0556, 0.0575, 0.0256, 0.0693, 0.0401, 0.045...              22   \n",
       "40  [0.0462, 0.0517, 0.0246, 0.0426, 0.0311, 0.032...              22   \n",
       "41  [0.0487, 0.0497, 0.0247, 0.0676, 0.031, 0.0375...              22   \n",
       "42  [0.0272, 0.0446, 0.0191, 0.0535, 0.0152, 0.024...              23   \n",
       "43  [0.0163, 0.0513, 0.0166, 0.0587, 0.0069, 0.017...              23   \n",
       "44  [0.0168, 0.0521, 0.0188, 0.0595, 0.0073, 0.016...              23   \n",
       "45  [0.0221, 0.044, 0.0171, 0.0573, 0.0118, 0.0205...              23   \n",
       "46  [0.0695, 0.0638, 0.049, 0.0494, 0.049, 0.0496,...              24   \n",
       "47  [0.053, 0.044, 0.0292, 0.0483, 0.0315, 0.0385,...              24   \n",
       "48  [0.0451, 0.0477, 0.0224, 0.0277, 0.0334, 0.034...              25   \n",
       "49  [0.0722, 0.0797, 0.042, 0.0377, 0.0423, 0.0648...              25   \n",
       "50  [0.056, 0.0521, 0.0348, 0.0372, 0.0368, 0.0452...              26   \n",
       "51  [0.0598, 0.0564, 0.0346, 0.0481, 0.0377, 0.051...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                 22  0.346154  0.137437  \n",
       "1                  8       NaN       NaN  \n",
       "2                 22       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                 22       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                 8       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                23       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                22       NaN       NaN  \n",
       "20                 8       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                22       NaN       NaN  \n",
       "23                 8       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                22       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                 8       NaN       NaN  \n",
       "35                22       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 8       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                 8       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                 8       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221 : Training: loss:  0.09870754\n",
      "1222 : Training: loss:  0.1426552\n",
      "1223 : Training: loss:  0.1447107\n",
      "1224 : Training: loss:  0.14177872\n",
      "1225 : Training: loss:  0.13816258\n",
      "1226 : Training: loss:  0.114047565\n",
      "1227 : Training: loss:  0.105681\n",
      "1228 : Training: loss:  0.15649071\n",
      "1229 : Training: loss:  0.13682097\n",
      "1230 : Training: loss:  0.13251841\n",
      "1231 : Training: loss:  0.15378113\n",
      "1232 : Training: loss:  0.13743217\n",
      "1233 : Training: loss:  0.12982713\n",
      "1234 : Training: loss:  0.12347262\n",
      "1235 : Training: loss:  0.10145092\n",
      "1236 : Training: loss:  0.121855624\n",
      "1237 : Training: loss:  0.123324126\n",
      "1238 : Training: loss:  0.13517915\n",
      "1239 : Training: loss:  0.13279517\n",
      "1240 : Training: loss:  0.119711116\n",
      "Validation: Loss:  0.13689654  Accuracy:  0.34615386\n",
      "1241 : Training: loss:  0.14865293\n",
      "1242 : Training: loss:  0.13207103\n",
      "1243 : Training: loss:  0.15100424\n",
      "1244 : Training: loss:  0.13372265\n",
      "1245 : Training: loss:  0.14557542\n",
      "1246 : Training: loss:  0.12530267\n",
      "1247 : Training: loss:  0.13316567\n",
      "1248 : Training: loss:  0.14948694\n",
      "1249 : Training: loss:  0.13350755\n",
      "1250 : Training: loss:  0.11820372\n",
      "1251 : Training: loss:  0.120910086\n",
      "1252 : Training: loss:  0.12208141\n",
      "1253 : Training: loss:  0.15745567\n",
      "1254 : Training: loss:  0.13870396\n",
      "1255 : Training: loss:  0.13767582\n",
      "1256 : Training: loss:  0.10510402\n",
      "1257 : Training: loss:  0.10889551\n",
      "1258 : Training: loss:  0.12004501\n",
      "1259 : Training: loss:  0.12559485\n",
      "1260 : Training: loss:  0.13056327\n",
      "Validation: Loss:  0.13653754  Accuracy:  0.32692307\n",
      "1261 : Training: loss:  0.10518816\n",
      "1262 : Training: loss:  0.14900137\n",
      "1263 : Training: loss:  0.15074289\n",
      "1264 : Training: loss:  0.14057724\n",
      "1265 : Training: loss:  0.12693115\n",
      "1266 : Training: loss:  0.1433466\n",
      "1267 : Training: loss:  0.11579902\n",
      "1268 : Training: loss:  0.13838346\n",
      "1269 : Training: loss:  0.09467446\n",
      "1270 : Training: loss:  0.13352616\n",
      "1271 : Training: loss:  0.14393084\n",
      "1272 : Training: loss:  0.11793435\n",
      "1273 : Training: loss:  0.1334705\n",
      "1274 : Training: loss:  0.112381905\n",
      "1275 : Training: loss:  0.10602402\n",
      "1276 : Training: loss:  0.15832163\n",
      "1277 : Training: loss:  0.13581397\n",
      "1278 : Training: loss:  0.1447068\n",
      "1279 : Training: loss:  0.14071636\n",
      "1280 : Training: loss:  0.1414022\n",
      "Validation: Loss:  0.13623872  Accuracy:  0.32692307\n",
      "1281 : Training: loss:  0.16457435\n",
      "1282 : Training: loss:  0.14002131\n",
      "1283 : Training: loss:  0.13964574\n",
      "1284 : Training: loss:  0.12934399\n",
      "1285 : Training: loss:  0.14438869\n",
      "1286 : Training: loss:  0.12505972\n",
      "1287 : Training: loss:  0.1261747\n",
      "1288 : Training: loss:  0.11701255\n",
      "1289 : Training: loss:  0.15337928\n",
      "1290 : Training: loss:  0.11689008\n",
      "1291 : Training: loss:  0.11341429\n",
      "1292 : Training: loss:  0.13277625\n",
      "1293 : Training: loss:  0.14611138\n",
      "1294 : Training: loss:  0.15175927\n",
      "1295 : Training: loss:  0.080570236\n",
      "1296 : Training: loss:  0.13343918\n",
      "1297 : Training: loss:  0.14931296\n",
      "1298 : Training: loss:  0.13846737\n",
      "1299 : Training: loss:  0.16001035\n",
      "1300 : Training: loss:  0.12227378\n",
      "Validation: Loss:  0.13597447  Accuracy:  0.28846154\n",
      "1301 : Training: loss:  0.13446777\n",
      "1302 : Training: loss:  0.11695651\n",
      "1303 : Training: loss:  0.15216589\n",
      "1304 : Training: loss:  0.1268473\n",
      "1305 : Training: loss:  0.13755468\n",
      "1306 : Training: loss:  0.123013124\n",
      "1307 : Training: loss:  0.1033312\n",
      "1308 : Training: loss:  0.12883353\n",
      "1309 : Training: loss:  0.13400863\n",
      "1310 : Training: loss:  0.13087067\n",
      "1311 : Training: loss:  0.12986061\n",
      "1312 : Training: loss:  0.14667194\n",
      "1313 : Training: loss:  0.15725529\n",
      "1314 : Training: loss:  0.15325049\n",
      "1315 : Training: loss:  0.14301264\n",
      "1316 : Training: loss:  0.14275366\n",
      "1317 : Training: loss:  0.105932675\n",
      "1318 : Training: loss:  0.115289204\n",
      "1319 : Training: loss:  0.12504473\n",
      "1320 : Training: loss:  0.15841722\n",
      "Validation: Loss:  0.13547121  Accuracy:  0.28846154\n",
      "1321 : Training: loss:  0.14722581\n",
      "1322 : Training: loss:  0.1376481\n",
      "1323 : Training: loss:  0.14348434\n",
      "1324 : Training: loss:  0.12522122\n",
      "1325 : Training: loss:  0.13356858\n",
      "1326 : Training: loss:  0.142739\n",
      "1327 : Training: loss:  0.15661007\n",
      "1328 : Training: loss:  0.1458504\n",
      "1329 : Training: loss:  0.12826937\n",
      "1330 : Training: loss:  0.13920864\n",
      "1331 : Training: loss:  0.1454428\n",
      "1332 : Training: loss:  0.14415711\n",
      "1333 : Training: loss:  0.107225336\n",
      "1334 : Training: loss:  0.13885812\n",
      "1335 : Training: loss:  0.14476328\n",
      "1336 : Training: loss:  0.121419765\n",
      "1337 : Training: loss:  0.14726803\n",
      "1338 : Training: loss:  0.13508385\n",
      "1339 : Training: loss:  0.15262315\n",
      "1340 : Training: loss:  0.12745534\n",
      "Validation: Loss:  0.13518268  Accuracy:  0.32692307\n",
      "1341 : Training: loss:  0.14157672\n",
      "1342 : Training: loss:  0.13572478\n",
      "1343 : Training: loss:  0.09281067\n",
      "1344 : Training: loss:  0.14362605\n",
      "1345 : Training: loss:  0.13860609\n",
      "1346 : Training: loss:  0.11968512\n",
      "1347 : Training: loss:  0.14771685\n",
      "1348 : Training: loss:  0.13229309\n",
      "1349 : Training: loss:  0.15060748\n",
      "1350 : Training: loss:  0.12567222\n",
      "1351 : Training: loss:  0.14189723\n",
      "1352 : Training: loss:  0.12174657\n",
      "1353 : Training: loss:  0.1626092\n",
      "1354 : Training: loss:  0.14897558\n",
      "1355 : Training: loss:  0.153084\n",
      "1356 : Training: loss:  0.13885278\n",
      "1357 : Training: loss:  0.105766125\n",
      "1358 : Training: loss:  0.11952391\n",
      "1359 : Training: loss:  0.16185333\n",
      "1360 : Training: loss:  0.14341028\n",
      "Validation: Loss:  0.13473913  Accuracy:  0.34615386\n",
      "1361 : Training: loss:  0.14005531\n",
      "1362 : Training: loss:  0.14607902\n",
      "1363 : Training: loss:  0.15475576\n",
      "1364 : Training: loss:  0.12516145\n",
      "1365 : Training: loss:  0.13567579\n",
      "1366 : Training: loss:  0.14165618\n",
      "1367 : Training: loss:  0.12693144\n",
      "1368 : Training: loss:  0.13515778\n",
      "1369 : Training: loss:  0.14831764\n",
      "1370 : Training: loss:  0.12079454\n",
      "1371 : Training: loss:  0.13904329\n",
      "1372 : Training: loss:  0.1258376\n",
      "1373 : Training: loss:  0.11136503\n",
      "1374 : Training: loss:  0.1400218\n",
      "1375 : Training: loss:  0.11708253\n",
      "1376 : Training: loss:  0.14585336\n",
      "1377 : Training: loss:  0.1444976\n",
      "1378 : Training: loss:  0.13542654\n",
      "1379 : Training: loss:  0.1307988\n",
      "1380 : Training: loss:  0.1170023\n",
      "Validation: Loss:  0.13422629  Accuracy:  0.3846154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0555, 0.0391, 0.0255, 0.0609, 0.0315, 0.041...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.134226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0527, 0.0401, 0.0231, 0.0515, 0.0338, 0.049...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0551, 0.0475, 0.0265, 0.0635, 0.0303, 0.047...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0347, 0.0272, 0.0136, 0.0339, 0.0196, 0.031...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0484, 0.0371, 0.0209, 0.0558, 0.0269, 0.039...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.046, 0.0433, 0.0272, 0.0981, 0.0293, 0.0418...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0404, 0.0401, 0.0168, 0.141, 0.0254, 0.0379...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0474, 0.0517, 0.0213, 0.1619, 0.0327, 0.047...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0818, 0.0598, 0.0462, 0.1251, 0.0572, 0.072...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0446, 0.0347, 0.0156, 0.0327, 0.0383, 0.046...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0313, 0.038, 0.0106, 0.0297, 0.0257, 0.0371...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0396, 0.041, 0.0189, 0.0366, 0.0221, 0.046,...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0308, 0.055, 0.0263, 0.0394, 0.0206, 0.0306...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0296, 0.0814, 0.0446, 0.0494, 0.0301, 0.034...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0331, 0.0284, 0.0112, 0.0313, 0.0212, 0.031...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0567, 0.0431, 0.0251, 0.0261, 0.0258, 0.046...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0465, 0.0399, 0.0269, 0.0868, 0.0316, 0.041...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0401, 0.0394, 0.0244, 0.0653, 0.0291, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0551, 0.0448, 0.0234, 0.0502, 0.0358, 0.056...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0673, 0.052, 0.0277, 0.0471, 0.0413, 0.0675...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0547, 0.0382, 0.0278, 0.0483, 0.0324, 0.048...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0295, 0.0309, 0.0128, 0.0528, 0.0166, 0.030...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0455, 0.0294, 0.0184, 0.0348, 0.0262, 0.041...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0571, 0.0488, 0.0273, 0.0332, 0.0356, 0.061...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0519, 0.0367, 0.0231, 0.0702, 0.0298, 0.044...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0497, 0.0385, 0.0224, 0.0634, 0.0324, 0.037...</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0518, 0.0394, 0.0211, 0.0561, 0.0341, 0.039...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0382, 0.0334, 0.0172, 0.0535, 0.0277, 0.034...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0258, 0.0413, 0.0198, 0.0358, 0.0255, 0.028...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0236, 0.032, 0.0137, 0.0269, 0.0202, 0.0235...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1005, 0.1148, 0.0901, 0.0121, 0.0626, 0.104...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.047, 0.0401, 0.0282, 0.0728, 0.0314, 0.0401...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.029, 0.0263, 0.0145, 0.052, 0.0184, 0.025, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0408, 0.0332, 0.0178, 0.0563, 0.0242, 0.034...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0523, 0.0395, 0.0244, 0.0432, 0.0292, 0.047...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0332, 0.0248, 0.0124, 0.0446, 0.0169, 0.026...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0359, 0.0376, 0.0184, 0.0828, 0.0204, 0.034...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0146, 0.032, 0.0113, 0.0559, 0.0069, 0.019,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0668, 0.0619, 0.0221, 0.1074, 0.0518, 0.059...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0508, 0.0447, 0.0187, 0.0868, 0.0347, 0.047...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0408, 0.039, 0.0175, 0.047, 0.0258, 0.0314,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0449, 0.0384, 0.0187, 0.0881, 0.0265, 0.038...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0224, 0.032, 0.0136, 0.0692, 0.0116, 0.023,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0118, 0.0322, 0.0101, 0.0742, 0.0045, 0.014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0119, 0.0312, 0.0111, 0.07, 0.0047, 0.0132,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0174, 0.0299, 0.0115, 0.0759, 0.0086, 0.018...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0642, 0.0497, 0.0397, 0.0496, 0.0428, 0.048...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0506, 0.0343, 0.0236, 0.0575, 0.0277, 0.040...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0428, 0.0389, 0.0169, 0.0291, 0.0305, 0.038...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0671, 0.0676, 0.032, 0.0363, 0.0365, 0.0715...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0525, 0.0417, 0.027, 0.0376, 0.032, 0.0483,...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0563, 0.0455, 0.0274, 0.0517, 0.0328, 0.055...</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0555, 0.0391, 0.0255, 0.0609, 0.0315, 0.041...               0   \n",
       "1   [0.0527, 0.0401, 0.0231, 0.0515, 0.0338, 0.049...               0   \n",
       "2   [0.0551, 0.0475, 0.0265, 0.0635, 0.0303, 0.047...               0   \n",
       "3   [0.0347, 0.0272, 0.0136, 0.0339, 0.0196, 0.031...               1   \n",
       "4   [0.0484, 0.0371, 0.0209, 0.0558, 0.0269, 0.039...               1   \n",
       "5   [0.046, 0.0433, 0.0272, 0.0981, 0.0293, 0.0418...               2   \n",
       "6   [0.0404, 0.0401, 0.0168, 0.141, 0.0254, 0.0379...               3   \n",
       "7   [0.0474, 0.0517, 0.0213, 0.1619, 0.0327, 0.047...               3   \n",
       "8   [0.0818, 0.0598, 0.0462, 0.1251, 0.0572, 0.072...               3   \n",
       "9   [0.0446, 0.0347, 0.0156, 0.0327, 0.0383, 0.046...               4   \n",
       "10  [0.0313, 0.038, 0.0106, 0.0297, 0.0257, 0.0371...               4   \n",
       "11  [0.0396, 0.041, 0.0189, 0.0366, 0.0221, 0.046,...               5   \n",
       "12  [0.0308, 0.055, 0.0263, 0.0394, 0.0206, 0.0306...               6   \n",
       "13  [0.0296, 0.0814, 0.0446, 0.0494, 0.0301, 0.034...               7   \n",
       "14  [0.0331, 0.0284, 0.0112, 0.0313, 0.0212, 0.031...               8   \n",
       "15  [0.0567, 0.0431, 0.0251, 0.0261, 0.0258, 0.046...               8   \n",
       "16  [0.0465, 0.0399, 0.0269, 0.0868, 0.0316, 0.041...               9   \n",
       "17  [0.0401, 0.0394, 0.0244, 0.0653, 0.0291, 0.036...               9   \n",
       "18  [0.0551, 0.0448, 0.0234, 0.0502, 0.0358, 0.056...              10   \n",
       "19  [0.0673, 0.052, 0.0277, 0.0471, 0.0413, 0.0675...              10   \n",
       "20  [0.0547, 0.0382, 0.0278, 0.0483, 0.0324, 0.048...              11   \n",
       "21  [0.0295, 0.0309, 0.0128, 0.0528, 0.0166, 0.030...              11   \n",
       "22  [0.0455, 0.0294, 0.0184, 0.0348, 0.0262, 0.041...              12   \n",
       "23  [0.0571, 0.0488, 0.0273, 0.0332, 0.0356, 0.061...              13   \n",
       "24  [0.0519, 0.0367, 0.0231, 0.0702, 0.0298, 0.044...              13   \n",
       "25  [0.0497, 0.0385, 0.0224, 0.0634, 0.0324, 0.037...              14   \n",
       "26  [0.0518, 0.0394, 0.0211, 0.0561, 0.0341, 0.039...              14   \n",
       "27  [0.0382, 0.0334, 0.0172, 0.0535, 0.0277, 0.034...              15   \n",
       "28  [0.0258, 0.0413, 0.0198, 0.0358, 0.0255, 0.028...              15   \n",
       "29  [0.0236, 0.032, 0.0137, 0.0269, 0.0202, 0.0235...              15   \n",
       "30  [0.1005, 0.1148, 0.0901, 0.0121, 0.0626, 0.104...              16   \n",
       "31  [0.047, 0.0401, 0.0282, 0.0728, 0.0314, 0.0401...              17   \n",
       "32  [0.029, 0.0263, 0.0145, 0.052, 0.0184, 0.025, ...              17   \n",
       "33  [0.0408, 0.0332, 0.0178, 0.0563, 0.0242, 0.034...              18   \n",
       "34  [0.0523, 0.0395, 0.0244, 0.0432, 0.0292, 0.047...              19   \n",
       "35  [0.0332, 0.0248, 0.0124, 0.0446, 0.0169, 0.026...              20   \n",
       "36  [0.0359, 0.0376, 0.0184, 0.0828, 0.0204, 0.034...              21   \n",
       "37  [0.0146, 0.032, 0.0113, 0.0559, 0.0069, 0.019,...              21   \n",
       "38  [0.0668, 0.0619, 0.0221, 0.1074, 0.0518, 0.059...              22   \n",
       "39  [0.0508, 0.0447, 0.0187, 0.0868, 0.0347, 0.047...              22   \n",
       "40  [0.0408, 0.039, 0.0175, 0.047, 0.0258, 0.0314,...              22   \n",
       "41  [0.0449, 0.0384, 0.0187, 0.0881, 0.0265, 0.038...              22   \n",
       "42  [0.0224, 0.032, 0.0136, 0.0692, 0.0116, 0.023,...              23   \n",
       "43  [0.0118, 0.0322, 0.0101, 0.0742, 0.0045, 0.014...              23   \n",
       "44  [0.0119, 0.0312, 0.0111, 0.07, 0.0047, 0.0132,...              23   \n",
       "45  [0.0174, 0.0299, 0.0115, 0.0759, 0.0086, 0.018...              23   \n",
       "46  [0.0642, 0.0497, 0.0397, 0.0496, 0.0428, 0.048...              24   \n",
       "47  [0.0506, 0.0343, 0.0236, 0.0575, 0.0277, 0.040...              24   \n",
       "48  [0.0428, 0.0389, 0.0169, 0.0291, 0.0305, 0.038...              25   \n",
       "49  [0.0671, 0.0676, 0.032, 0.0363, 0.0365, 0.0715...              25   \n",
       "50  [0.0525, 0.0417, 0.027, 0.0376, 0.032, 0.0483,...              26   \n",
       "51  [0.0563, 0.0455, 0.0274, 0.0517, 0.0328, 0.055...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                 13  0.384615  0.134226  \n",
       "1                 10       NaN       NaN  \n",
       "2                 22       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                 22       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                 10       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                17       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                13       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                22       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                17       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                22       NaN       NaN  \n",
       "35                 3       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                13       NaN       NaN  \n",
       "47                 9       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                13       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381 : Training: loss:  0.12788023\n",
      "1382 : Training: loss:  0.13507928\n",
      "1383 : Training: loss:  0.114964\n",
      "1384 : Training: loss:  0.158498\n",
      "1385 : Training: loss:  0.12987107\n",
      "1386 : Training: loss:  0.13185619\n",
      "1387 : Training: loss:  0.16269876\n",
      "1388 : Training: loss:  0.13004158\n",
      "1389 : Training: loss:  0.13318418\n",
      "1390 : Training: loss:  0.14778225\n",
      "1391 : Training: loss:  0.14856058\n",
      "1392 : Training: loss:  0.14672619\n",
      "1393 : Training: loss:  0.15226614\n",
      "1394 : Training: loss:  0.08830799\n",
      "1395 : Training: loss:  0.13264285\n",
      "1396 : Training: loss:  0.11931014\n",
      "1397 : Training: loss:  0.14905012\n",
      "1398 : Training: loss:  0.16499795\n",
      "1399 : Training: loss:  0.14287063\n",
      "1400 : Training: loss:  0.15084049\n",
      "Validation: Loss:  0.13362518  Accuracy:  0.40384614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0597, 0.0375, 0.0256, 0.0592, 0.0321, 0.043...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.133625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0568, 0.0386, 0.0231, 0.0498, 0.0347, 0.050...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0591, 0.0458, 0.0265, 0.0618, 0.0309, 0.049...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.038, 0.0262, 0.0138, 0.0328, 0.0206, 0.0331...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0525, 0.0358, 0.0211, 0.0545, 0.0276, 0.040...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0488, 0.0415, 0.0271, 0.097, 0.0297, 0.0427...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0434, 0.0387, 0.0169, 0.142, 0.0263, 0.0392...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0506, 0.0499, 0.0213, 0.1631, 0.0337, 0.049...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0864, 0.0576, 0.0463, 0.123, 0.0581, 0.0739...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0486, 0.0335, 0.0157, 0.0316, 0.0405, 0.048...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0344, 0.0369, 0.0107, 0.0289, 0.0277, 0.039...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0428, 0.0398, 0.019, 0.0353, 0.0229, 0.0481...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0327, 0.0539, 0.0268, 0.0382, 0.0216, 0.031...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.03, 0.0768, 0.0428, 0.0475, 0.0305, 0.0338,...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0361, 0.0273, 0.0113, 0.0302, 0.0223, 0.032...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0606, 0.0413, 0.0249, 0.0246, 0.0262, 0.048...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0497, 0.0384, 0.027, 0.0857, 0.0323, 0.0423...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0429, 0.0379, 0.0245, 0.0641, 0.03, 0.0371,...</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0598, 0.0435, 0.0236, 0.0489, 0.0374, 0.058...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0732, 0.0509, 0.0281, 0.0457, 0.0433, 0.070...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0591, 0.037, 0.0282, 0.0469, 0.0334, 0.0508...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0321, 0.03, 0.0129, 0.0522, 0.0173, 0.0314,...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0498, 0.0284, 0.0187, 0.0337, 0.0274, 0.043...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.062, 0.0477, 0.0278, 0.0321, 0.0373, 0.0647...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0559, 0.0353, 0.0232, 0.0689, 0.0305, 0.045...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0531, 0.0367, 0.0223, 0.0615, 0.0331, 0.037...</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0561, 0.0381, 0.0213, 0.0546, 0.0355, 0.040...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0411, 0.0321, 0.0173, 0.0523, 0.0287, 0.035...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0269, 0.039, 0.0193, 0.0347, 0.0262, 0.0286...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0253, 0.0308, 0.0137, 0.0259, 0.0211, 0.024...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.105, 0.1121, 0.0894, 0.0115, 0.0657, 0.1067...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0502, 0.0387, 0.0284, 0.0713, 0.0322, 0.041...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0312, 0.0252, 0.0145, 0.0508, 0.0189, 0.025...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0444, 0.032, 0.018, 0.0553, 0.0251, 0.0356,...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0567, 0.0384, 0.0248, 0.0419, 0.0302, 0.049...</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0362, 0.0237, 0.0124, 0.0434, 0.0175, 0.027...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0386, 0.0362, 0.0185, 0.0821, 0.021, 0.0356...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0155, 0.0308, 0.0113, 0.0555, 0.0072, 0.019...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0719, 0.0602, 0.0224, 0.1063, 0.0547, 0.062...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.055, 0.0433, 0.019, 0.086, 0.0364, 0.049, 0...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0439, 0.0374, 0.0176, 0.0453, 0.0268, 0.032...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0486, 0.037, 0.0189, 0.0872, 0.0275, 0.0397...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0241, 0.0308, 0.0137, 0.0687, 0.0119, 0.023...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0125, 0.0308, 0.01, 0.0741, 0.0047, 0.0151,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0127, 0.0297, 0.011, 0.0697, 0.0049, 0.0136...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0187, 0.0288, 0.0116, 0.076, 0.0088, 0.0192...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0682, 0.0481, 0.0401, 0.0478, 0.0439, 0.05,...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0543, 0.0328, 0.0237, 0.0558, 0.0282, 0.041...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0465, 0.0376, 0.0171, 0.0279, 0.0322, 0.040...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0715, 0.0658, 0.0319, 0.0346, 0.0375, 0.074...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0545, 0.0389, 0.026, 0.0353, 0.0317, 0.0485...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0594, 0.0433, 0.0269, 0.0496, 0.033, 0.0566...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0597, 0.0375, 0.0256, 0.0592, 0.0321, 0.043...               0   \n",
       "1   [0.0568, 0.0386, 0.0231, 0.0498, 0.0347, 0.050...               0   \n",
       "2   [0.0591, 0.0458, 0.0265, 0.0618, 0.0309, 0.049...               0   \n",
       "3   [0.038, 0.0262, 0.0138, 0.0328, 0.0206, 0.0331...               1   \n",
       "4   [0.0525, 0.0358, 0.0211, 0.0545, 0.0276, 0.040...               1   \n",
       "5   [0.0488, 0.0415, 0.0271, 0.097, 0.0297, 0.0427...               2   \n",
       "6   [0.0434, 0.0387, 0.0169, 0.142, 0.0263, 0.0392...               3   \n",
       "7   [0.0506, 0.0499, 0.0213, 0.1631, 0.0337, 0.049...               3   \n",
       "8   [0.0864, 0.0576, 0.0463, 0.123, 0.0581, 0.0739...               3   \n",
       "9   [0.0486, 0.0335, 0.0157, 0.0316, 0.0405, 0.048...               4   \n",
       "10  [0.0344, 0.0369, 0.0107, 0.0289, 0.0277, 0.039...               4   \n",
       "11  [0.0428, 0.0398, 0.019, 0.0353, 0.0229, 0.0481...               5   \n",
       "12  [0.0327, 0.0539, 0.0268, 0.0382, 0.0216, 0.031...               6   \n",
       "13  [0.03, 0.0768, 0.0428, 0.0475, 0.0305, 0.0338,...               7   \n",
       "14  [0.0361, 0.0273, 0.0113, 0.0302, 0.0223, 0.032...               8   \n",
       "15  [0.0606, 0.0413, 0.0249, 0.0246, 0.0262, 0.048...               8   \n",
       "16  [0.0497, 0.0384, 0.027, 0.0857, 0.0323, 0.0423...               9   \n",
       "17  [0.0429, 0.0379, 0.0245, 0.0641, 0.03, 0.0371,...               9   \n",
       "18  [0.0598, 0.0435, 0.0236, 0.0489, 0.0374, 0.058...              10   \n",
       "19  [0.0732, 0.0509, 0.0281, 0.0457, 0.0433, 0.070...              10   \n",
       "20  [0.0591, 0.037, 0.0282, 0.0469, 0.0334, 0.0508...              11   \n",
       "21  [0.0321, 0.03, 0.0129, 0.0522, 0.0173, 0.0314,...              11   \n",
       "22  [0.0498, 0.0284, 0.0187, 0.0337, 0.0274, 0.043...              12   \n",
       "23  [0.062, 0.0477, 0.0278, 0.0321, 0.0373, 0.0647...              13   \n",
       "24  [0.0559, 0.0353, 0.0232, 0.0689, 0.0305, 0.045...              13   \n",
       "25  [0.0531, 0.0367, 0.0223, 0.0615, 0.0331, 0.037...              14   \n",
       "26  [0.0561, 0.0381, 0.0213, 0.0546, 0.0355, 0.040...              14   \n",
       "27  [0.0411, 0.0321, 0.0173, 0.0523, 0.0287, 0.035...              15   \n",
       "28  [0.0269, 0.039, 0.0193, 0.0347, 0.0262, 0.0286...              15   \n",
       "29  [0.0253, 0.0308, 0.0137, 0.0259, 0.0211, 0.024...              15   \n",
       "30  [0.105, 0.1121, 0.0894, 0.0115, 0.0657, 0.1067...              16   \n",
       "31  [0.0502, 0.0387, 0.0284, 0.0713, 0.0322, 0.041...              17   \n",
       "32  [0.0312, 0.0252, 0.0145, 0.0508, 0.0189, 0.025...              17   \n",
       "33  [0.0444, 0.032, 0.018, 0.0553, 0.0251, 0.0356,...              18   \n",
       "34  [0.0567, 0.0384, 0.0248, 0.0419, 0.0302, 0.049...              19   \n",
       "35  [0.0362, 0.0237, 0.0124, 0.0434, 0.0175, 0.027...              20   \n",
       "36  [0.0386, 0.0362, 0.0185, 0.0821, 0.021, 0.0356...              21   \n",
       "37  [0.0155, 0.0308, 0.0113, 0.0555, 0.0072, 0.019...              21   \n",
       "38  [0.0719, 0.0602, 0.0224, 0.1063, 0.0547, 0.062...              22   \n",
       "39  [0.055, 0.0433, 0.019, 0.086, 0.0364, 0.049, 0...              22   \n",
       "40  [0.0439, 0.0374, 0.0176, 0.0453, 0.0268, 0.032...              22   \n",
       "41  [0.0486, 0.037, 0.0189, 0.0872, 0.0275, 0.0397...              22   \n",
       "42  [0.0241, 0.0308, 0.0137, 0.0687, 0.0119, 0.023...              23   \n",
       "43  [0.0125, 0.0308, 0.01, 0.0741, 0.0047, 0.0151,...              23   \n",
       "44  [0.0127, 0.0297, 0.011, 0.0697, 0.0049, 0.0136...              23   \n",
       "45  [0.0187, 0.0288, 0.0116, 0.076, 0.0088, 0.0192...              23   \n",
       "46  [0.0682, 0.0481, 0.0401, 0.0478, 0.0439, 0.05,...              24   \n",
       "47  [0.0543, 0.0328, 0.0237, 0.0558, 0.0282, 0.041...              24   \n",
       "48  [0.0465, 0.0376, 0.0171, 0.0279, 0.0322, 0.040...              25   \n",
       "49  [0.0715, 0.0658, 0.0319, 0.0346, 0.0375, 0.074...              25   \n",
       "50  [0.0545, 0.0389, 0.026, 0.0353, 0.0317, 0.0485...              26   \n",
       "51  [0.0594, 0.0433, 0.0269, 0.0496, 0.033, 0.0566...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                 13  0.403846  0.133625  \n",
       "1                 10       NaN       NaN  \n",
       "2                 13       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                 22       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                 8       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                17       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                13       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                17       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                 8       NaN       NaN  \n",
       "35                 3       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                13       NaN       NaN  \n",
       "47                 9       NaN       NaN  \n",
       "48                 8       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                 8       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401 : Training: loss:  0.140913\n",
      "1402 : Training: loss:  0.11679629\n",
      "1403 : Training: loss:  0.12518594\n",
      "1404 : Training: loss:  0.1282637\n",
      "1405 : Training: loss:  0.115657814\n",
      "1406 : Training: loss:  0.139233\n",
      "1407 : Training: loss:  0.13015485\n",
      "1408 : Training: loss:  0.12125281\n",
      "1409 : Training: loss:  0.10672272\n",
      "1410 : Training: loss:  0.15686986\n",
      "1411 : Training: loss:  0.12071886\n",
      "1412 : Training: loss:  0.122725755\n",
      "1413 : Training: loss:  0.1318937\n",
      "1414 : Training: loss:  0.14181364\n",
      "1415 : Training: loss:  0.15556325\n",
      "1416 : Training: loss:  0.11621446\n",
      "1417 : Training: loss:  0.11615265\n",
      "1418 : Training: loss:  0.13114889\n",
      "1419 : Training: loss:  0.13753293\n",
      "1420 : Training: loss:  0.13581665\n",
      "Validation: Loss:  0.13320042  Accuracy:  0.3846154\n",
      "1421 : Training: loss:  0.15646683\n",
      "1422 : Training: loss:  0.15346424\n",
      "1423 : Training: loss:  0.11877747\n",
      "1424 : Training: loss:  0.13916734\n",
      "1425 : Training: loss:  0.12899426\n",
      "1426 : Training: loss:  0.14775088\n",
      "1427 : Training: loss:  0.15292536\n",
      "1428 : Training: loss:  0.100369446\n",
      "1429 : Training: loss:  0.14278956\n",
      "1430 : Training: loss:  0.13790008\n",
      "1431 : Training: loss:  0.14696854\n",
      "1432 : Training: loss:  0.14986579\n",
      "1433 : Training: loss:  0.1274847\n",
      "1434 : Training: loss:  0.12715025\n",
      "1435 : Training: loss:  0.15019779\n",
      "1436 : Training: loss:  0.15063994\n",
      "1437 : Training: loss:  0.13671969\n",
      "1438 : Training: loss:  0.1538243\n",
      "1439 : Training: loss:  0.15077509\n",
      "1440 : Training: loss:  0.14544745\n",
      "Validation: Loss:  0.13290523  Accuracy:  0.40384614\n",
      "1441 : Training: loss:  0.15214284\n",
      "1442 : Training: loss:  0.10988323\n",
      "1443 : Training: loss:  0.117752805\n",
      "1444 : Training: loss:  0.11602273\n",
      "1445 : Training: loss:  0.15308625\n",
      "1446 : Training: loss:  0.14334162\n",
      "1447 : Training: loss:  0.13976267\n",
      "1448 : Training: loss:  0.11721069\n",
      "1449 : Training: loss:  0.11530995\n",
      "1450 : Training: loss:  0.13646755\n",
      "1451 : Training: loss:  0.122215234\n",
      "1452 : Training: loss:  0.10536843\n",
      "1453 : Training: loss:  0.11282465\n",
      "1454 : Training: loss:  0.15403894\n",
      "1455 : Training: loss:  0.15087229\n",
      "1456 : Training: loss:  0.1451833\n",
      "1457 : Training: loss:  0.12219163\n",
      "1458 : Training: loss:  0.1263712\n",
      "1459 : Training: loss:  0.12162603\n",
      "1460 : Training: loss:  0.12609446\n",
      "Validation: Loss:  0.13213593  Accuracy:  0.3846154\n",
      "1461 : Training: loss:  0.13753484\n",
      "1462 : Training: loss:  0.13078517\n",
      "1463 : Training: loss:  0.12109317\n",
      "1464 : Training: loss:  0.15082762\n",
      "1465 : Training: loss:  0.1382822\n",
      "1466 : Training: loss:  0.12674429\n",
      "1467 : Training: loss:  0.1338895\n",
      "1468 : Training: loss:  0.113810845\n",
      "1469 : Training: loss:  0.14718525\n",
      "1470 : Training: loss:  0.1368119\n",
      "1471 : Training: loss:  0.14170085\n",
      "1472 : Training: loss:  0.10782651\n",
      "1473 : Training: loss:  0.08712142\n",
      "1474 : Training: loss:  0.15874194\n",
      "1475 : Training: loss:  0.12706463\n",
      "1476 : Training: loss:  0.15532169\n",
      "1477 : Training: loss:  0.13386118\n",
      "1478 : Training: loss:  0.1393843\n",
      "1479 : Training: loss:  0.14403158\n",
      "1480 : Training: loss:  0.09491773\n",
      "Validation: Loss:  0.13154621  Accuracy:  0.40384614\n",
      "1481 : Training: loss:  0.096706875\n",
      "1482 : Training: loss:  0.11306081\n",
      "1483 : Training: loss:  0.12090245\n",
      "1484 : Training: loss:  0.15096495\n",
      "1485 : Training: loss:  0.12756093\n",
      "1486 : Training: loss:  0.15330642\n",
      "1487 : Training: loss:  0.14348234\n",
      "1488 : Training: loss:  0.104317956\n",
      "1489 : Training: loss:  0.13439338\n",
      "1490 : Training: loss:  0.11669393\n",
      "1491 : Training: loss:  0.15290862\n",
      "1492 : Training: loss:  0.11994367\n",
      "1493 : Training: loss:  0.14922403\n",
      "1494 : Training: loss:  0.1324982\n",
      "1495 : Training: loss:  0.13172433\n",
      "1496 : Training: loss:  0.123367675\n",
      "1497 : Training: loss:  0.1461835\n",
      "1498 : Training: loss:  0.14434026\n",
      "1499 : Training: loss:  0.13316259\n",
      "1500 : Training: loss:  0.10205499\n",
      "Validation: Loss:  0.13114324  Accuracy:  0.40384614\n",
      "1501 : Training: loss:  0.10067337\n",
      "1502 : Training: loss:  0.11376789\n",
      "1503 : Training: loss:  0.14116602\n",
      "1504 : Training: loss:  0.13609445\n",
      "1505 : Training: loss:  0.12344798\n",
      "1506 : Training: loss:  0.11999836\n",
      "1507 : Training: loss:  0.09917096\n",
      "1508 : Training: loss:  0.14716579\n",
      "1509 : Training: loss:  0.12758814\n",
      "1510 : Training: loss:  0.13926466\n",
      "1511 : Training: loss:  0.118779376\n",
      "1512 : Training: loss:  0.14679994\n",
      "1513 : Training: loss:  0.11579226\n",
      "1514 : Training: loss:  0.13121031\n",
      "1515 : Training: loss:  0.105943024\n",
      "1516 : Training: loss:  0.096034825\n",
      "1517 : Training: loss:  0.1180483\n",
      "1518 : Training: loss:  0.12192587\n",
      "1519 : Training: loss:  0.15896589\n",
      "1520 : Training: loss:  0.14786111\n",
      "Validation: Loss:  0.13045041  Accuracy:  0.40384614\n",
      "1521 : Training: loss:  0.09932598\n",
      "1522 : Training: loss:  0.10573468\n",
      "1523 : Training: loss:  0.11804864\n",
      "1524 : Training: loss:  0.13341197\n",
      "1525 : Training: loss:  0.14405712\n",
      "1526 : Training: loss:  0.13161539\n",
      "1527 : Training: loss:  0.11676074\n",
      "1528 : Training: loss:  0.13067703\n",
      "1529 : Training: loss:  0.12060324\n",
      "1530 : Training: loss:  0.14696892\n",
      "1531 : Training: loss:  0.120873064\n",
      "1532 : Training: loss:  0.12469223\n",
      "1533 : Training: loss:  0.13896929\n",
      "1534 : Training: loss:  0.14351414\n",
      "1535 : Training: loss:  0.1434469\n",
      "1536 : Training: loss:  0.1281694\n",
      "1537 : Training: loss:  0.13685496\n",
      "1538 : Training: loss:  0.11307568\n",
      "1539 : Training: loss:  0.12051532\n",
      "1540 : Training: loss:  0.11628755\n",
      "Validation: Loss:  0.12950937  Accuracy:  0.42307693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0728, 0.0397, 0.0331, 0.067, 0.0398, 0.0373...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.129509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0685, 0.0421, 0.0282, 0.0534, 0.044, 0.045,...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0699, 0.0503, 0.033, 0.0683, 0.0376, 0.0436...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0467, 0.029, 0.0177, 0.0356, 0.0275, 0.0289...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0638, 0.0384, 0.027, 0.0622, 0.0347, 0.0356...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0545, 0.0401, 0.0336, 0.1162, 0.0341, 0.034...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0489, 0.038, 0.0211, 0.1951, 0.0322, 0.0323...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0554, 0.0491, 0.0256, 0.22, 0.0408, 0.0407,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0959, 0.0555, 0.0552, 0.1407, 0.0667, 0.063...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0605, 0.0387, 0.0186, 0.0323, 0.0584, 0.045...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0422, 0.0453, 0.0125, 0.0304, 0.0413, 0.036...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0502, 0.0463, 0.0225, 0.0359, 0.0287, 0.043...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0336, 0.0535, 0.0286, 0.0367, 0.0239, 0.024...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0267, 0.0668, 0.0412, 0.0437, 0.0312, 0.024...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0436, 0.0309, 0.0139, 0.0331, 0.0314, 0.028...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0748, 0.0496, 0.0303, 0.0234, 0.0333, 0.044...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0563, 0.0371, 0.0334, 0.0999, 0.0379, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0476, 0.0365, 0.0296, 0.0703, 0.0351, 0.030...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0714, 0.0484, 0.0287, 0.052, 0.0482, 0.0532...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0883, 0.0583, 0.0336, 0.0468, 0.0567, 0.065...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0716, 0.0394, 0.0363, 0.0502, 0.0424, 0.045...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0369, 0.0316, 0.0161, 0.0632, 0.0218, 0.026...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0621, 0.0313, 0.0249, 0.0367, 0.037, 0.0396...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0746, 0.0561, 0.034, 0.0312, 0.049, 0.0609,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0663, 0.0361, 0.0295, 0.08, 0.0375, 0.0391,...</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0615, 0.0369, 0.0271, 0.0685, 0.0397, 0.031...</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0678, 0.0398, 0.0268, 0.0605, 0.0458, 0.035...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0468, 0.0323, 0.0206, 0.0579, 0.0359, 0.029...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0269, 0.0368, 0.0205, 0.0354, 0.0304, 0.022...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0271, 0.0314, 0.0153, 0.0262, 0.0261, 0.019...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1147, 0.1287, 0.0955, 0.0094, 0.0977, 0.096...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0573, 0.0383, 0.0355, 0.0804, 0.0379, 0.034...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0363, 0.0254, 0.0185, 0.0605, 0.0229, 0.020...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0542, 0.0343, 0.0236, 0.0664, 0.0327, 0.030...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0687, 0.043, 0.0317, 0.0447, 0.0388, 0.0453...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0439, 0.0248, 0.0159, 0.0515, 0.0222, 0.022...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0437, 0.0368, 0.023, 0.1002, 0.025, 0.0295,...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.015, 0.0293, 0.0121, 0.0625, 0.0077, 0.0146...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.082, 0.0639, 0.0266, 0.1272, 0.0736, 0.0553...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0643, 0.0457, 0.0234, 0.1051, 0.0479, 0.042...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0504, 0.039, 0.0213, 0.0492, 0.0337, 0.0268...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0569, 0.0378, 0.024, 0.1085, 0.0345, 0.0335...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0263, 0.0304, 0.0168, 0.0854, 0.0137, 0.018...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0121, 0.0275, 0.011, 0.0919, 0.0049, 0.0106...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0121, 0.0253, 0.0116, 0.0822, 0.005, 0.0093...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0193, 0.0271, 0.0138, 0.0979, 0.0098, 0.014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0762, 0.0479, 0.0487, 0.0468, 0.0512, 0.042...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0654, 0.0338, 0.0308, 0.0635, 0.0345, 0.035...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0562, 0.0438, 0.0206, 0.0275, 0.0446, 0.036...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0838, 0.0806, 0.0359, 0.0327, 0.0463, 0.069...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0614, 0.0411, 0.0296, 0.0335, 0.0368, 0.041...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0682, 0.0464, 0.0317, 0.0504, 0.0389, 0.049...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0728, 0.0397, 0.0331, 0.067, 0.0398, 0.0373...               0   \n",
       "1   [0.0685, 0.0421, 0.0282, 0.0534, 0.044, 0.045,...               0   \n",
       "2   [0.0699, 0.0503, 0.033, 0.0683, 0.0376, 0.0436...               0   \n",
       "3   [0.0467, 0.029, 0.0177, 0.0356, 0.0275, 0.0289...               1   \n",
       "4   [0.0638, 0.0384, 0.027, 0.0622, 0.0347, 0.0356...               1   \n",
       "5   [0.0545, 0.0401, 0.0336, 0.1162, 0.0341, 0.034...               2   \n",
       "6   [0.0489, 0.038, 0.0211, 0.1951, 0.0322, 0.0323...               3   \n",
       "7   [0.0554, 0.0491, 0.0256, 0.22, 0.0408, 0.0407,...               3   \n",
       "8   [0.0959, 0.0555, 0.0552, 0.1407, 0.0667, 0.063...               3   \n",
       "9   [0.0605, 0.0387, 0.0186, 0.0323, 0.0584, 0.045...               4   \n",
       "10  [0.0422, 0.0453, 0.0125, 0.0304, 0.0413, 0.036...               4   \n",
       "11  [0.0502, 0.0463, 0.0225, 0.0359, 0.0287, 0.043...               5   \n",
       "12  [0.0336, 0.0535, 0.0286, 0.0367, 0.0239, 0.024...               6   \n",
       "13  [0.0267, 0.0668, 0.0412, 0.0437, 0.0312, 0.024...               7   \n",
       "14  [0.0436, 0.0309, 0.0139, 0.0331, 0.0314, 0.028...               8   \n",
       "15  [0.0748, 0.0496, 0.0303, 0.0234, 0.0333, 0.044...               8   \n",
       "16  [0.0563, 0.0371, 0.0334, 0.0999, 0.0379, 0.034...               9   \n",
       "17  [0.0476, 0.0365, 0.0296, 0.0703, 0.0351, 0.030...               9   \n",
       "18  [0.0714, 0.0484, 0.0287, 0.052, 0.0482, 0.0532...              10   \n",
       "19  [0.0883, 0.0583, 0.0336, 0.0468, 0.0567, 0.065...              10   \n",
       "20  [0.0716, 0.0394, 0.0363, 0.0502, 0.0424, 0.045...              11   \n",
       "21  [0.0369, 0.0316, 0.0161, 0.0632, 0.0218, 0.026...              11   \n",
       "22  [0.0621, 0.0313, 0.0249, 0.0367, 0.037, 0.0396...              12   \n",
       "23  [0.0746, 0.0561, 0.034, 0.0312, 0.049, 0.0609,...              13   \n",
       "24  [0.0663, 0.0361, 0.0295, 0.08, 0.0375, 0.0391,...              13   \n",
       "25  [0.0615, 0.0369, 0.0271, 0.0685, 0.0397, 0.031...              14   \n",
       "26  [0.0678, 0.0398, 0.0268, 0.0605, 0.0458, 0.035...              14   \n",
       "27  [0.0468, 0.0323, 0.0206, 0.0579, 0.0359, 0.029...              15   \n",
       "28  [0.0269, 0.0368, 0.0205, 0.0354, 0.0304, 0.022...              15   \n",
       "29  [0.0271, 0.0314, 0.0153, 0.0262, 0.0261, 0.019...              15   \n",
       "30  [0.1147, 0.1287, 0.0955, 0.0094, 0.0977, 0.096...              16   \n",
       "31  [0.0573, 0.0383, 0.0355, 0.0804, 0.0379, 0.034...              17   \n",
       "32  [0.0363, 0.0254, 0.0185, 0.0605, 0.0229, 0.020...              17   \n",
       "33  [0.0542, 0.0343, 0.0236, 0.0664, 0.0327, 0.030...              18   \n",
       "34  [0.0687, 0.043, 0.0317, 0.0447, 0.0388, 0.0453...              19   \n",
       "35  [0.0439, 0.0248, 0.0159, 0.0515, 0.0222, 0.022...              20   \n",
       "36  [0.0437, 0.0368, 0.023, 0.1002, 0.025, 0.0295,...              21   \n",
       "37  [0.015, 0.0293, 0.0121, 0.0625, 0.0077, 0.0146...              21   \n",
       "38  [0.082, 0.0639, 0.0266, 0.1272, 0.0736, 0.0553...              22   \n",
       "39  [0.0643, 0.0457, 0.0234, 0.1051, 0.0479, 0.042...              22   \n",
       "40  [0.0504, 0.039, 0.0213, 0.0492, 0.0337, 0.0268...              22   \n",
       "41  [0.0569, 0.0378, 0.024, 0.1085, 0.0345, 0.0335...              22   \n",
       "42  [0.0263, 0.0304, 0.0168, 0.0854, 0.0137, 0.018...              23   \n",
       "43  [0.0121, 0.0275, 0.011, 0.0919, 0.0049, 0.0106...              23   \n",
       "44  [0.0121, 0.0253, 0.0116, 0.0822, 0.005, 0.0093...              23   \n",
       "45  [0.0193, 0.0271, 0.0138, 0.0979, 0.0098, 0.014...              23   \n",
       "46  [0.0762, 0.0479, 0.0487, 0.0468, 0.0512, 0.042...              24   \n",
       "47  [0.0654, 0.0338, 0.0308, 0.0635, 0.0345, 0.035...              24   \n",
       "48  [0.0562, 0.0438, 0.0206, 0.0275, 0.0446, 0.036...              25   \n",
       "49  [0.0838, 0.0806, 0.0359, 0.0327, 0.0463, 0.069...              25   \n",
       "50  [0.0614, 0.0411, 0.0296, 0.0335, 0.0368, 0.041...              26   \n",
       "51  [0.0682, 0.0464, 0.0317, 0.0504, 0.0389, 0.049...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.423077  0.129509  \n",
       "1                 10       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 3       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                22       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                 3       NaN       NaN  \n",
       "25                 3       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                 3       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 3       NaN       NaN  \n",
       "34                 0       NaN       NaN  \n",
       "35                 3       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                13       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                 8       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541 : Training: loss:  0.123813786\n",
      "1542 : Training: loss:  0.09924921\n",
      "1543 : Training: loss:  0.15626952\n",
      "1544 : Training: loss:  0.14524515\n",
      "1545 : Training: loss:  0.10403208\n",
      "1546 : Training: loss:  0.1279945\n",
      "1547 : Training: loss:  0.13491751\n",
      "1548 : Training: loss:  0.10796985\n",
      "1549 : Training: loss:  0.13479866\n",
      "1550 : Training: loss:  0.12206834\n",
      "1551 : Training: loss:  0.1416543\n",
      "1552 : Training: loss:  0.122056164\n",
      "1553 : Training: loss:  0.11385501\n",
      "1554 : Training: loss:  0.13832246\n",
      "1555 : Training: loss:  0.15152037\n",
      "1556 : Training: loss:  0.10289485\n",
      "1557 : Training: loss:  0.11994437\n",
      "1558 : Training: loss:  0.14117235\n",
      "1559 : Training: loss:  0.121914685\n",
      "1560 : Training: loss:  0.15730597\n",
      "Validation: Loss:  0.12873498  Accuracy:  0.44230768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0787, 0.0388, 0.0328, 0.0607, 0.0421, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.128735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.074, 0.0413, 0.0276, 0.0476, 0.0465, 0.0444...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0746, 0.0491, 0.0322, 0.0614, 0.0395, 0.042...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0506, 0.0284, 0.0173, 0.0312, 0.0297, 0.028...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0688, 0.0375, 0.0266, 0.0558, 0.0369, 0.035...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.058, 0.0388, 0.033, 0.1062, 0.0356, 0.0341,...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0522, 0.0371, 0.0207, 0.1813, 0.0343, 0.031...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0587, 0.048, 0.0251, 0.2052, 0.0432, 0.0399...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1016, 0.0541, 0.0546, 0.131, 0.0696, 0.0625...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0649, 0.0381, 0.0179, 0.0283, 0.0625, 0.044...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.045, 0.0451, 0.012, 0.0265, 0.0447, 0.0359,...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0535, 0.0453, 0.0218, 0.0315, 0.0302, 0.042...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0357, 0.054, 0.0286, 0.0332, 0.0254, 0.0246...</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0263, 0.0633, 0.0388, 0.039, 0.0314, 0.0231...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0473, 0.0306, 0.0136, 0.0291, 0.0343, 0.028...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0795, 0.0486, 0.0292, 0.0206, 0.035, 0.0432...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0603, 0.0361, 0.033, 0.0911, 0.0397, 0.0342...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0507, 0.0354, 0.0291, 0.0635, 0.0367, 0.029...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0761, 0.0472, 0.028, 0.0465, 0.0509, 0.0523...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0934, 0.0569, 0.0327, 0.0416, 0.0597, 0.064...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.077, 0.0385, 0.0359, 0.0452, 0.045, 0.0448,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0395, 0.0307, 0.0158, 0.0565, 0.0233, 0.025...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0671, 0.0305, 0.0245, 0.0328, 0.0398, 0.039...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0786, 0.0544, 0.033, 0.0275, 0.0514, 0.0595...</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0712, 0.035, 0.029, 0.0722, 0.0396, 0.0384,...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0658, 0.0358, 0.0265, 0.0617, 0.0418, 0.030...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0733, 0.0392, 0.0265, 0.0548, 0.0491, 0.034...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0501, 0.0315, 0.0201, 0.0516, 0.0381, 0.028...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0274, 0.0354, 0.0195, 0.0314, 0.0312, 0.021...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0284, 0.0308, 0.0147, 0.0231, 0.0273, 0.019...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1073, 0.1132, 0.0849, 0.0082, 0.0991, 0.082...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0615, 0.0375, 0.0352, 0.0736, 0.0398, 0.034...</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0394, 0.0249, 0.0184, 0.0549, 0.0243, 0.020...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.059, 0.0338, 0.0234, 0.06, 0.0352, 0.0306, ...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0741, 0.0424, 0.0313, 0.0401, 0.0416, 0.045...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0477, 0.0242, 0.0156, 0.046, 0.0238, 0.0223...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0465, 0.0356, 0.0225, 0.0908, 0.0262, 0.028...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0154, 0.028, 0.0115, 0.0552, 0.0079, 0.0139...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0868, 0.0634, 0.0261, 0.1173, 0.0795, 0.054...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.069, 0.045, 0.0231, 0.0959, 0.0517, 0.0422,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0538, 0.0382, 0.0208, 0.0442, 0.0359, 0.026...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.061, 0.0369, 0.0235, 0.0989, 0.0368, 0.0329...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0279, 0.0294, 0.0163, 0.0771, 0.0143, 0.018...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0126, 0.0263, 0.0105, 0.0824, 0.0051, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0126, 0.0243, 0.0112, 0.0737, 0.0052, 0.009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0202, 0.0261, 0.0133, 0.0882, 0.0102, 0.013...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0805, 0.0468, 0.0483, 0.0425, 0.0537, 0.041...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0704, 0.0329, 0.0304, 0.0574, 0.0363, 0.035...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0602, 0.0433, 0.02, 0.0241, 0.048, 0.0354, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0884, 0.0797, 0.0347, 0.0289, 0.0485, 0.068...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0639, 0.0393, 0.0279, 0.0295, 0.0376, 0.039...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0717, 0.0447, 0.0304, 0.0448, 0.0402, 0.047...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0787, 0.0388, 0.0328, 0.0607, 0.0421, 0.036...               0   \n",
       "1   [0.074, 0.0413, 0.0276, 0.0476, 0.0465, 0.0444...               0   \n",
       "2   [0.0746, 0.0491, 0.0322, 0.0614, 0.0395, 0.042...               0   \n",
       "3   [0.0506, 0.0284, 0.0173, 0.0312, 0.0297, 0.028...               1   \n",
       "4   [0.0688, 0.0375, 0.0266, 0.0558, 0.0369, 0.035...               1   \n",
       "5   [0.058, 0.0388, 0.033, 0.1062, 0.0356, 0.0341,...               2   \n",
       "6   [0.0522, 0.0371, 0.0207, 0.1813, 0.0343, 0.031...               3   \n",
       "7   [0.0587, 0.048, 0.0251, 0.2052, 0.0432, 0.0399...               3   \n",
       "8   [0.1016, 0.0541, 0.0546, 0.131, 0.0696, 0.0625...               3   \n",
       "9   [0.0649, 0.0381, 0.0179, 0.0283, 0.0625, 0.044...               4   \n",
       "10  [0.045, 0.0451, 0.012, 0.0265, 0.0447, 0.0359,...               4   \n",
       "11  [0.0535, 0.0453, 0.0218, 0.0315, 0.0302, 0.042...               5   \n",
       "12  [0.0357, 0.054, 0.0286, 0.0332, 0.0254, 0.0246...               6   \n",
       "13  [0.0263, 0.0633, 0.0388, 0.039, 0.0314, 0.0231...               7   \n",
       "14  [0.0473, 0.0306, 0.0136, 0.0291, 0.0343, 0.028...               8   \n",
       "15  [0.0795, 0.0486, 0.0292, 0.0206, 0.035, 0.0432...               8   \n",
       "16  [0.0603, 0.0361, 0.033, 0.0911, 0.0397, 0.0342...               9   \n",
       "17  [0.0507, 0.0354, 0.0291, 0.0635, 0.0367, 0.029...               9   \n",
       "18  [0.0761, 0.0472, 0.028, 0.0465, 0.0509, 0.0523...              10   \n",
       "19  [0.0934, 0.0569, 0.0327, 0.0416, 0.0597, 0.064...              10   \n",
       "20  [0.077, 0.0385, 0.0359, 0.0452, 0.045, 0.0448,...              11   \n",
       "21  [0.0395, 0.0307, 0.0158, 0.0565, 0.0233, 0.025...              11   \n",
       "22  [0.0671, 0.0305, 0.0245, 0.0328, 0.0398, 0.039...              12   \n",
       "23  [0.0786, 0.0544, 0.033, 0.0275, 0.0514, 0.0595...              13   \n",
       "24  [0.0712, 0.035, 0.029, 0.0722, 0.0396, 0.0384,...              13   \n",
       "25  [0.0658, 0.0358, 0.0265, 0.0617, 0.0418, 0.030...              14   \n",
       "26  [0.0733, 0.0392, 0.0265, 0.0548, 0.0491, 0.034...              14   \n",
       "27  [0.0501, 0.0315, 0.0201, 0.0516, 0.0381, 0.028...              15   \n",
       "28  [0.0274, 0.0354, 0.0195, 0.0314, 0.0312, 0.021...              15   \n",
       "29  [0.0284, 0.0308, 0.0147, 0.0231, 0.0273, 0.019...              15   \n",
       "30  [0.1073, 0.1132, 0.0849, 0.0082, 0.0991, 0.082...              16   \n",
       "31  [0.0615, 0.0375, 0.0352, 0.0736, 0.0398, 0.034...              17   \n",
       "32  [0.0394, 0.0249, 0.0184, 0.0549, 0.0243, 0.020...              17   \n",
       "33  [0.059, 0.0338, 0.0234, 0.06, 0.0352, 0.0306, ...              18   \n",
       "34  [0.0741, 0.0424, 0.0313, 0.0401, 0.0416, 0.045...              19   \n",
       "35  [0.0477, 0.0242, 0.0156, 0.046, 0.0238, 0.0223...              20   \n",
       "36  [0.0465, 0.0356, 0.0225, 0.0908, 0.0262, 0.028...              21   \n",
       "37  [0.0154, 0.028, 0.0115, 0.0552, 0.0079, 0.0139...              21   \n",
       "38  [0.0868, 0.0634, 0.0261, 0.1173, 0.0795, 0.054...              22   \n",
       "39  [0.069, 0.045, 0.0231, 0.0959, 0.0517, 0.0422,...              22   \n",
       "40  [0.0538, 0.0382, 0.0208, 0.0442, 0.0359, 0.026...              22   \n",
       "41  [0.061, 0.0369, 0.0235, 0.0989, 0.0368, 0.0329...              22   \n",
       "42  [0.0279, 0.0294, 0.0163, 0.0771, 0.0143, 0.018...              23   \n",
       "43  [0.0126, 0.0263, 0.0105, 0.0824, 0.0051, 0.010...              23   \n",
       "44  [0.0126, 0.0243, 0.0112, 0.0737, 0.0052, 0.009...              23   \n",
       "45  [0.0202, 0.0261, 0.0133, 0.0882, 0.0102, 0.013...              23   \n",
       "46  [0.0805, 0.0468, 0.0483, 0.0425, 0.0537, 0.041...              24   \n",
       "47  [0.0704, 0.0329, 0.0304, 0.0574, 0.0363, 0.035...              24   \n",
       "48  [0.0602, 0.0433, 0.02, 0.0241, 0.048, 0.0354, ...              25   \n",
       "49  [0.0884, 0.0797, 0.0347, 0.0289, 0.0485, 0.068...              25   \n",
       "50  [0.0639, 0.0393, 0.0279, 0.0295, 0.0376, 0.039...              26   \n",
       "51  [0.0717, 0.0447, 0.0304, 0.0448, 0.0402, 0.047...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.442308  0.128735  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                15       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                 0       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                16       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                 0       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                 9       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                 0       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 9       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                 8       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                 8       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561 : Training: loss:  0.12446575\n",
      "1562 : Training: loss:  0.1356131\n",
      "1563 : Training: loss:  0.13517413\n",
      "1564 : Training: loss:  0.14954609\n",
      "1565 : Training: loss:  0.13341688\n",
      "1566 : Training: loss:  0.116191156\n",
      "1567 : Training: loss:  0.08967544\n",
      "1568 : Training: loss:  0.12766312\n",
      "1569 : Training: loss:  0.13847902\n",
      "1570 : Training: loss:  0.12787399\n",
      "1571 : Training: loss:  0.120458044\n",
      "1572 : Training: loss:  0.13354023\n",
      "1573 : Training: loss:  0.13492474\n",
      "1574 : Training: loss:  0.10679528\n",
      "1575 : Training: loss:  0.1457874\n",
      "1576 : Training: loss:  0.14446871\n",
      "1577 : Training: loss:  0.10951157\n",
      "1578 : Training: loss:  0.110105\n",
      "1579 : Training: loss:  0.12745367\n",
      "1580 : Training: loss:  0.09427832\n",
      "Validation: Loss:  0.12820382  Accuracy:  0.44230768\n",
      "1581 : Training: loss:  0.13640383\n",
      "1582 : Training: loss:  0.13208875\n",
      "1583 : Training: loss:  0.1389139\n",
      "1584 : Training: loss:  0.14522988\n",
      "1585 : Training: loss:  0.10300903\n",
      "1586 : Training: loss:  0.15184115\n",
      "1587 : Training: loss:  0.09586669\n",
      "1588 : Training: loss:  0.1153602\n",
      "1589 : Training: loss:  0.13011388\n",
      "1590 : Training: loss:  0.12638734\n",
      "1591 : Training: loss:  0.13973162\n",
      "1592 : Training: loss:  0.15411526\n",
      "1593 : Training: loss:  0.13687819\n",
      "1594 : Training: loss:  0.13324153\n",
      "1595 : Training: loss:  0.09038388\n",
      "1596 : Training: loss:  0.09876818\n",
      "1597 : Training: loss:  0.12549934\n",
      "1598 : Training: loss:  0.108649746\n",
      "1599 : Training: loss:  0.14655356\n",
      "1600 : Training: loss:  0.12984897\n",
      "Validation: Loss:  0.1277087  Accuracy:  0.44230768\n",
      "1601 : Training: loss:  0.12387107\n",
      "1602 : Training: loss:  0.13588332\n",
      "1603 : Training: loss:  0.12935811\n",
      "1604 : Training: loss:  0.15486372\n",
      "1605 : Training: loss:  0.13560547\n",
      "1606 : Training: loss:  0.13030793\n",
      "1607 : Training: loss:  0.15364607\n",
      "1608 : Training: loss:  0.092657566\n",
      "1609 : Training: loss:  0.12879878\n",
      "1610 : Training: loss:  0.14311624\n",
      "1611 : Training: loss:  0.12544087\n",
      "1612 : Training: loss:  0.12932512\n",
      "1613 : Training: loss:  0.14036123\n",
      "1614 : Training: loss:  0.15753916\n",
      "1615 : Training: loss:  0.081052385\n",
      "1616 : Training: loss:  0.14245565\n",
      "1617 : Training: loss:  0.12571678\n",
      "1618 : Training: loss:  0.13582216\n",
      "1619 : Training: loss:  0.14135724\n",
      "1620 : Training: loss:  0.11869275\n",
      "Validation: Loss:  0.1274172  Accuracy:  0.48076922\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0798, 0.0373, 0.0442, 0.0496, 0.0425, 0.042...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.127417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0744, 0.0395, 0.036, 0.0379, 0.047, 0.0529,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0741, 0.0471, 0.0419, 0.0501, 0.0394, 0.050...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0496, 0.0264, 0.0229, 0.0237, 0.0299, 0.033...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0687, 0.0357, 0.0356, 0.0448, 0.037, 0.041,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0568, 0.0366, 0.0445, 0.09, 0.035, 0.038, 0...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0518, 0.0355, 0.0287, 0.1578, 0.0348, 0.035...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0574, 0.0456, 0.0336, 0.179, 0.0436, 0.0444...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1016, 0.0516, 0.0708, 0.1124, 0.0698, 0.069...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0639, 0.0362, 0.0219, 0.0217, 0.0649, 0.053...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0439, 0.0434, 0.0147, 0.0206, 0.0468, 0.044...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.052, 0.043, 0.0272, 0.0249, 0.0299, 0.0526,...</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0341, 0.0532, 0.036, 0.0285, 0.0258, 0.0291...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.023, 0.0562, 0.042, 0.0328, 0.0299, 0.0245,...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0465, 0.0291, 0.0177, 0.022, 0.0355, 0.0338...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0794, 0.0469, 0.0366, 0.0158, 0.035, 0.0531...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0597, 0.0342, 0.0445, 0.0766, 0.0395, 0.038...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0495, 0.0336, 0.0383, 0.0528, 0.0365, 0.033...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0739, 0.0439, 0.0349, 0.0369, 0.0509, 0.061...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0911, 0.0531, 0.04, 0.0329, 0.0601, 0.0759,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0757, 0.0359, 0.0462, 0.0357, 0.0447, 0.051...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0379, 0.0288, 0.021, 0.045, 0.0231, 0.0298,...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0658, 0.0282, 0.0319, 0.0249, 0.0402, 0.045...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0748, 0.0499, 0.0396, 0.0213, 0.0506, 0.070...</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0709, 0.0328, 0.0393, 0.0584, 0.0396, 0.043...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0657, 0.0341, 0.0354, 0.0504, 0.0423, 0.034...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0728, 0.0373, 0.0348, 0.0442, 0.0501, 0.039...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0491, 0.0296, 0.0265, 0.0407, 0.0383, 0.033...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0255, 0.0328, 0.023, 0.0255, 0.0312, 0.0247...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.027, 0.0291, 0.018, 0.0181, 0.0277, 0.0225,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0932, 0.0937, 0.0871, 0.0065, 0.0961, 0.089...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0617, 0.0364, 0.0477, 0.0622, 0.0402, 0.039...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0397, 0.0244, 0.0262, 0.0464, 0.0251, 0.024...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0591, 0.0323, 0.0319, 0.0482, 0.0357, 0.035...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0731, 0.04, 0.04, 0.0314, 0.0414, 0.0528, 0...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0481, 0.0231, 0.0219, 0.0365, 0.0243, 0.026...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0452, 0.0337, 0.0305, 0.0764, 0.0258, 0.033...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0142, 0.0267, 0.0153, 0.0471, 0.0077, 0.016...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0845, 0.0605, 0.0326, 0.0967, 0.0829, 0.060...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0671, 0.0424, 0.0299, 0.0779, 0.0528, 0.047...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0523, 0.0362, 0.027, 0.0349, 0.0362, 0.0299...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0603, 0.035, 0.032, 0.0817, 0.0372, 0.0374,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0269, 0.0282, 0.0226, 0.0653, 0.014, 0.021,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0116, 0.0246, 0.0142, 0.0696, 0.0049, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0118, 0.0226, 0.0148, 0.0613, 0.005, 0.0101...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0191, 0.0248, 0.0184, 0.0749, 0.0099, 0.015...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0772, 0.0434, 0.0602, 0.034, 0.0526, 0.0457...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0709, 0.0312, 0.0414, 0.0464, 0.0364, 0.040...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0585, 0.0412, 0.0248, 0.0183, 0.0492, 0.042...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0865, 0.0768, 0.0408, 0.023, 0.0482, 0.0835...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0625, 0.0371, 0.0349, 0.0229, 0.0372, 0.046...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0702, 0.042, 0.0384, 0.0357, 0.0397, 0.0566...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0798, 0.0373, 0.0442, 0.0496, 0.0425, 0.042...               0   \n",
       "1   [0.0744, 0.0395, 0.036, 0.0379, 0.047, 0.0529,...               0   \n",
       "2   [0.0741, 0.0471, 0.0419, 0.0501, 0.0394, 0.050...               0   \n",
       "3   [0.0496, 0.0264, 0.0229, 0.0237, 0.0299, 0.033...               1   \n",
       "4   [0.0687, 0.0357, 0.0356, 0.0448, 0.037, 0.041,...               1   \n",
       "5   [0.0568, 0.0366, 0.0445, 0.09, 0.035, 0.038, 0...               2   \n",
       "6   [0.0518, 0.0355, 0.0287, 0.1578, 0.0348, 0.035...               3   \n",
       "7   [0.0574, 0.0456, 0.0336, 0.179, 0.0436, 0.0444...               3   \n",
       "8   [0.1016, 0.0516, 0.0708, 0.1124, 0.0698, 0.069...               3   \n",
       "9   [0.0639, 0.0362, 0.0219, 0.0217, 0.0649, 0.053...               4   \n",
       "10  [0.0439, 0.0434, 0.0147, 0.0206, 0.0468, 0.044...               4   \n",
       "11  [0.052, 0.043, 0.0272, 0.0249, 0.0299, 0.0526,...               5   \n",
       "12  [0.0341, 0.0532, 0.036, 0.0285, 0.0258, 0.0291...               6   \n",
       "13  [0.023, 0.0562, 0.042, 0.0328, 0.0299, 0.0245,...               7   \n",
       "14  [0.0465, 0.0291, 0.0177, 0.022, 0.0355, 0.0338...               8   \n",
       "15  [0.0794, 0.0469, 0.0366, 0.0158, 0.035, 0.0531...               8   \n",
       "16  [0.0597, 0.0342, 0.0445, 0.0766, 0.0395, 0.038...               9   \n",
       "17  [0.0495, 0.0336, 0.0383, 0.0528, 0.0365, 0.033...               9   \n",
       "18  [0.0739, 0.0439, 0.0349, 0.0369, 0.0509, 0.061...              10   \n",
       "19  [0.0911, 0.0531, 0.04, 0.0329, 0.0601, 0.0759,...              10   \n",
       "20  [0.0757, 0.0359, 0.0462, 0.0357, 0.0447, 0.051...              11   \n",
       "21  [0.0379, 0.0288, 0.021, 0.045, 0.0231, 0.0298,...              11   \n",
       "22  [0.0658, 0.0282, 0.0319, 0.0249, 0.0402, 0.045...              12   \n",
       "23  [0.0748, 0.0499, 0.0396, 0.0213, 0.0506, 0.070...              13   \n",
       "24  [0.0709, 0.0328, 0.0393, 0.0584, 0.0396, 0.043...              13   \n",
       "25  [0.0657, 0.0341, 0.0354, 0.0504, 0.0423, 0.034...              14   \n",
       "26  [0.0728, 0.0373, 0.0348, 0.0442, 0.0501, 0.039...              14   \n",
       "27  [0.0491, 0.0296, 0.0265, 0.0407, 0.0383, 0.033...              15   \n",
       "28  [0.0255, 0.0328, 0.023, 0.0255, 0.0312, 0.0247...              15   \n",
       "29  [0.027, 0.0291, 0.018, 0.0181, 0.0277, 0.0225,...              15   \n",
       "30  [0.0932, 0.0937, 0.0871, 0.0065, 0.0961, 0.089...              16   \n",
       "31  [0.0617, 0.0364, 0.0477, 0.0622, 0.0402, 0.039...              17   \n",
       "32  [0.0397, 0.0244, 0.0262, 0.0464, 0.0251, 0.024...              17   \n",
       "33  [0.0591, 0.0323, 0.0319, 0.0482, 0.0357, 0.035...              18   \n",
       "34  [0.0731, 0.04, 0.04, 0.0314, 0.0414, 0.0528, 0...              19   \n",
       "35  [0.0481, 0.0231, 0.0219, 0.0365, 0.0243, 0.026...              20   \n",
       "36  [0.0452, 0.0337, 0.0305, 0.0764, 0.0258, 0.033...              21   \n",
       "37  [0.0142, 0.0267, 0.0153, 0.0471, 0.0077, 0.016...              21   \n",
       "38  [0.0845, 0.0605, 0.0326, 0.0967, 0.0829, 0.060...              22   \n",
       "39  [0.0671, 0.0424, 0.0299, 0.0779, 0.0528, 0.047...              22   \n",
       "40  [0.0523, 0.0362, 0.027, 0.0349, 0.0362, 0.0299...              22   \n",
       "41  [0.0603, 0.035, 0.032, 0.0817, 0.0372, 0.0374,...              22   \n",
       "42  [0.0269, 0.0282, 0.0226, 0.0653, 0.014, 0.021,...              23   \n",
       "43  [0.0116, 0.0246, 0.0142, 0.0696, 0.0049, 0.011...              23   \n",
       "44  [0.0118, 0.0226, 0.0148, 0.0613, 0.005, 0.0101...              23   \n",
       "45  [0.0191, 0.0248, 0.0184, 0.0749, 0.0099, 0.015...              23   \n",
       "46  [0.0772, 0.0434, 0.0602, 0.034, 0.0526, 0.0457...              24   \n",
       "47  [0.0709, 0.0312, 0.0414, 0.0464, 0.0364, 0.040...              24   \n",
       "48  [0.0585, 0.0412, 0.0248, 0.0183, 0.0492, 0.042...              25   \n",
       "49  [0.0865, 0.0768, 0.0408, 0.023, 0.0482, 0.0835...              25   \n",
       "50  [0.0625, 0.0371, 0.0349, 0.0229, 0.0372, 0.046...              26   \n",
       "51  [0.0702, 0.042, 0.0384, 0.0357, 0.0397, 0.0566...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.480769  0.127417  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                19       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                 0       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                19       NaN       NaN  \n",
       "24                 0       NaN       NaN  \n",
       "25                 0       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                22       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                 3       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 9       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621 : Training: loss:  0.11957981\n",
      "1622 : Training: loss:  0.108648404\n",
      "1623 : Training: loss:  0.11362666\n",
      "1624 : Training: loss:  0.12474209\n",
      "1625 : Training: loss:  0.12206231\n",
      "1626 : Training: loss:  0.10841881\n",
      "1627 : Training: loss:  0.104838766\n",
      "1628 : Training: loss:  0.13103102\n",
      "1629 : Training: loss:  0.13938992\n",
      "1630 : Training: loss:  0.12599131\n",
      "1631 : Training: loss:  0.12535004\n",
      "1632 : Training: loss:  0.10476799\n",
      "1633 : Training: loss:  0.11159148\n",
      "1634 : Training: loss:  0.13143136\n",
      "1635 : Training: loss:  0.1489266\n",
      "1636 : Training: loss:  0.12717435\n",
      "1637 : Training: loss:  0.107203044\n",
      "1638 : Training: loss:  0.13892537\n",
      "1639 : Training: loss:  0.12410983\n",
      "1640 : Training: loss:  0.12718357\n",
      "Validation: Loss:  0.12688513  Accuracy:  0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0744, 0.0363, 0.0464, 0.05, 0.04, 0.0401, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0686, 0.0382, 0.0373, 0.0376, 0.0442, 0.049...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0688, 0.0458, 0.0436, 0.0502, 0.037, 0.0472...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0451, 0.0254, 0.024, 0.0234, 0.0277, 0.0312...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0637, 0.0347, 0.0375, 0.0451, 0.0347, 0.038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0529, 0.0357, 0.0473, 0.0921, 0.0329, 0.035...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0479, 0.0348, 0.0307, 0.165, 0.0328, 0.0334...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0533, 0.0447, 0.0358, 0.1866, 0.0412, 0.041...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0957, 0.0503, 0.0743, 0.1144, 0.0664, 0.065...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0584, 0.0349, 0.0224, 0.0213, 0.0609, 0.05,...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0393, 0.0415, 0.0149, 0.0203, 0.0433, 0.041...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0473, 0.0413, 0.0279, 0.0244, 0.0277, 0.048...</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0314, 0.051, 0.0368, 0.0279, 0.0241, 0.027,...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0212, 0.0543, 0.043, 0.032, 0.0286, 0.0229,...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0419, 0.0279, 0.0183, 0.0219, 0.0328, 0.031...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0729, 0.0452, 0.0374, 0.0154, 0.0326, 0.049...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0558, 0.0334, 0.0473, 0.078, 0.0373, 0.0361...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0462, 0.0328, 0.0404, 0.0533, 0.0345, 0.031...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0683, 0.0425, 0.0361, 0.0364, 0.0479, 0.057...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0842, 0.0512, 0.0412, 0.0321, 0.0566, 0.071...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0704, 0.0348, 0.0483, 0.0356, 0.0421, 0.048...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0346, 0.028, 0.0222, 0.0458, 0.0215, 0.0278...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.061, 0.0274, 0.0335, 0.0249, 0.0379, 0.0431...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.069, 0.0481, 0.0409, 0.0207, 0.0475, 0.0661...</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0658, 0.0319, 0.0417, 0.0589, 0.0371, 0.041...</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0611, 0.0331, 0.0372, 0.0505, 0.0398, 0.032...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0676, 0.0363, 0.0363, 0.0443, 0.0472, 0.037...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0451, 0.0286, 0.0278, 0.0407, 0.0359, 0.030...</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0234, 0.0317, 0.0238, 0.0251, 0.0296, 0.023...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0246, 0.028, 0.0186, 0.0177, 0.0259, 0.0209...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0839, 0.0862, 0.0862, 0.0061, 0.0923, 0.080...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0576, 0.0355, 0.0502, 0.0631, 0.0379, 0.037...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0367, 0.0238, 0.0277, 0.0475, 0.0235, 0.023...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0546, 0.0315, 0.0336, 0.049, 0.0335, 0.0334...</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0676, 0.0388, 0.0415, 0.0313, 0.0389, 0.049...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0441, 0.0224, 0.0231, 0.0369, 0.0226, 0.024...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0417, 0.0329, 0.0323, 0.0783, 0.0242, 0.030...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0128, 0.0258, 0.0161, 0.0475, 0.007, 0.0152...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0785, 0.0591, 0.034, 0.0975, 0.0784, 0.0569...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0619, 0.0411, 0.0314, 0.0789, 0.0495, 0.044...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0481, 0.0351, 0.0282, 0.0348, 0.0339, 0.027...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0557, 0.0341, 0.0339, 0.0836, 0.0348, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0246, 0.0276, 0.024, 0.0676, 0.013, 0.0196,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0106, 0.024, 0.0152, 0.0721, 0.0045, 0.0109...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0107, 0.0219, 0.0157, 0.0626, 0.0046, 0.009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0175, 0.0244, 0.0198, 0.0782, 0.0092, 0.014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0722, 0.0421, 0.0626, 0.0334, 0.0499, 0.042...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.066, 0.0304, 0.0438, 0.0469, 0.0343, 0.0384...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0533, 0.0396, 0.0255, 0.0179, 0.0459, 0.039...</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0794, 0.0737, 0.0409, 0.0222, 0.045, 0.078,...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0577, 0.0358, 0.0359, 0.0224, 0.0349, 0.043...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0649, 0.0407, 0.0397, 0.0352, 0.0372, 0.053...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0744, 0.0363, 0.0464, 0.05, 0.04, 0.0401, 0...               0   \n",
       "1   [0.0686, 0.0382, 0.0373, 0.0376, 0.0442, 0.049...               0   \n",
       "2   [0.0688, 0.0458, 0.0436, 0.0502, 0.037, 0.0472...               0   \n",
       "3   [0.0451, 0.0254, 0.024, 0.0234, 0.0277, 0.0312...               1   \n",
       "4   [0.0637, 0.0347, 0.0375, 0.0451, 0.0347, 0.038...               1   \n",
       "5   [0.0529, 0.0357, 0.0473, 0.0921, 0.0329, 0.035...               2   \n",
       "6   [0.0479, 0.0348, 0.0307, 0.165, 0.0328, 0.0334...               3   \n",
       "7   [0.0533, 0.0447, 0.0358, 0.1866, 0.0412, 0.041...               3   \n",
       "8   [0.0957, 0.0503, 0.0743, 0.1144, 0.0664, 0.065...               3   \n",
       "9   [0.0584, 0.0349, 0.0224, 0.0213, 0.0609, 0.05,...               4   \n",
       "10  [0.0393, 0.0415, 0.0149, 0.0203, 0.0433, 0.041...               4   \n",
       "11  [0.0473, 0.0413, 0.0279, 0.0244, 0.0277, 0.048...               5   \n",
       "12  [0.0314, 0.051, 0.0368, 0.0279, 0.0241, 0.027,...               6   \n",
       "13  [0.0212, 0.0543, 0.043, 0.032, 0.0286, 0.0229,...               7   \n",
       "14  [0.0419, 0.0279, 0.0183, 0.0219, 0.0328, 0.031...               8   \n",
       "15  [0.0729, 0.0452, 0.0374, 0.0154, 0.0326, 0.049...               8   \n",
       "16  [0.0558, 0.0334, 0.0473, 0.078, 0.0373, 0.0361...               9   \n",
       "17  [0.0462, 0.0328, 0.0404, 0.0533, 0.0345, 0.031...               9   \n",
       "18  [0.0683, 0.0425, 0.0361, 0.0364, 0.0479, 0.057...              10   \n",
       "19  [0.0842, 0.0512, 0.0412, 0.0321, 0.0566, 0.071...              10   \n",
       "20  [0.0704, 0.0348, 0.0483, 0.0356, 0.0421, 0.048...              11   \n",
       "21  [0.0346, 0.028, 0.0222, 0.0458, 0.0215, 0.0278...              11   \n",
       "22  [0.061, 0.0274, 0.0335, 0.0249, 0.0379, 0.0431...              12   \n",
       "23  [0.069, 0.0481, 0.0409, 0.0207, 0.0475, 0.0661...              13   \n",
       "24  [0.0658, 0.0319, 0.0417, 0.0589, 0.0371, 0.041...              13   \n",
       "25  [0.0611, 0.0331, 0.0372, 0.0505, 0.0398, 0.032...              14   \n",
       "26  [0.0676, 0.0363, 0.0363, 0.0443, 0.0472, 0.037...              14   \n",
       "27  [0.0451, 0.0286, 0.0278, 0.0407, 0.0359, 0.030...              15   \n",
       "28  [0.0234, 0.0317, 0.0238, 0.0251, 0.0296, 0.023...              15   \n",
       "29  [0.0246, 0.028, 0.0186, 0.0177, 0.0259, 0.0209...              15   \n",
       "30  [0.0839, 0.0862, 0.0862, 0.0061, 0.0923, 0.080...              16   \n",
       "31  [0.0576, 0.0355, 0.0502, 0.0631, 0.0379, 0.037...              17   \n",
       "32  [0.0367, 0.0238, 0.0277, 0.0475, 0.0235, 0.023...              17   \n",
       "33  [0.0546, 0.0315, 0.0336, 0.049, 0.0335, 0.0334...              18   \n",
       "34  [0.0676, 0.0388, 0.0415, 0.0313, 0.0389, 0.049...              19   \n",
       "35  [0.0441, 0.0224, 0.0231, 0.0369, 0.0226, 0.024...              20   \n",
       "36  [0.0417, 0.0329, 0.0323, 0.0783, 0.0242, 0.030...              21   \n",
       "37  [0.0128, 0.0258, 0.0161, 0.0475, 0.007, 0.0152...              21   \n",
       "38  [0.0785, 0.0591, 0.034, 0.0975, 0.0784, 0.0569...              22   \n",
       "39  [0.0619, 0.0411, 0.0314, 0.0789, 0.0495, 0.044...              22   \n",
       "40  [0.0481, 0.0351, 0.0282, 0.0348, 0.0339, 0.027...              22   \n",
       "41  [0.0557, 0.0341, 0.0339, 0.0836, 0.0348, 0.034...              22   \n",
       "42  [0.0246, 0.0276, 0.024, 0.0676, 0.013, 0.0196,...              23   \n",
       "43  [0.0106, 0.024, 0.0152, 0.0721, 0.0045, 0.0109...              23   \n",
       "44  [0.0107, 0.0219, 0.0157, 0.0626, 0.0046, 0.009...              23   \n",
       "45  [0.0175, 0.0244, 0.0198, 0.0782, 0.0092, 0.014...              23   \n",
       "46  [0.0722, 0.0421, 0.0626, 0.0334, 0.0499, 0.042...              24   \n",
       "47  [0.066, 0.0304, 0.0438, 0.0469, 0.0343, 0.0384...              24   \n",
       "48  [0.0533, 0.0396, 0.0255, 0.0179, 0.0459, 0.039...              25   \n",
       "49  [0.0794, 0.0737, 0.0409, 0.0222, 0.045, 0.078,...              25   \n",
       "50  [0.0577, 0.0358, 0.0359, 0.0224, 0.0349, 0.043...              26   \n",
       "51  [0.0649, 0.0407, 0.0397, 0.0352, 0.0372, 0.053...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0       0.5  0.126885  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                 22       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  8       NaN       NaN  \n",
       "10                22       NaN       NaN  \n",
       "11                19       NaN       NaN  \n",
       "12                23       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                22       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                22       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                19       NaN       NaN  \n",
       "24                22       NaN       NaN  \n",
       "25                 9       NaN       NaN  \n",
       "26                22       NaN       NaN  \n",
       "27                 9       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                22       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                 9       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                22       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1641 : Training: loss:  0.09390243\n",
      "1642 : Training: loss:  0.12175459\n",
      "1643 : Training: loss:  0.15612581\n",
      "1644 : Training: loss:  0.07870045\n",
      "1645 : Training: loss:  0.1490137\n",
      "1646 : Training: loss:  0.08775543\n",
      "1647 : Training: loss:  0.12867115\n",
      "1648 : Training: loss:  0.13287598\n",
      "1649 : Training: loss:  0.11237864\n",
      "1650 : Training: loss:  0.123062424\n",
      "1651 : Training: loss:  0.14597934\n",
      "1652 : Training: loss:  0.11887685\n",
      "1653 : Training: loss:  0.1278711\n",
      "1654 : Training: loss:  0.13044101\n",
      "1655 : Training: loss:  0.11073628\n",
      "1656 : Training: loss:  0.12204656\n",
      "1657 : Training: loss:  0.12163328\n",
      "1658 : Training: loss:  0.13727055\n",
      "1659 : Training: loss:  0.10505704\n",
      "1660 : Training: loss:  0.111252494\n",
      "Validation: Loss:  0.1262418  Accuracy:  0.48076922\n",
      "1661 : Training: loss:  0.14532846\n",
      "1662 : Training: loss:  0.08359253\n",
      "1663 : Training: loss:  0.15124553\n",
      "1664 : Training: loss:  0.1249547\n",
      "1665 : Training: loss:  0.115662426\n",
      "1666 : Training: loss:  0.13313721\n",
      "1667 : Training: loss:  0.12434166\n",
      "1668 : Training: loss:  0.10807319\n",
      "1669 : Training: loss:  0.1090735\n",
      "1670 : Training: loss:  0.13656937\n",
      "1671 : Training: loss:  0.0876762\n",
      "1672 : Training: loss:  0.14153378\n",
      "1673 : Training: loss:  0.074940644\n",
      "1674 : Training: loss:  0.095721714\n",
      "1675 : Training: loss:  0.11612287\n",
      "1676 : Training: loss:  0.14949785\n",
      "1677 : Training: loss:  0.12847115\n",
      "1678 : Training: loss:  0.12322667\n",
      "1679 : Training: loss:  0.12510623\n",
      "1680 : Training: loss:  0.14980954\n",
      "Validation: Loss:  0.12556201  Accuracy:  0.5\n",
      "1681 : Training: loss:  0.1353084\n",
      "1682 : Training: loss:  0.11536469\n",
      "1683 : Training: loss:  0.14919485\n",
      "1684 : Training: loss:  0.097571224\n",
      "1685 : Training: loss:  0.1470061\n",
      "1686 : Training: loss:  0.14912951\n",
      "1687 : Training: loss:  0.091842614\n",
      "1688 : Training: loss:  0.13454072\n",
      "1689 : Training: loss:  0.10378432\n",
      "1690 : Training: loss:  0.11420365\n",
      "1691 : Training: loss:  0.121022426\n",
      "1692 : Training: loss:  0.12333724\n",
      "1693 : Training: loss:  0.09535808\n",
      "1694 : Training: loss:  0.13376325\n",
      "1695 : Training: loss:  0.14481981\n",
      "1696 : Training: loss:  0.14755557\n",
      "1697 : Training: loss:  0.10301546\n",
      "1698 : Training: loss:  0.13187033\n",
      "1699 : Training: loss:  0.11670588\n",
      "1700 : Training: loss:  0.1075038\n",
      "Validation: Loss:  0.12504384  Accuracy:  0.48076922\n",
      "1701 : Training: loss:  0.09048262\n",
      "1702 : Training: loss:  0.1276136\n",
      "1703 : Training: loss:  0.13364738\n",
      "1704 : Training: loss:  0.114406645\n",
      "1705 : Training: loss:  0.1366145\n",
      "1706 : Training: loss:  0.1369924\n",
      "1707 : Training: loss:  0.10476106\n",
      "1708 : Training: loss:  0.09598829\n",
      "1709 : Training: loss:  0.13238966\n",
      "1710 : Training: loss:  0.116345845\n",
      "1711 : Training: loss:  0.117907494\n",
      "1712 : Training: loss:  0.12956758\n",
      "1713 : Training: loss:  0.12357753\n",
      "1714 : Training: loss:  0.15343684\n",
      "1715 : Training: loss:  0.1330057\n",
      "1716 : Training: loss:  0.10595065\n",
      "1717 : Training: loss:  0.12693594\n",
      "1718 : Training: loss:  0.11466409\n",
      "1719 : Training: loss:  0.13625167\n",
      "1720 : Training: loss:  0.15289946\n",
      "Validation: Loss:  0.124552734  Accuracy:  0.5\n",
      "1721 : Training: loss:  0.11770714\n",
      "1722 : Training: loss:  0.13074885\n",
      "1723 : Training: loss:  0.1079998\n",
      "1724 : Training: loss:  0.11997851\n",
      "1725 : Training: loss:  0.12625131\n",
      "1726 : Training: loss:  0.11547302\n",
      "1727 : Training: loss:  0.15019988\n",
      "1728 : Training: loss:  0.123029724\n",
      "1729 : Training: loss:  0.11778439\n",
      "1730 : Training: loss:  0.10194835\n",
      "1731 : Training: loss:  0.103369005\n",
      "1732 : Training: loss:  0.1324164\n",
      "1733 : Training: loss:  0.10110531\n",
      "1734 : Training: loss:  0.124813646\n",
      "1735 : Training: loss:  0.13840409\n",
      "1736 : Training: loss:  0.0948171\n",
      "1737 : Training: loss:  0.1285877\n",
      "1738 : Training: loss:  0.12927993\n",
      "1739 : Training: loss:  0.1294976\n",
      "1740 : Training: loss:  0.11105383\n",
      "Validation: Loss:  0.12404567  Accuracy:  0.44230768\n",
      "1741 : Training: loss:  0.12590322\n",
      "1742 : Training: loss:  0.10567851\n",
      "1743 : Training: loss:  0.08153799\n",
      "1744 : Training: loss:  0.13703649\n",
      "1745 : Training: loss:  0.1487924\n",
      "1746 : Training: loss:  0.117303476\n",
      "1747 : Training: loss:  0.13594057\n",
      "1748 : Training: loss:  0.09698141\n",
      "1749 : Training: loss:  0.0996809\n",
      "1750 : Training: loss:  0.11523646\n",
      "1751 : Training: loss:  0.124907725\n",
      "1752 : Training: loss:  0.11683778\n",
      "1753 : Training: loss:  0.11624304\n",
      "1754 : Training: loss:  0.13453324\n",
      "1755 : Training: loss:  0.11382944\n",
      "1756 : Training: loss:  0.10342582\n",
      "1757 : Training: loss:  0.07562918\n",
      "1758 : Training: loss:  0.112954475\n",
      "1759 : Training: loss:  0.14661242\n",
      "1760 : Training: loss:  0.13822454\n",
      "Validation: Loss:  0.123329155  Accuracy:  0.48076922\n",
      "1761 : Training: loss:  0.1184237\n",
      "1762 : Training: loss:  0.14049335\n",
      "1763 : Training: loss:  0.12357163\n",
      "1764 : Training: loss:  0.12104705\n",
      "1765 : Training: loss:  0.11569986\n",
      "1766 : Training: loss:  0.14137013\n",
      "1767 : Training: loss:  0.107571155\n",
      "1768 : Training: loss:  0.120363854\n",
      "1769 : Training: loss:  0.12345165\n",
      "1770 : Training: loss:  0.115105875\n",
      "1771 : Training: loss:  0.13677132\n",
      "1772 : Training: loss:  0.15860462\n",
      "1773 : Training: loss:  0.12273167\n",
      "1774 : Training: loss:  0.13379227\n",
      "1775 : Training: loss:  0.14747639\n",
      "1776 : Training: loss:  0.14514025\n",
      "1777 : Training: loss:  0.10565013\n",
      "1778 : Training: loss:  0.13081601\n",
      "1779 : Training: loss:  0.089367054\n",
      "1780 : Training: loss:  0.13660304\n",
      "Validation: Loss:  0.12288772  Accuracy:  0.46153846\n",
      "1781 : Training: loss:  0.120979734\n",
      "1782 : Training: loss:  0.122100726\n",
      "1783 : Training: loss:  0.09158659\n",
      "1784 : Training: loss:  0.13036829\n",
      "1785 : Training: loss:  0.08735181\n",
      "1786 : Training: loss:  0.095036395\n",
      "1787 : Training: loss:  0.07985257\n",
      "1788 : Training: loss:  0.11028498\n",
      "1789 : Training: loss:  0.10381344\n",
      "1790 : Training: loss:  0.117585115\n",
      "1791 : Training: loss:  0.11967068\n",
      "1792 : Training: loss:  0.13743117\n",
      "1793 : Training: loss:  0.15250036\n",
      "1794 : Training: loss:  0.08316159\n",
      "1795 : Training: loss:  0.14457963\n",
      "1796 : Training: loss:  0.11602941\n",
      "1797 : Training: loss:  0.11639053\n",
      "1798 : Training: loss:  0.12496523\n",
      "1799 : Training: loss:  0.09935876\n",
      "1800 : Training: loss:  0.110341646\n",
      "Validation: Loss:  0.12233052  Accuracy:  0.48076922\n",
      "1801 : Training: loss:  0.13826263\n",
      "1802 : Training: loss:  0.139456\n",
      "1803 : Training: loss:  0.09356956\n",
      "1804 : Training: loss:  0.14312379\n",
      "1805 : Training: loss:  0.10727042\n",
      "1806 : Training: loss:  0.14406265\n",
      "1807 : Training: loss:  0.13312635\n",
      "1808 : Training: loss:  0.14194563\n",
      "1809 : Training: loss:  0.11639469\n",
      "1810 : Training: loss:  0.13171272\n",
      "1811 : Training: loss:  0.11669578\n",
      "1812 : Training: loss:  0.123376705\n",
      "1813 : Training: loss:  0.11326396\n",
      "1814 : Training: loss:  0.10932227\n",
      "1815 : Training: loss:  0.09937111\n",
      "1816 : Training: loss:  0.109377675\n",
      "1817 : Training: loss:  0.0846451\n",
      "1818 : Training: loss:  0.07652952\n",
      "1819 : Training: loss:  0.114696115\n",
      "1820 : Training: loss:  0.13699166\n",
      "Validation: Loss:  0.12184342  Accuracy:  0.48076922\n",
      "1821 : Training: loss:  0.09363127\n",
      "1822 : Training: loss:  0.117036544\n",
      "1823 : Training: loss:  0.12152942\n",
      "1824 : Training: loss:  0.10958515\n",
      "1825 : Training: loss:  0.09002369\n",
      "1826 : Training: loss:  0.096813135\n",
      "1827 : Training: loss:  0.08303514\n",
      "1828 : Training: loss:  0.12800418\n",
      "1829 : Training: loss:  0.11989131\n",
      "1830 : Training: loss:  0.1273627\n",
      "1831 : Training: loss:  0.13014163\n",
      "1832 : Training: loss:  0.09956005\n",
      "1833 : Training: loss:  0.10692728\n",
      "1834 : Training: loss:  0.091596656\n",
      "1835 : Training: loss:  0.108045705\n",
      "1836 : Training: loss:  0.12046809\n",
      "1837 : Training: loss:  0.07976238\n",
      "1838 : Training: loss:  0.123247676\n",
      "1839 : Training: loss:  0.11297983\n",
      "1840 : Training: loss:  0.12469735\n",
      "Validation: Loss:  0.121432744  Accuracy:  0.46153846\n",
      "1841 : Training: loss:  0.12460059\n",
      "1842 : Training: loss:  0.12433265\n",
      "1843 : Training: loss:  0.123014554\n",
      "1844 : Training: loss:  0.10122685\n",
      "1845 : Training: loss:  0.13918003\n",
      "1846 : Training: loss:  0.088665396\n",
      "1847 : Training: loss:  0.07478975\n",
      "1848 : Training: loss:  0.14566424\n",
      "1849 : Training: loss:  0.11371306\n",
      "1850 : Training: loss:  0.13542102\n",
      "1851 : Training: loss:  0.10090087\n",
      "1852 : Training: loss:  0.121440925\n",
      "1853 : Training: loss:  0.11942576\n",
      "1854 : Training: loss:  0.10023452\n",
      "1855 : Training: loss:  0.1450571\n",
      "1856 : Training: loss:  0.11520766\n",
      "1857 : Training: loss:  0.09859611\n",
      "1858 : Training: loss:  0.12057681\n",
      "1859 : Training: loss:  0.11015633\n",
      "1860 : Training: loss:  0.100460306\n",
      "Validation: Loss:  0.12100343  Accuracy:  0.48076922\n",
      "1861 : Training: loss:  0.09074043\n",
      "1862 : Training: loss:  0.12817156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863 : Training: loss:  0.102376804\n",
      "1864 : Training: loss:  0.12725565\n",
      "1865 : Training: loss:  0.11103362\n",
      "1866 : Training: loss:  0.13890235\n",
      "1867 : Training: loss:  0.08410655\n",
      "1868 : Training: loss:  0.115201205\n",
      "1869 : Training: loss:  0.11766252\n",
      "1870 : Training: loss:  0.08907966\n",
      "1871 : Training: loss:  0.13749868\n",
      "1872 : Training: loss:  0.12679142\n",
      "1873 : Training: loss:  0.13901527\n",
      "1874 : Training: loss:  0.12721123\n",
      "1875 : Training: loss:  0.15218128\n",
      "1876 : Training: loss:  0.103243366\n",
      "1877 : Training: loss:  0.11232701\n",
      "1878 : Training: loss:  0.14644086\n",
      "1879 : Training: loss:  0.07720824\n",
      "1880 : Training: loss:  0.14512438\n",
      "Validation: Loss:  0.120253846  Accuracy:  0.5\n",
      "1881 : Training: loss:  0.121520564\n",
      "1882 : Training: loss:  0.07052496\n",
      "1883 : Training: loss:  0.10567819\n",
      "1884 : Training: loss:  0.10341299\n",
      "1885 : Training: loss:  0.120983355\n",
      "1886 : Training: loss:  0.09802493\n",
      "1887 : Training: loss:  0.12392772\n",
      "1888 : Training: loss:  0.09413939\n",
      "1889 : Training: loss:  0.10326928\n",
      "1890 : Training: loss:  0.15140806\n",
      "1891 : Training: loss:  0.12584266\n",
      "1892 : Training: loss:  0.11021319\n",
      "1893 : Training: loss:  0.12329176\n",
      "1894 : Training: loss:  0.12725495\n",
      "1895 : Training: loss:  0.12205425\n",
      "1896 : Training: loss:  0.08918991\n",
      "1897 : Training: loss:  0.111037865\n",
      "1898 : Training: loss:  0.13951497\n",
      "1899 : Training: loss:  0.10573317\n",
      "1900 : Training: loss:  0.1303168\n",
      "Validation: Loss:  0.11942475  Accuracy:  0.46153846\n",
      "1901 : Training: loss:  0.11632148\n",
      "1902 : Training: loss:  0.07509075\n",
      "1903 : Training: loss:  0.117413536\n",
      "1904 : Training: loss:  0.13423677\n",
      "1905 : Training: loss:  0.08879189\n",
      "1906 : Training: loss:  0.1287543\n",
      "1907 : Training: loss:  0.12423856\n",
      "1908 : Training: loss:  0.12410598\n",
      "1909 : Training: loss:  0.1301349\n",
      "1910 : Training: loss:  0.12224788\n",
      "1911 : Training: loss:  0.11226403\n",
      "1912 : Training: loss:  0.1217074\n",
      "1913 : Training: loss:  0.15060747\n",
      "1914 : Training: loss:  0.13603178\n",
      "1915 : Training: loss:  0.094887204\n",
      "1916 : Training: loss:  0.12181398\n",
      "1917 : Training: loss:  0.12001223\n",
      "1918 : Training: loss:  0.121307485\n",
      "1919 : Training: loss:  0.12866378\n",
      "1920 : Training: loss:  0.10137214\n",
      "Validation: Loss:  0.11880972  Accuracy:  0.5192308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0784, 0.0385, 0.0606, 0.0452, 0.0372, 0.032...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.11881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0715, 0.0432, 0.042, 0.0284, 0.0443, 0.0414...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0676, 0.0512, 0.0501, 0.0417, 0.0336, 0.038...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.043, 0.0276, 0.027, 0.0165, 0.0271, 0.0239,...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0645, 0.0371, 0.0462, 0.0391, 0.0327, 0.030...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0484, 0.0323, 0.0621, 0.0986, 0.0272, 0.025...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0394, 0.0305, 0.0382, 0.2104, 0.0284, 0.022...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0423, 0.0388, 0.0417, 0.2316, 0.0366, 0.027...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0882, 0.0438, 0.0902, 0.1162, 0.0578, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0587, 0.0422, 0.0199, 0.0133, 0.0749, 0.041...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0366, 0.0559, 0.0125, 0.013, 0.0544, 0.0335...</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0419, 0.0477, 0.025, 0.0154, 0.0251, 0.0395...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0247, 0.0501, 0.0335, 0.0194, 0.0198, 0.018...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0131, 0.0409, 0.0319, 0.0208, 0.0213, 0.012...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0356, 0.0302, 0.0174, 0.0153, 0.0343, 0.021...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0717, 0.0543, 0.0344, 0.0086, 0.0304, 0.040...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0537, 0.0309, 0.0638, 0.0791, 0.0327, 0.027...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0417, 0.0298, 0.0483, 0.0473, 0.0299, 0.023...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0637, 0.0455, 0.0373, 0.0259, 0.047, 0.0462...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0834, 0.0589, 0.0421, 0.0208, 0.0591, 0.060...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0688, 0.0357, 0.0573, 0.0275, 0.0385, 0.038...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.026, 0.0253, 0.0222, 0.0396, 0.0175, 0.018,...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0568, 0.0275, 0.0373, 0.0182, 0.037, 0.0338...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0676, 0.058, 0.0417, 0.0125, 0.05, 0.0587, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0656, 0.0312, 0.0553, 0.055, 0.0337, 0.0319...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0614, 0.0339, 0.0466, 0.045, 0.0372, 0.0248...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.064, 0.0359, 0.0411, 0.0362, 0.0449, 0.0269...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0414, 0.0284, 0.0318, 0.0338, 0.034, 0.0228...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0175, 0.029, 0.0209, 0.0194, 0.0263, 0.0161...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0197, 0.0277, 0.0164, 0.0123, 0.0235, 0.014...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0557, 0.0873, 0.063, 0.0034, 0.0929, 0.0522...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0546, 0.0333, 0.0634, 0.0595, 0.0329, 0.028...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0336, 0.0226, 0.0347, 0.0475, 0.0198, 0.016...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0534, 0.0338, 0.0424, 0.047, 0.0322, 0.0256...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0624, 0.0412, 0.044, 0.0228, 0.0358, 0.039,...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0422, 0.0226, 0.028, 0.0334, 0.0199, 0.0177...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0355, 0.031, 0.039, 0.0796, 0.0195, 0.0224,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0071, 0.0182, 0.0125, 0.0355, 0.0042, 0.008...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0644, 0.0587, 0.0348, 0.0902, 0.0853, 0.040...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0514, 0.0389, 0.034, 0.0734, 0.0485, 0.0305...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0401, 0.0342, 0.0291, 0.0263, 0.0302, 0.018...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0505, 0.0329, 0.0427, 0.0873, 0.0317, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0183, 0.0234, 0.0256, 0.0672, 0.0091, 0.012...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.006, 0.0155, 0.0118, 0.0624, 0.0025, 0.0053...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0061, 0.0134, 0.0115, 0.0507, 0.0025, 0.004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0116, 0.0186, 0.0197, 0.0804, 0.0059, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.063, 0.0379, 0.07, 0.0241, 0.0421, 0.0307, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0665, 0.0299, 0.0574, 0.0429, 0.0305, 0.030...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0525, 0.0519, 0.0252, 0.011, 0.0536, 0.0327...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0718, 0.0905, 0.0323, 0.0123, 0.0423, 0.063...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0555, 0.0386, 0.0349, 0.0142, 0.0309, 0.034...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0619, 0.0427, 0.0407, 0.025, 0.033, 0.0428,...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0784, 0.0385, 0.0606, 0.0452, 0.0372, 0.032...               0   \n",
       "1   [0.0715, 0.0432, 0.042, 0.0284, 0.0443, 0.0414...               0   \n",
       "2   [0.0676, 0.0512, 0.0501, 0.0417, 0.0336, 0.038...               0   \n",
       "3   [0.043, 0.0276, 0.027, 0.0165, 0.0271, 0.0239,...               1   \n",
       "4   [0.0645, 0.0371, 0.0462, 0.0391, 0.0327, 0.030...               1   \n",
       "5   [0.0484, 0.0323, 0.0621, 0.0986, 0.0272, 0.025...               2   \n",
       "6   [0.0394, 0.0305, 0.0382, 0.2104, 0.0284, 0.022...               3   \n",
       "7   [0.0423, 0.0388, 0.0417, 0.2316, 0.0366, 0.027...               3   \n",
       "8   [0.0882, 0.0438, 0.0902, 0.1162, 0.0578, 0.050...               3   \n",
       "9   [0.0587, 0.0422, 0.0199, 0.0133, 0.0749, 0.041...               4   \n",
       "10  [0.0366, 0.0559, 0.0125, 0.013, 0.0544, 0.0335...               4   \n",
       "11  [0.0419, 0.0477, 0.025, 0.0154, 0.0251, 0.0395...               5   \n",
       "12  [0.0247, 0.0501, 0.0335, 0.0194, 0.0198, 0.018...               6   \n",
       "13  [0.0131, 0.0409, 0.0319, 0.0208, 0.0213, 0.012...               7   \n",
       "14  [0.0356, 0.0302, 0.0174, 0.0153, 0.0343, 0.021...               8   \n",
       "15  [0.0717, 0.0543, 0.0344, 0.0086, 0.0304, 0.040...               8   \n",
       "16  [0.0537, 0.0309, 0.0638, 0.0791, 0.0327, 0.027...               9   \n",
       "17  [0.0417, 0.0298, 0.0483, 0.0473, 0.0299, 0.023...               9   \n",
       "18  [0.0637, 0.0455, 0.0373, 0.0259, 0.047, 0.0462...              10   \n",
       "19  [0.0834, 0.0589, 0.0421, 0.0208, 0.0591, 0.060...              10   \n",
       "20  [0.0688, 0.0357, 0.0573, 0.0275, 0.0385, 0.038...              11   \n",
       "21  [0.026, 0.0253, 0.0222, 0.0396, 0.0175, 0.018,...              11   \n",
       "22  [0.0568, 0.0275, 0.0373, 0.0182, 0.037, 0.0338...              12   \n",
       "23  [0.0676, 0.058, 0.0417, 0.0125, 0.05, 0.0587, ...              13   \n",
       "24  [0.0656, 0.0312, 0.0553, 0.055, 0.0337, 0.0319...              13   \n",
       "25  [0.0614, 0.0339, 0.0466, 0.045, 0.0372, 0.0248...              14   \n",
       "26  [0.064, 0.0359, 0.0411, 0.0362, 0.0449, 0.0269...              14   \n",
       "27  [0.0414, 0.0284, 0.0318, 0.0338, 0.034, 0.0228...              15   \n",
       "28  [0.0175, 0.029, 0.0209, 0.0194, 0.0263, 0.0161...              15   \n",
       "29  [0.0197, 0.0277, 0.0164, 0.0123, 0.0235, 0.014...              15   \n",
       "30  [0.0557, 0.0873, 0.063, 0.0034, 0.0929, 0.0522...              16   \n",
       "31  [0.0546, 0.0333, 0.0634, 0.0595, 0.0329, 0.028...              17   \n",
       "32  [0.0336, 0.0226, 0.0347, 0.0475, 0.0198, 0.016...              17   \n",
       "33  [0.0534, 0.0338, 0.0424, 0.047, 0.0322, 0.0256...              18   \n",
       "34  [0.0624, 0.0412, 0.044, 0.0228, 0.0358, 0.039,...              19   \n",
       "35  [0.0422, 0.0226, 0.028, 0.0334, 0.0199, 0.0177...              20   \n",
       "36  [0.0355, 0.031, 0.039, 0.0796, 0.0195, 0.0224,...              21   \n",
       "37  [0.0071, 0.0182, 0.0125, 0.0355, 0.0042, 0.008...              21   \n",
       "38  [0.0644, 0.0587, 0.0348, 0.0902, 0.0853, 0.040...              22   \n",
       "39  [0.0514, 0.0389, 0.034, 0.0734, 0.0485, 0.0305...              22   \n",
       "40  [0.0401, 0.0342, 0.0291, 0.0263, 0.0302, 0.018...              22   \n",
       "41  [0.0505, 0.0329, 0.0427, 0.0873, 0.0317, 0.025...              22   \n",
       "42  [0.0183, 0.0234, 0.0256, 0.0672, 0.0091, 0.012...              23   \n",
       "43  [0.006, 0.0155, 0.0118, 0.0624, 0.0025, 0.0053...              23   \n",
       "44  [0.0061, 0.0134, 0.0115, 0.0507, 0.0025, 0.004...              23   \n",
       "45  [0.0116, 0.0186, 0.0197, 0.0804, 0.0059, 0.008...              23   \n",
       "46  [0.063, 0.0379, 0.07, 0.0241, 0.0421, 0.0307, ...              24   \n",
       "47  [0.0665, 0.0299, 0.0574, 0.0429, 0.0305, 0.030...              24   \n",
       "48  [0.0525, 0.0519, 0.0252, 0.011, 0.0536, 0.0327...              25   \n",
       "49  [0.0718, 0.0905, 0.0323, 0.0123, 0.0423, 0.063...              25   \n",
       "50  [0.0555, 0.0386, 0.0349, 0.0142, 0.0309, 0.034...              26   \n",
       "51  [0.0619, 0.0427, 0.0407, 0.025, 0.033, 0.0428,...              26   \n",
       "\n",
       "    Predicted labels  Accuracy     Loss  \n",
       "0                  0  0.519231  0.11881  \n",
       "1                 10       NaN      NaN  \n",
       "2                  0       NaN      NaN  \n",
       "3                 22       NaN      NaN  \n",
       "4                  0       NaN      NaN  \n",
       "5                  3       NaN      NaN  \n",
       "6                  3       NaN      NaN  \n",
       "7                  3       NaN      NaN  \n",
       "8                  9       NaN      NaN  \n",
       "9                 25       NaN      NaN  \n",
       "10                22       NaN      NaN  \n",
       "11                 8       NaN      NaN  \n",
       "12                17       NaN      NaN  \n",
       "13                 7       NaN      NaN  \n",
       "14                22       NaN      NaN  \n",
       "15                 8       NaN      NaN  \n",
       "16                 9       NaN      NaN  \n",
       "17                 9       NaN      NaN  \n",
       "18                10       NaN      NaN  \n",
       "19                25       NaN      NaN  \n",
       "20                 0       NaN      NaN  \n",
       "21                22       NaN      NaN  \n",
       "22                12       NaN      NaN  \n",
       "23                25       NaN      NaN  \n",
       "24                 0       NaN      NaN  \n",
       "25                 9       NaN      NaN  \n",
       "26                22       NaN      NaN  \n",
       "27                15       NaN      NaN  \n",
       "28                15       NaN      NaN  \n",
       "29                15       NaN      NaN  \n",
       "30                16       NaN      NaN  \n",
       "31                17       NaN      NaN  \n",
       "32                17       NaN      NaN  \n",
       "33                 0       NaN      NaN  \n",
       "34                 0       NaN      NaN  \n",
       "35                 0       NaN      NaN  \n",
       "36                23       NaN      NaN  \n",
       "37                23       NaN      NaN  \n",
       "38                22       NaN      NaN  \n",
       "39                22       NaN      NaN  \n",
       "40                22       NaN      NaN  \n",
       "41                22       NaN      NaN  \n",
       "42                23       NaN      NaN  \n",
       "43                23       NaN      NaN  \n",
       "44                23       NaN      NaN  \n",
       "45                23       NaN      NaN  \n",
       "46                21       NaN      NaN  \n",
       "47                 0       NaN      NaN  \n",
       "48                25       NaN      NaN  \n",
       "49                 8       NaN      NaN  \n",
       "50                26       NaN      NaN  \n",
       "51                10       NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1921 : Training: loss:  0.13040307\n",
      "1922 : Training: loss:  0.11356212\n",
      "1923 : Training: loss:  0.101519786\n",
      "1924 : Training: loss:  0.08289338\n",
      "1925 : Training: loss:  0.1473451\n",
      "1926 : Training: loss:  0.13594767\n",
      "1927 : Training: loss:  0.13110074\n",
      "1928 : Training: loss:  0.1134414\n",
      "1929 : Training: loss:  0.10475694\n",
      "1930 : Training: loss:  0.12560442\n",
      "1931 : Training: loss:  0.15104479\n",
      "1932 : Training: loss:  0.1067069\n",
      "1933 : Training: loss:  0.10296412\n",
      "1934 : Training: loss:  0.10279539\n",
      "1935 : Training: loss:  0.13002838\n",
      "1936 : Training: loss:  0.09205416\n",
      "1937 : Training: loss:  0.11626586\n",
      "1938 : Training: loss:  0.07704743\n",
      "1939 : Training: loss:  0.15031348\n",
      "1940 : Training: loss:  0.12096213\n",
      "Validation: Loss:  0.11849061  Accuracy:  0.5\n",
      "1941 : Training: loss:  0.14773415\n",
      "1942 : Training: loss:  0.13874534\n",
      "1943 : Training: loss:  0.1040026\n",
      "1944 : Training: loss:  0.13576242\n",
      "1945 : Training: loss:  0.123328514\n",
      "1946 : Training: loss:  0.09565495\n",
      "1947 : Training: loss:  0.14681742\n",
      "1948 : Training: loss:  0.13458677\n",
      "1949 : Training: loss:  0.085974745\n",
      "1950 : Training: loss:  0.11250077\n",
      "1951 : Training: loss:  0.086480804\n",
      "1952 : Training: loss:  0.13678066\n",
      "1953 : Training: loss:  0.1211809\n",
      "1954 : Training: loss:  0.11551771\n",
      "1955 : Training: loss:  0.09749484\n",
      "1956 : Training: loss:  0.111059144\n",
      "1957 : Training: loss:  0.11110312\n",
      "1958 : Training: loss:  0.09485232\n",
      "1959 : Training: loss:  0.09886466\n",
      "1960 : Training: loss:  0.124735355\n",
      "Validation: Loss:  0.11809196  Accuracy:  0.5\n",
      "1961 : Training: loss:  0.09300742\n",
      "1962 : Training: loss:  0.14054802\n",
      "1963 : Training: loss:  0.094951436\n",
      "1964 : Training: loss:  0.085228436\n",
      "1965 : Training: loss:  0.074391395\n",
      "1966 : Training: loss:  0.12405759\n",
      "1967 : Training: loss:  0.090180114\n",
      "1968 : Training: loss:  0.10201587\n",
      "1969 : Training: loss:  0.10721995\n",
      "1970 : Training: loss:  0.110391706\n",
      "1971 : Training: loss:  0.13801807\n",
      "1972 : Training: loss:  0.11530279\n",
      "1973 : Training: loss:  0.10499821\n",
      "1974 : Training: loss:  0.11000163\n",
      "1975 : Training: loss:  0.12625211\n",
      "1976 : Training: loss:  0.121243104\n",
      "1977 : Training: loss:  0.10615884\n",
      "1978 : Training: loss:  0.13343179\n",
      "1979 : Training: loss:  0.084837355\n",
      "1980 : Training: loss:  0.12338792\n",
      "Validation: Loss:  0.11748789  Accuracy:  0.5\n",
      "1981 : Training: loss:  0.13796619\n",
      "1982 : Training: loss:  0.14139524\n",
      "1983 : Training: loss:  0.07869595\n",
      "1984 : Training: loss:  0.13855037\n",
      "1985 : Training: loss:  0.13532501\n",
      "1986 : Training: loss:  0.08034583\n",
      "1987 : Training: loss:  0.1313098\n",
      "1988 : Training: loss:  0.11409997\n",
      "1989 : Training: loss:  0.12682082\n",
      "1990 : Training: loss:  0.11658515\n",
      "1991 : Training: loss:  0.12267665\n",
      "1992 : Training: loss:  0.12091017\n",
      "1993 : Training: loss:  0.10659226\n",
      "1994 : Training: loss:  0.11387125\n",
      "1995 : Training: loss:  0.12999403\n",
      "1996 : Training: loss:  0.1411663\n",
      "1997 : Training: loss:  0.10084657\n",
      "1998 : Training: loss:  0.11699196\n",
      "1999 : Training: loss:  0.12851156\n",
      "2000 : Training: loss:  0.106923595\n",
      "Validation: Loss:  0.1170487  Accuracy:  0.5192308\n",
      "2001 : Training: loss:  0.13077715\n",
      "2002 : Training: loss:  0.10415721\n",
      "2003 : Training: loss:  0.107548386\n",
      "2004 : Training: loss:  0.11146611\n",
      "2005 : Training: loss:  0.08467336\n",
      "2006 : Training: loss:  0.08236262\n",
      "2007 : Training: loss:  0.07039596\n",
      "2008 : Training: loss:  0.13648316\n",
      "2009 : Training: loss:  0.12054602\n",
      "2010 : Training: loss:  0.12078356\n",
      "2011 : Training: loss:  0.10337489\n",
      "2012 : Training: loss:  0.09201594\n",
      "2013 : Training: loss:  0.13629249\n",
      "2014 : Training: loss:  0.07930392\n",
      "2015 : Training: loss:  0.10892245\n",
      "2016 : Training: loss:  0.116087675\n",
      "2017 : Training: loss:  0.13403523\n",
      "2018 : Training: loss:  0.13192275\n",
      "2019 : Training: loss:  0.089511186\n",
      "2020 : Training: loss:  0.09925768\n",
      "Validation: Loss:  0.11664358  Accuracy:  0.5192308\n",
      "2021 : Training: loss:  0.085477255\n",
      "2022 : Training: loss:  0.13216592\n",
      "2023 : Training: loss:  0.11754332\n",
      "2024 : Training: loss:  0.10182143\n",
      "2025 : Training: loss:  0.13722603\n",
      "2026 : Training: loss:  0.14310524\n",
      "2027 : Training: loss:  0.07471013\n",
      "2028 : Training: loss:  0.13299552\n",
      "2029 : Training: loss:  0.09400258\n",
      "2030 : Training: loss:  0.12197376\n",
      "2031 : Training: loss:  0.13962294\n",
      "2032 : Training: loss:  0.07297529\n",
      "2033 : Training: loss:  0.10072061\n",
      "2034 : Training: loss:  0.10262346\n",
      "2035 : Training: loss:  0.0912439\n",
      "2036 : Training: loss:  0.09562419\n",
      "2037 : Training: loss:  0.116260946\n",
      "2038 : Training: loss:  0.08874294\n",
      "2039 : Training: loss:  0.120122\n",
      "2040 : Training: loss:  0.07868887\n",
      "Validation: Loss:  0.11623972  Accuracy:  0.5192308\n",
      "2041 : Training: loss:  0.08148542\n",
      "2042 : Training: loss:  0.11828805\n",
      "2043 : Training: loss:  0.13857394\n",
      "2044 : Training: loss:  0.12871137\n",
      "2045 : Training: loss:  0.11821407\n",
      "2046 : Training: loss:  0.13503492\n",
      "2047 : Training: loss:  0.10927039\n",
      "2048 : Training: loss:  0.08613716\n",
      "2049 : Training: loss:  0.118965216\n",
      "2050 : Training: loss:  0.076949865\n",
      "2051 : Training: loss:  0.1022864\n",
      "2052 : Training: loss:  0.093555115\n",
      "2053 : Training: loss:  0.11240172\n",
      "2054 : Training: loss:  0.102341935\n",
      "2055 : Training: loss:  0.12381729\n",
      "2056 : Training: loss:  0.13825382\n",
      "2057 : Training: loss:  0.13300456\n",
      "2058 : Training: loss:  0.07767368\n",
      "2059 : Training: loss:  0.10698268\n",
      "2060 : Training: loss:  0.09917494\n",
      "Validation: Loss:  0.115690455  Accuracy:  0.5192308\n",
      "2061 : Training: loss:  0.11362058\n",
      "2062 : Training: loss:  0.13221444\n",
      "2063 : Training: loss:  0.10747126\n",
      "2064 : Training: loss:  0.12898931\n",
      "2065 : Training: loss:  0.09849396\n",
      "2066 : Training: loss:  0.11766696\n",
      "2067 : Training: loss:  0.09169546\n",
      "2068 : Training: loss:  0.10851148\n",
      "2069 : Training: loss:  0.103083305\n",
      "2070 : Training: loss:  0.11399549\n",
      "2071 : Training: loss:  0.12280227\n",
      "2072 : Training: loss:  0.08192234\n",
      "2073 : Training: loss:  0.08996249\n",
      "2074 : Training: loss:  0.11553932\n",
      "2075 : Training: loss:  0.06813633\n",
      "2076 : Training: loss:  0.1029132\n",
      "2077 : Training: loss:  0.09941493\n",
      "2078 : Training: loss:  0.09700589\n",
      "2079 : Training: loss:  0.107929565\n",
      "2080 : Training: loss:  0.07662347\n",
      "Validation: Loss:  0.11505529  Accuracy:  0.5192308\n",
      "2081 : Training: loss:  0.10727871\n",
      "2082 : Training: loss:  0.099594444\n",
      "2083 : Training: loss:  0.11962608\n",
      "2084 : Training: loss:  0.099810734\n",
      "2085 : Training: loss:  0.0905715\n",
      "2086 : Training: loss:  0.10301823\n",
      "2087 : Training: loss:  0.05670364\n",
      "2088 : Training: loss:  0.15055692\n",
      "2089 : Training: loss:  0.08958398\n",
      "2090 : Training: loss:  0.10336146\n",
      "2091 : Training: loss:  0.09840064\n",
      "2092 : Training: loss:  0.10163086\n",
      "2093 : Training: loss:  0.11042562\n",
      "2094 : Training: loss:  0.10272918\n",
      "2095 : Training: loss:  0.117212325\n",
      "2096 : Training: loss:  0.10167994\n",
      "2097 : Training: loss:  0.11380461\n",
      "2098 : Training: loss:  0.11363386\n",
      "2099 : Training: loss:  0.107856\n",
      "2100 : Training: loss:  0.11628001\n",
      "Validation: Loss:  0.114396535  Accuracy:  0.46153846\n",
      "2101 : Training: loss:  0.11235773\n",
      "2102 : Training: loss:  0.06274666\n",
      "2103 : Training: loss:  0.103259295\n",
      "2104 : Training: loss:  0.1263512\n",
      "2105 : Training: loss:  0.12373473\n",
      "2106 : Training: loss:  0.12176511\n",
      "2107 : Training: loss:  0.10570689\n",
      "2108 : Training: loss:  0.121844634\n",
      "2109 : Training: loss:  0.14380632\n",
      "2110 : Training: loss:  0.12857409\n",
      "2111 : Training: loss:  0.1265712\n",
      "2112 : Training: loss:  0.108193114\n",
      "2113 : Training: loss:  0.11723399\n",
      "2114 : Training: loss:  0.1285206\n",
      "2115 : Training: loss:  0.11359663\n",
      "2116 : Training: loss:  0.10909725\n",
      "2117 : Training: loss:  0.13319346\n",
      "2118 : Training: loss:  0.121457085\n",
      "2119 : Training: loss:  0.098723546\n",
      "2120 : Training: loss:  0.12850791\n",
      "Validation: Loss:  0.11396359  Accuracy:  0.44230768\n",
      "2121 : Training: loss:  0.12874426\n",
      "2122 : Training: loss:  0.11305541\n",
      "2123 : Training: loss:  0.13823777\n",
      "2124 : Training: loss:  0.089759246\n",
      "2125 : Training: loss:  0.06313267\n",
      "2126 : Training: loss:  0.13453905\n",
      "2127 : Training: loss:  0.09330867\n",
      "2128 : Training: loss:  0.059233155\n",
      "2129 : Training: loss:  0.116474144\n",
      "2130 : Training: loss:  0.12298009\n",
      "2131 : Training: loss:  0.108820334\n",
      "2132 : Training: loss:  0.11902279\n",
      "2133 : Training: loss:  0.07873019\n",
      "2134 : Training: loss:  0.14289592\n",
      "2135 : Training: loss:  0.12165896\n",
      "2136 : Training: loss:  0.13683584\n",
      "2137 : Training: loss:  0.08759839\n",
      "2138 : Training: loss:  0.14553055\n",
      "2139 : Training: loss:  0.13666621\n",
      "2140 : Training: loss:  0.093923554\n",
      "Validation: Loss:  0.11369465  Accuracy:  0.42307693\n",
      "2141 : Training: loss:  0.0959413\n",
      "2142 : Training: loss:  0.11633932\n",
      "2143 : Training: loss:  0.11800848\n",
      "2144 : Training: loss:  0.11486227\n",
      "2145 : Training: loss:  0.13116929\n",
      "2146 : Training: loss:  0.111147575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2147 : Training: loss:  0.09976401\n",
      "2148 : Training: loss:  0.11931198\n",
      "2149 : Training: loss:  0.13219929\n",
      "2150 : Training: loss:  0.12696654\n",
      "2151 : Training: loss:  0.092278145\n",
      "2152 : Training: loss:  0.05727065\n",
      "2153 : Training: loss:  0.07786297\n",
      "2154 : Training: loss:  0.0975474\n",
      "2155 : Training: loss:  0.14838445\n",
      "2156 : Training: loss:  0.122333206\n",
      "2157 : Training: loss:  0.11647522\n",
      "2158 : Training: loss:  0.13157377\n",
      "2159 : Training: loss:  0.09984026\n",
      "2160 : Training: loss:  0.1409648\n",
      "Validation: Loss:  0.1133362  Accuracy:  0.44230768\n",
      "2161 : Training: loss:  0.09116435\n",
      "2162 : Training: loss:  0.11461516\n",
      "2163 : Training: loss:  0.09453824\n",
      "2164 : Training: loss:  0.10420757\n",
      "2165 : Training: loss:  0.109524995\n",
      "2166 : Training: loss:  0.08917193\n",
      "2167 : Training: loss:  0.10014366\n",
      "2168 : Training: loss:  0.11677267\n",
      "2169 : Training: loss:  0.11359992\n",
      "2170 : Training: loss:  0.085338496\n",
      "2171 : Training: loss:  0.119436845\n",
      "2172 : Training: loss:  0.12234217\n",
      "2173 : Training: loss:  0.082915895\n",
      "2174 : Training: loss:  0.088359505\n",
      "2175 : Training: loss:  0.109137006\n",
      "2176 : Training: loss:  0.070084594\n",
      "2177 : Training: loss:  0.1323269\n",
      "2178 : Training: loss:  0.097997814\n",
      "2179 : Training: loss:  0.14340203\n",
      "2180 : Training: loss:  0.10947242\n",
      "Validation: Loss:  0.11294177  Accuracy:  0.48076922\n",
      "2181 : Training: loss:  0.1146193\n",
      "2182 : Training: loss:  0.06527421\n",
      "2183 : Training: loss:  0.10479628\n",
      "2184 : Training: loss:  0.08411258\n",
      "2185 : Training: loss:  0.100832224\n",
      "2186 : Training: loss:  0.091766395\n",
      "2187 : Training: loss:  0.09927745\n",
      "2188 : Training: loss:  0.12736835\n",
      "2189 : Training: loss:  0.12931937\n",
      "2190 : Training: loss:  0.11391264\n",
      "2191 : Training: loss:  0.10455237\n",
      "2192 : Training: loss:  0.1339507\n",
      "2193 : Training: loss:  0.13270047\n",
      "2194 : Training: loss:  0.1296453\n",
      "2195 : Training: loss:  0.12084757\n",
      "2196 : Training: loss:  0.09274452\n",
      "2197 : Training: loss:  0.10929351\n",
      "2198 : Training: loss:  0.06499979\n",
      "2199 : Training: loss:  0.1158224\n",
      "2200 : Training: loss:  0.1228837\n",
      "Validation: Loss:  0.11271156  Accuracy:  0.46153846\n",
      "2201 : Training: loss:  0.110534966\n",
      "2202 : Training: loss:  0.11388473\n",
      "2203 : Training: loss:  0.10994369\n",
      "2204 : Training: loss:  0.10831303\n",
      "2205 : Training: loss:  0.1302553\n",
      "2206 : Training: loss:  0.09442503\n",
      "2207 : Training: loss:  0.12109731\n",
      "2208 : Training: loss:  0.102058\n",
      "2209 : Training: loss:  0.08875036\n",
      "2210 : Training: loss:  0.09986118\n",
      "2211 : Training: loss:  0.12615441\n",
      "2212 : Training: loss:  0.10442811\n",
      "2213 : Training: loss:  0.09775213\n",
      "2214 : Training: loss:  0.1146423\n",
      "2215 : Training: loss:  0.08479959\n",
      "2216 : Training: loss:  0.07398328\n",
      "2217 : Training: loss:  0.07864124\n",
      "2218 : Training: loss:  0.12388755\n",
      "2219 : Training: loss:  0.10341178\n",
      "2220 : Training: loss:  0.11138764\n",
      "Validation: Loss:  0.112345226  Accuracy:  0.5192308\n",
      "2221 : Training: loss:  0.11311897\n",
      "2222 : Training: loss:  0.07875189\n",
      "2223 : Training: loss:  0.13788538\n",
      "2224 : Training: loss:  0.10704652\n",
      "2225 : Training: loss:  0.1054674\n",
      "2226 : Training: loss:  0.098711185\n",
      "2227 : Training: loss:  0.03883435\n",
      "2228 : Training: loss:  0.09602238\n",
      "2229 : Training: loss:  0.1041397\n",
      "2230 : Training: loss:  0.11950302\n",
      "2231 : Training: loss:  0.12046201\n",
      "2232 : Training: loss:  0.09090729\n",
      "2233 : Training: loss:  0.04883972\n",
      "2234 : Training: loss:  0.115532696\n",
      "2235 : Training: loss:  0.10451826\n",
      "2236 : Training: loss:  0.1045159\n",
      "2237 : Training: loss:  0.1135456\n",
      "2238 : Training: loss:  0.09303602\n",
      "2239 : Training: loss:  0.07782769\n",
      "2240 : Training: loss:  0.08857204\n",
      "Validation: Loss:  0.11176004  Accuracy:  0.5\n",
      "2241 : Training: loss:  0.13172218\n",
      "2242 : Training: loss:  0.1144467\n",
      "2243 : Training: loss:  0.09633685\n",
      "2244 : Training: loss:  0.0781013\n",
      "2245 : Training: loss:  0.115413524\n",
      "2246 : Training: loss:  0.11473829\n",
      "2247 : Training: loss:  0.085222155\n",
      "2248 : Training: loss:  0.08618136\n",
      "2249 : Training: loss:  0.09206877\n",
      "2250 : Training: loss:  0.088224225\n",
      "2251 : Training: loss:  0.13521521\n",
      "2252 : Training: loss:  0.10379281\n",
      "2253 : Training: loss:  0.11303039\n",
      "2254 : Training: loss:  0.11907762\n",
      "2255 : Training: loss:  0.14244606\n",
      "2256 : Training: loss:  0.113140054\n",
      "2257 : Training: loss:  0.10059271\n",
      "2258 : Training: loss:  0.10593499\n",
      "2259 : Training: loss:  0.11853396\n",
      "2260 : Training: loss:  0.12064143\n",
      "Validation: Loss:  0.11104164  Accuracy:  0.53846157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0961, 0.0395, 0.064, 0.0417, 0.036, 0.0363,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.111042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.084, 0.048, 0.0333, 0.0206, 0.0484, 0.0528,...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.074, 0.0594, 0.0444, 0.0354, 0.0313, 0.0477...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.044, 0.0292, 0.0205, 0.011, 0.0288, 0.0289,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0739, 0.0391, 0.0434, 0.0343, 0.0324, 0.035...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0495, 0.0277, 0.0678, 0.1221, 0.023, 0.026,...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0365, 0.0252, 0.039, 0.3165, 0.0277, 0.0207...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.037, 0.0323, 0.0406, 0.3426, 0.0362, 0.0252...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0909, 0.0351, 0.0892, 0.1244, 0.0523, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0647, 0.0503, 0.0114, 0.0074, 0.1142, 0.055...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0376, 0.0807, 0.0068, 0.008, 0.0893, 0.0501...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0414, 0.0648, 0.0156, 0.0099, 0.0254, 0.059...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0217, 0.0558, 0.0231, 0.0127, 0.0215, 0.023...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0074, 0.0321, 0.0184, 0.0147, 0.0168, 0.010...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0333, 0.0338, 0.0115, 0.01, 0.044, 0.0256, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.08, 0.0746, 0.0224, 0.0044, 0.0309, 0.0551,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0569, 0.0258, 0.0662, 0.0853, 0.0294, 0.028...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0399, 0.0237, 0.0408, 0.0404, 0.0268, 0.023...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0649, 0.0497, 0.0284, 0.0187, 0.052, 0.059,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.089, 0.0704, 0.0302, 0.0133, 0.0695, 0.0815...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0727, 0.035, 0.0522, 0.0207, 0.0363, 0.0433...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0213, 0.0244, 0.0173, 0.0381, 0.0161, 0.018...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0571, 0.0273, 0.0315, 0.013, 0.0393, 0.0401...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0672, 0.0728, 0.0294, 0.0074, 0.0562, 0.084...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0738, 0.0288, 0.0575, 0.0554, 0.0318, 0.035...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0675, 0.0311, 0.0433, 0.0383, 0.0356, 0.025...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0689, 0.0336, 0.0366, 0.0283, 0.0491, 0.028...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0392, 0.0246, 0.0237, 0.0269, 0.0344, 0.023...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0121, 0.0241, 0.0116, 0.0137, 0.0241, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0151, 0.0239, 0.0085, 0.0078, 0.0233, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0321, 0.0852, 0.0315, 0.0018, 0.0874, 0.047...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0582, 0.0297, 0.0626, 0.0562, 0.0305, 0.031...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0363, 0.0214, 0.0347, 0.0498, 0.0192, 0.019...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0589, 0.0356, 0.0423, 0.0483, 0.0342, 0.029...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0619, 0.0451, 0.0349, 0.0161, 0.0349, 0.046...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0476, 0.0228, 0.026, 0.0314, 0.0195, 0.0198...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0342, 0.0303, 0.0381, 0.0961, 0.0168, 0.025...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0045, 0.0169, 0.0076, 0.0334, 0.003, 0.0084...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.05, 0.0495, 0.0254, 0.0783, 0.1001, 0.0353,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0452, 0.0341, 0.0283, 0.0719, 0.0541, 0.029...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0346, 0.0309, 0.0217, 0.0181, 0.0283, 0.017...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.05, 0.0297, 0.042, 0.1003, 0.0309, 0.0253, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0156, 0.0223, 0.0216, 0.0801, 0.007, 0.0132...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0039, 0.0124, 0.0071, 0.0656, 0.0017, 0.004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.004, 0.0099, 0.0065, 0.048, 0.0016, 0.0038,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0086, 0.0157, 0.0152, 0.0995, 0.0042, 0.007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0573, 0.0319, 0.0645, 0.017, 0.0357, 0.0294...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0783, 0.029, 0.0618, 0.0415, 0.0281, 0.0345...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0511, 0.0635, 0.0151, 0.0058, 0.0694, 0.041...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0716, 0.1312, 0.0179, 0.0064, 0.0441, 0.090...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0579, 0.0428, 0.0236, 0.0082, 0.0283, 0.041...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0666, 0.0488, 0.0317, 0.018, 0.0313, 0.0563...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0961, 0.0395, 0.064, 0.0417, 0.036, 0.0363,...               0   \n",
       "1   [0.084, 0.048, 0.0333, 0.0206, 0.0484, 0.0528,...               0   \n",
       "2   [0.074, 0.0594, 0.0444, 0.0354, 0.0313, 0.0477...               0   \n",
       "3   [0.044, 0.0292, 0.0205, 0.011, 0.0288, 0.0289,...               1   \n",
       "4   [0.0739, 0.0391, 0.0434, 0.0343, 0.0324, 0.035...               1   \n",
       "5   [0.0495, 0.0277, 0.0678, 0.1221, 0.023, 0.026,...               2   \n",
       "6   [0.0365, 0.0252, 0.039, 0.3165, 0.0277, 0.0207...               3   \n",
       "7   [0.037, 0.0323, 0.0406, 0.3426, 0.0362, 0.0252...               3   \n",
       "8   [0.0909, 0.0351, 0.0892, 0.1244, 0.0523, 0.050...               3   \n",
       "9   [0.0647, 0.0503, 0.0114, 0.0074, 0.1142, 0.055...               4   \n",
       "10  [0.0376, 0.0807, 0.0068, 0.008, 0.0893, 0.0501...               4   \n",
       "11  [0.0414, 0.0648, 0.0156, 0.0099, 0.0254, 0.059...               5   \n",
       "12  [0.0217, 0.0558, 0.0231, 0.0127, 0.0215, 0.023...               6   \n",
       "13  [0.0074, 0.0321, 0.0184, 0.0147, 0.0168, 0.010...               7   \n",
       "14  [0.0333, 0.0338, 0.0115, 0.01, 0.044, 0.0256, ...               8   \n",
       "15  [0.08, 0.0746, 0.0224, 0.0044, 0.0309, 0.0551,...               8   \n",
       "16  [0.0569, 0.0258, 0.0662, 0.0853, 0.0294, 0.028...               9   \n",
       "17  [0.0399, 0.0237, 0.0408, 0.0404, 0.0268, 0.023...               9   \n",
       "18  [0.0649, 0.0497, 0.0284, 0.0187, 0.052, 0.059,...              10   \n",
       "19  [0.089, 0.0704, 0.0302, 0.0133, 0.0695, 0.0815...              10   \n",
       "20  [0.0727, 0.035, 0.0522, 0.0207, 0.0363, 0.0433...              11   \n",
       "21  [0.0213, 0.0244, 0.0173, 0.0381, 0.0161, 0.018...              11   \n",
       "22  [0.0571, 0.0273, 0.0315, 0.013, 0.0393, 0.0401...              12   \n",
       "23  [0.0672, 0.0728, 0.0294, 0.0074, 0.0562, 0.084...              13   \n",
       "24  [0.0738, 0.0288, 0.0575, 0.0554, 0.0318, 0.035...              13   \n",
       "25  [0.0675, 0.0311, 0.0433, 0.0383, 0.0356, 0.025...              14   \n",
       "26  [0.0689, 0.0336, 0.0366, 0.0283, 0.0491, 0.028...              14   \n",
       "27  [0.0392, 0.0246, 0.0237, 0.0269, 0.0344, 0.023...              15   \n",
       "28  [0.0121, 0.0241, 0.0116, 0.0137, 0.0241, 0.015...              15   \n",
       "29  [0.0151, 0.0239, 0.0085, 0.0078, 0.0233, 0.015...              15   \n",
       "30  [0.0321, 0.0852, 0.0315, 0.0018, 0.0874, 0.047...              16   \n",
       "31  [0.0582, 0.0297, 0.0626, 0.0562, 0.0305, 0.031...              17   \n",
       "32  [0.0363, 0.0214, 0.0347, 0.0498, 0.0192, 0.019...              17   \n",
       "33  [0.0589, 0.0356, 0.0423, 0.0483, 0.0342, 0.029...              18   \n",
       "34  [0.0619, 0.0451, 0.0349, 0.0161, 0.0349, 0.046...              19   \n",
       "35  [0.0476, 0.0228, 0.026, 0.0314, 0.0195, 0.0198...              20   \n",
       "36  [0.0342, 0.0303, 0.0381, 0.0961, 0.0168, 0.025...              21   \n",
       "37  [0.0045, 0.0169, 0.0076, 0.0334, 0.003, 0.0084...              21   \n",
       "38  [0.05, 0.0495, 0.0254, 0.0783, 0.1001, 0.0353,...              22   \n",
       "39  [0.0452, 0.0341, 0.0283, 0.0719, 0.0541, 0.029...              22   \n",
       "40  [0.0346, 0.0309, 0.0217, 0.0181, 0.0283, 0.017...              22   \n",
       "41  [0.05, 0.0297, 0.042, 0.1003, 0.0309, 0.0253, ...              22   \n",
       "42  [0.0156, 0.0223, 0.0216, 0.0801, 0.007, 0.0132...              23   \n",
       "43  [0.0039, 0.0124, 0.0071, 0.0656, 0.0017, 0.004...              23   \n",
       "44  [0.004, 0.0099, 0.0065, 0.048, 0.0016, 0.0038,...              23   \n",
       "45  [0.0086, 0.0157, 0.0152, 0.0995, 0.0042, 0.007...              23   \n",
       "46  [0.0573, 0.0319, 0.0645, 0.017, 0.0357, 0.0294...              24   \n",
       "47  [0.0783, 0.029, 0.0618, 0.0415, 0.0281, 0.0345...              24   \n",
       "48  [0.0511, 0.0635, 0.0151, 0.0058, 0.0694, 0.041...              25   \n",
       "49  [0.0716, 0.1312, 0.0179, 0.0064, 0.0441, 0.090...              25   \n",
       "50  [0.0579, 0.0428, 0.0236, 0.0082, 0.0283, 0.041...              26   \n",
       "51  [0.0666, 0.0488, 0.0317, 0.018, 0.0313, 0.0563...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.538462  0.111042  \n",
       "1                  8       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                25       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                25       NaN       NaN  \n",
       "24                 9       NaN       NaN  \n",
       "25                 9       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                 8       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261 : Training: loss:  0.12788269\n",
      "2262 : Training: loss:  0.11017927\n",
      "2263 : Training: loss:  0.09078392\n",
      "2264 : Training: loss:  0.10533499\n",
      "2265 : Training: loss:  0.11172206\n",
      "2266 : Training: loss:  0.11619318\n",
      "2267 : Training: loss:  0.12163768\n",
      "2268 : Training: loss:  0.115799494\n",
      "2269 : Training: loss:  0.076327026\n",
      "2270 : Training: loss:  0.08376678\n",
      "2271 : Training: loss:  0.10660658\n",
      "2272 : Training: loss:  0.11597654\n",
      "2273 : Training: loss:  0.112421244\n",
      "2274 : Training: loss:  0.12075218\n",
      "2275 : Training: loss:  0.1055025\n",
      "2276 : Training: loss:  0.107400835\n",
      "2277 : Training: loss:  0.097102344\n",
      "2278 : Training: loss:  0.13760935\n",
      "2279 : Training: loss:  0.12148663\n",
      "2280 : Training: loss:  0.10611545\n",
      "Validation: Loss:  0.11036287  Accuracy:  0.5769231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0984, 0.0406, 0.0637, 0.0404, 0.038, 0.039,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.110363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0842, 0.0491, 0.0321, 0.0197, 0.0509, 0.056...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.075, 0.0616, 0.044, 0.0345, 0.033, 0.0519, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0447, 0.0305, 0.0202, 0.0106, 0.0312, 0.032...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0758, 0.0407, 0.0433, 0.0334, 0.0347, 0.038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0498, 0.0278, 0.0671, 0.1192, 0.0238, 0.027...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0371, 0.0258, 0.0389, 0.3159, 0.0298, 0.022...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0372, 0.0329, 0.0404, 0.3419, 0.0389, 0.026...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0922, 0.0353, 0.0883, 0.1222, 0.0544, 0.052...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0652, 0.0526, 0.011, 0.0071, 0.1248, 0.0616...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0372, 0.0841, 0.0065, 0.0078, 0.0971, 0.055...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0413, 0.0674, 0.0152, 0.0096, 0.0267, 0.065...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0215, 0.056, 0.0225, 0.0124, 0.022, 0.0246,...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0069, 0.0301, 0.0171, 0.0138, 0.0163, 0.010...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0336, 0.0353, 0.0112, 0.0097, 0.0488, 0.028...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0797, 0.077, 0.0216, 0.0042, 0.0323, 0.06, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0571, 0.0258, 0.0648, 0.0825, 0.0304, 0.029...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0396, 0.0235, 0.0394, 0.0386, 0.0275, 0.024...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0663, 0.0521, 0.0283, 0.0184, 0.0561, 0.065...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0907, 0.0739, 0.03, 0.0129, 0.0747, 0.0903,...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0745, 0.036, 0.0523, 0.0202, 0.0386, 0.0471...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0218, 0.0256, 0.0174, 0.0377, 0.0176, 0.021...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0593, 0.0287, 0.0321, 0.0128, 0.0432, 0.044...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.068, 0.0764, 0.0292, 0.0072, 0.0604, 0.0943...</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0752, 0.0295, 0.0572, 0.0538, 0.0337, 0.037...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0677, 0.0316, 0.0422, 0.0369, 0.0371, 0.026...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0714, 0.0351, 0.0368, 0.028, 0.0536, 0.0308...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0385, 0.0246, 0.0225, 0.0256, 0.0359, 0.025...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0111, 0.0225, 0.0105, 0.0127, 0.0234, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0142, 0.023, 0.0077, 0.0073, 0.0232, 0.0159...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.031, 0.0844, 0.0302, 0.0017, 0.0905, 0.0511...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0587, 0.0298, 0.0615, 0.0545, 0.0314, 0.033...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0368, 0.0215, 0.0341, 0.0485, 0.0199, 0.020...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0605, 0.0369, 0.0423, 0.0475, 0.037, 0.0319...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0631, 0.047, 0.0348, 0.0157, 0.0376, 0.0512...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.049, 0.0235, 0.0259, 0.0306, 0.0209, 0.0215...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0348, 0.0312, 0.0382, 0.0955, 0.0177, 0.027...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0044, 0.0172, 0.0074, 0.0329, 0.0031, 0.009...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0508, 0.0518, 0.0252, 0.0779, 0.1112, 0.038...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0464, 0.0358, 0.0284, 0.0716, 0.0603, 0.032...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0349, 0.0319, 0.0213, 0.0175, 0.0303, 0.018...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0511, 0.0307, 0.0422, 0.0991, 0.0334, 0.027...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0156, 0.0224, 0.0214, 0.079, 0.0073, 0.0141...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0038, 0.0124, 0.0069, 0.0644, 0.0017, 0.005...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0039, 0.0097, 0.0062, 0.0461, 0.0017, 0.004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0085, 0.0157, 0.0149, 0.0981, 0.0044, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0587, 0.0327, 0.0653, 0.0168, 0.0376, 0.031...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0803, 0.0296, 0.0618, 0.0404, 0.0296, 0.037...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0507, 0.066, 0.0146, 0.0056, 0.0747, 0.0452...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0709, 0.1364, 0.0172, 0.0062, 0.046, 0.0996...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0564, 0.0426, 0.0222, 0.0077, 0.0286, 0.043...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0669, 0.05, 0.031, 0.0174, 0.0326, 0.0611, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0984, 0.0406, 0.0637, 0.0404, 0.038, 0.039,...               0   \n",
       "1   [0.0842, 0.0491, 0.0321, 0.0197, 0.0509, 0.056...               0   \n",
       "2   [0.075, 0.0616, 0.044, 0.0345, 0.033, 0.0519, ...               0   \n",
       "3   [0.0447, 0.0305, 0.0202, 0.0106, 0.0312, 0.032...               1   \n",
       "4   [0.0758, 0.0407, 0.0433, 0.0334, 0.0347, 0.038...               1   \n",
       "5   [0.0498, 0.0278, 0.0671, 0.1192, 0.0238, 0.027...               2   \n",
       "6   [0.0371, 0.0258, 0.0389, 0.3159, 0.0298, 0.022...               3   \n",
       "7   [0.0372, 0.0329, 0.0404, 0.3419, 0.0389, 0.026...               3   \n",
       "8   [0.0922, 0.0353, 0.0883, 0.1222, 0.0544, 0.052...               3   \n",
       "9   [0.0652, 0.0526, 0.011, 0.0071, 0.1248, 0.0616...               4   \n",
       "10  [0.0372, 0.0841, 0.0065, 0.0078, 0.0971, 0.055...               4   \n",
       "11  [0.0413, 0.0674, 0.0152, 0.0096, 0.0267, 0.065...               5   \n",
       "12  [0.0215, 0.056, 0.0225, 0.0124, 0.022, 0.0246,...               6   \n",
       "13  [0.0069, 0.0301, 0.0171, 0.0138, 0.0163, 0.010...               7   \n",
       "14  [0.0336, 0.0353, 0.0112, 0.0097, 0.0488, 0.028...               8   \n",
       "15  [0.0797, 0.077, 0.0216, 0.0042, 0.0323, 0.06, ...               8   \n",
       "16  [0.0571, 0.0258, 0.0648, 0.0825, 0.0304, 0.029...               9   \n",
       "17  [0.0396, 0.0235, 0.0394, 0.0386, 0.0275, 0.024...               9   \n",
       "18  [0.0663, 0.0521, 0.0283, 0.0184, 0.0561, 0.065...              10   \n",
       "19  [0.0907, 0.0739, 0.03, 0.0129, 0.0747, 0.0903,...              10   \n",
       "20  [0.0745, 0.036, 0.0523, 0.0202, 0.0386, 0.0471...              11   \n",
       "21  [0.0218, 0.0256, 0.0174, 0.0377, 0.0176, 0.021...              11   \n",
       "22  [0.0593, 0.0287, 0.0321, 0.0128, 0.0432, 0.044...              12   \n",
       "23  [0.068, 0.0764, 0.0292, 0.0072, 0.0604, 0.0943...              13   \n",
       "24  [0.0752, 0.0295, 0.0572, 0.0538, 0.0337, 0.037...              13   \n",
       "25  [0.0677, 0.0316, 0.0422, 0.0369, 0.0371, 0.026...              14   \n",
       "26  [0.0714, 0.0351, 0.0368, 0.028, 0.0536, 0.0308...              14   \n",
       "27  [0.0385, 0.0246, 0.0225, 0.0256, 0.0359, 0.025...              15   \n",
       "28  [0.0111, 0.0225, 0.0105, 0.0127, 0.0234, 0.015...              15   \n",
       "29  [0.0142, 0.023, 0.0077, 0.0073, 0.0232, 0.0159...              15   \n",
       "30  [0.031, 0.0844, 0.0302, 0.0017, 0.0905, 0.0511...              16   \n",
       "31  [0.0587, 0.0298, 0.0615, 0.0545, 0.0314, 0.033...              17   \n",
       "32  [0.0368, 0.0215, 0.0341, 0.0485, 0.0199, 0.020...              17   \n",
       "33  [0.0605, 0.0369, 0.0423, 0.0475, 0.037, 0.0319...              18   \n",
       "34  [0.0631, 0.047, 0.0348, 0.0157, 0.0376, 0.0512...              19   \n",
       "35  [0.049, 0.0235, 0.0259, 0.0306, 0.0209, 0.0215...              20   \n",
       "36  [0.0348, 0.0312, 0.0382, 0.0955, 0.0177, 0.027...              21   \n",
       "37  [0.0044, 0.0172, 0.0074, 0.0329, 0.0031, 0.009...              21   \n",
       "38  [0.0508, 0.0518, 0.0252, 0.0779, 0.1112, 0.038...              22   \n",
       "39  [0.0464, 0.0358, 0.0284, 0.0716, 0.0603, 0.032...              22   \n",
       "40  [0.0349, 0.0319, 0.0213, 0.0175, 0.0303, 0.018...              22   \n",
       "41  [0.0511, 0.0307, 0.0422, 0.0991, 0.0334, 0.027...              22   \n",
       "42  [0.0156, 0.0224, 0.0214, 0.079, 0.0073, 0.0141...              23   \n",
       "43  [0.0038, 0.0124, 0.0069, 0.0644, 0.0017, 0.005...              23   \n",
       "44  [0.0039, 0.0097, 0.0062, 0.0461, 0.0017, 0.004...              23   \n",
       "45  [0.0085, 0.0157, 0.0149, 0.0981, 0.0044, 0.008...              23   \n",
       "46  [0.0587, 0.0327, 0.0653, 0.0168, 0.0376, 0.031...              24   \n",
       "47  [0.0803, 0.0296, 0.0618, 0.0404, 0.0296, 0.037...              24   \n",
       "48  [0.0507, 0.066, 0.0146, 0.0056, 0.0747, 0.0452...              25   \n",
       "49  [0.0709, 0.1364, 0.0172, 0.0062, 0.046, 0.0996...              25   \n",
       "50  [0.0564, 0.0426, 0.0222, 0.0077, 0.0286, 0.043...              26   \n",
       "51  [0.0669, 0.05, 0.031, 0.0174, 0.0326, 0.0611, ...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.576923  0.110363  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 8       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                25       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                16       NaN       NaN  \n",
       "24                 9       NaN       NaN  \n",
       "25                 9       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2281 : Training: loss:  0.06932295\n",
      "2282 : Training: loss:  0.06137576\n",
      "2283 : Training: loss:  0.101344265\n",
      "2284 : Training: loss:  0.09698493\n",
      "2285 : Training: loss:  0.12448539\n",
      "2286 : Training: loss:  0.05488432\n",
      "2287 : Training: loss:  0.09176959\n",
      "2288 : Training: loss:  0.11284403\n",
      "2289 : Training: loss:  0.12858544\n",
      "2290 : Training: loss:  0.099953376\n",
      "2291 : Training: loss:  0.10810483\n",
      "2292 : Training: loss:  0.11221009\n",
      "2293 : Training: loss:  0.098844975\n",
      "2294 : Training: loss:  0.07769853\n",
      "2295 : Training: loss:  0.12344486\n",
      "2296 : Training: loss:  0.08021049\n",
      "2297 : Training: loss:  0.12964727\n",
      "2298 : Training: loss:  0.09505485\n",
      "2299 : Training: loss:  0.10163093\n",
      "2300 : Training: loss:  0.06327308\n",
      "Validation: Loss:  0.10966103  Accuracy:  0.5769231\n",
      "2301 : Training: loss:  0.09600283\n",
      "2302 : Training: loss:  0.11649461\n",
      "2303 : Training: loss:  0.077598095\n",
      "2304 : Training: loss:  0.1169836\n",
      "2305 : Training: loss:  0.1252999\n",
      "2306 : Training: loss:  0.08003224\n",
      "2307 : Training: loss:  0.10824909\n",
      "2308 : Training: loss:  0.08032988\n",
      "2309 : Training: loss:  0.073543794\n",
      "2310 : Training: loss:  0.11900244\n",
      "2311 : Training: loss:  0.102836184\n",
      "2312 : Training: loss:  0.09921922\n",
      "2313 : Training: loss:  0.1009766\n",
      "2314 : Training: loss:  0.13989818\n",
      "2315 : Training: loss:  0.12492619\n",
      "2316 : Training: loss:  0.11368119\n",
      "2317 : Training: loss:  0.13184166\n",
      "2318 : Training: loss:  0.079897135\n",
      "2319 : Training: loss:  0.057515033\n",
      "2320 : Training: loss:  0.065620385\n",
      "Validation: Loss:  0.10936472  Accuracy:  0.53846157\n",
      "2321 : Training: loss:  0.09769416\n",
      "2322 : Training: loss:  0.058915608\n",
      "2323 : Training: loss:  0.124630444\n",
      "2324 : Training: loss:  0.09725361\n",
      "2325 : Training: loss:  0.12532416\n",
      "2326 : Training: loss:  0.13494655\n",
      "2327 : Training: loss:  0.10932005\n",
      "2328 : Training: loss:  0.123556264\n",
      "2329 : Training: loss:  0.1050097\n",
      "2330 : Training: loss:  0.07479603\n",
      "2331 : Training: loss:  0.090190955\n",
      "2332 : Training: loss:  0.12209759\n",
      "2333 : Training: loss:  0.11777574\n",
      "2334 : Training: loss:  0.109278426\n",
      "2335 : Training: loss:  0.13725501\n",
      "2336 : Training: loss:  0.06001007\n",
      "2337 : Training: loss:  0.13221253\n",
      "2338 : Training: loss:  0.12370113\n",
      "2339 : Training: loss:  0.08939154\n",
      "2340 : Training: loss:  0.08782901\n",
      "Validation: Loss:  0.108910576  Accuracy:  0.5769231\n",
      "2341 : Training: loss:  0.12100077\n",
      "2342 : Training: loss:  0.12862608\n",
      "2343 : Training: loss:  0.09345641\n",
      "2344 : Training: loss:  0.08006215\n",
      "2345 : Training: loss:  0.10076696\n",
      "2346 : Training: loss:  0.07596412\n",
      "2347 : Training: loss:  0.09532291\n",
      "2348 : Training: loss:  0.08776986\n",
      "2349 : Training: loss:  0.06336307\n",
      "2350 : Training: loss:  0.111175016\n",
      "2351 : Training: loss:  0.11527076\n",
      "2352 : Training: loss:  0.102436125\n",
      "2353 : Training: loss:  0.15078737\n",
      "2354 : Training: loss:  0.12303061\n",
      "2355 : Training: loss:  0.09361593\n",
      "2356 : Training: loss:  0.106779784\n",
      "2357 : Training: loss:  0.11332236\n",
      "2358 : Training: loss:  0.09974391\n",
      "2359 : Training: loss:  0.116196476\n",
      "2360 : Training: loss:  0.08947527\n",
      "Validation: Loss:  0.10829702  Accuracy:  0.59615386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0994, 0.0393, 0.0653, 0.0372, 0.0354, 0.042...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.108297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0862, 0.049, 0.0311, 0.0177, 0.0512, 0.0664...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0763, 0.0634, 0.0444, 0.0318, 0.0316, 0.059...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0445, 0.0304, 0.0196, 0.0094, 0.0307, 0.037...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0764, 0.04, 0.0437, 0.0309, 0.0329, 0.0435,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0476, 0.0264, 0.0691, 0.1168, 0.0213, 0.028...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0352, 0.0244, 0.0405, 0.3376, 0.0277, 0.022...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0356, 0.0318, 0.0422, 0.3701, 0.0373, 0.027...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0881, 0.0321, 0.0884, 0.1195, 0.0492, 0.053...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.068, 0.0543, 0.01, 0.0064, 0.1423, 0.0776, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0395, 0.0951, 0.0061, 0.0075, 0.1173, 0.074...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0423, 0.0738, 0.0144, 0.0089, 0.0275, 0.084...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0193, 0.0553, 0.0194, 0.0106, 0.0202, 0.024...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0057, 0.0295, 0.0147, 0.012, 0.0146, 0.0098...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0334, 0.0363, 0.0107, 0.0091, 0.0509, 0.033...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0847, 0.0825, 0.0209, 0.0037, 0.033, 0.0756...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.054, 0.0238, 0.064, 0.0769, 0.0275, 0.0302,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0358, 0.021, 0.0359, 0.0337, 0.0245, 0.0246...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0652, 0.0522, 0.0271, 0.0166, 0.0558, 0.075...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0918, 0.0757, 0.0285, 0.0117, 0.0761, 0.107...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0735, 0.0347, 0.0524, 0.0181, 0.0355, 0.051...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0203, 0.0253, 0.0169, 0.0367, 0.016, 0.0227...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0585, 0.0279, 0.0323, 0.0119, 0.0404, 0.050...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0681, 0.0803, 0.0275, 0.0064, 0.0611, 0.113...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0738, 0.0278, 0.0583, 0.0505, 0.031, 0.0404...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0646, 0.0292, 0.04, 0.0326, 0.0339, 0.0272,...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0686, 0.0331, 0.0356, 0.0251, 0.0507, 0.032...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0356, 0.0226, 0.0205, 0.0226, 0.0337, 0.026...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0092, 0.0198, 0.0083, 0.0108, 0.0205, 0.014...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0119, 0.0203, 0.006, 0.0059, 0.0209, 0.0154...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0245, 0.0738, 0.0236, 0.0015, 0.0761, 0.049...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0551, 0.0276, 0.0595, 0.0495, 0.0283, 0.033...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0342, 0.0198, 0.0328, 0.045, 0.0175, 0.0203...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0608, 0.0368, 0.0438, 0.0465, 0.0355, 0.035...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0631, 0.0478, 0.0346, 0.0144, 0.0361, 0.059...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0487, 0.0224, 0.0259, 0.0293, 0.019, 0.0231...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0333, 0.0312, 0.0391, 0.0945, 0.0163, 0.029...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0038, 0.0175, 0.0068, 0.0316, 0.0027, 0.009...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0461, 0.0489, 0.0233, 0.0737, 0.1098, 0.039...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0437, 0.0342, 0.0277, 0.0695, 0.0589, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0319, 0.0302, 0.0195, 0.015, 0.0275, 0.019,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0492, 0.0295, 0.0428, 0.0977, 0.0311, 0.028...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0145, 0.0226, 0.0212, 0.0784, 0.0064, 0.014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0033, 0.0128, 0.0065, 0.0643, 0.0015, 0.005...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0033, 0.0095, 0.0054, 0.0426, 0.0014, 0.003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0075, 0.0155, 0.0144, 0.0982, 0.0037, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0534, 0.0296, 0.0624, 0.0148, 0.0322, 0.030...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0802, 0.0283, 0.0644, 0.0384, 0.0268, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0508, 0.0683, 0.0132, 0.0048, 0.0796, 0.054...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0736, 0.1498, 0.0158, 0.0056, 0.0486, 0.127...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0562, 0.0417, 0.0206, 0.0067, 0.0272, 0.050...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0675, 0.0506, 0.0302, 0.0159, 0.0313, 0.071...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0994, 0.0393, 0.0653, 0.0372, 0.0354, 0.042...               0   \n",
       "1   [0.0862, 0.049, 0.0311, 0.0177, 0.0512, 0.0664...               0   \n",
       "2   [0.0763, 0.0634, 0.0444, 0.0318, 0.0316, 0.059...               0   \n",
       "3   [0.0445, 0.0304, 0.0196, 0.0094, 0.0307, 0.037...               1   \n",
       "4   [0.0764, 0.04, 0.0437, 0.0309, 0.0329, 0.0435,...               1   \n",
       "5   [0.0476, 0.0264, 0.0691, 0.1168, 0.0213, 0.028...               2   \n",
       "6   [0.0352, 0.0244, 0.0405, 0.3376, 0.0277, 0.022...               3   \n",
       "7   [0.0356, 0.0318, 0.0422, 0.3701, 0.0373, 0.027...               3   \n",
       "8   [0.0881, 0.0321, 0.0884, 0.1195, 0.0492, 0.053...               3   \n",
       "9   [0.068, 0.0543, 0.01, 0.0064, 0.1423, 0.0776, ...               4   \n",
       "10  [0.0395, 0.0951, 0.0061, 0.0075, 0.1173, 0.074...               4   \n",
       "11  [0.0423, 0.0738, 0.0144, 0.0089, 0.0275, 0.084...               5   \n",
       "12  [0.0193, 0.0553, 0.0194, 0.0106, 0.0202, 0.024...               6   \n",
       "13  [0.0057, 0.0295, 0.0147, 0.012, 0.0146, 0.0098...               7   \n",
       "14  [0.0334, 0.0363, 0.0107, 0.0091, 0.0509, 0.033...               8   \n",
       "15  [0.0847, 0.0825, 0.0209, 0.0037, 0.033, 0.0756...               8   \n",
       "16  [0.054, 0.0238, 0.064, 0.0769, 0.0275, 0.0302,...               9   \n",
       "17  [0.0358, 0.021, 0.0359, 0.0337, 0.0245, 0.0246...               9   \n",
       "18  [0.0652, 0.0522, 0.0271, 0.0166, 0.0558, 0.075...              10   \n",
       "19  [0.0918, 0.0757, 0.0285, 0.0117, 0.0761, 0.107...              10   \n",
       "20  [0.0735, 0.0347, 0.0524, 0.0181, 0.0355, 0.051...              11   \n",
       "21  [0.0203, 0.0253, 0.0169, 0.0367, 0.016, 0.0227...              11   \n",
       "22  [0.0585, 0.0279, 0.0323, 0.0119, 0.0404, 0.050...              12   \n",
       "23  [0.0681, 0.0803, 0.0275, 0.0064, 0.0611, 0.113...              13   \n",
       "24  [0.0738, 0.0278, 0.0583, 0.0505, 0.031, 0.0404...              13   \n",
       "25  [0.0646, 0.0292, 0.04, 0.0326, 0.0339, 0.0272,...              14   \n",
       "26  [0.0686, 0.0331, 0.0356, 0.0251, 0.0507, 0.032...              14   \n",
       "27  [0.0356, 0.0226, 0.0205, 0.0226, 0.0337, 0.026...              15   \n",
       "28  [0.0092, 0.0198, 0.0083, 0.0108, 0.0205, 0.014...              15   \n",
       "29  [0.0119, 0.0203, 0.006, 0.0059, 0.0209, 0.0154...              15   \n",
       "30  [0.0245, 0.0738, 0.0236, 0.0015, 0.0761, 0.049...              16   \n",
       "31  [0.0551, 0.0276, 0.0595, 0.0495, 0.0283, 0.033...              17   \n",
       "32  [0.0342, 0.0198, 0.0328, 0.045, 0.0175, 0.0203...              17   \n",
       "33  [0.0608, 0.0368, 0.0438, 0.0465, 0.0355, 0.035...              18   \n",
       "34  [0.0631, 0.0478, 0.0346, 0.0144, 0.0361, 0.059...              19   \n",
       "35  [0.0487, 0.0224, 0.0259, 0.0293, 0.019, 0.0231...              20   \n",
       "36  [0.0333, 0.0312, 0.0391, 0.0945, 0.0163, 0.029...              21   \n",
       "37  [0.0038, 0.0175, 0.0068, 0.0316, 0.0027, 0.009...              21   \n",
       "38  [0.0461, 0.0489, 0.0233, 0.0737, 0.1098, 0.039...              22   \n",
       "39  [0.0437, 0.0342, 0.0277, 0.0695, 0.0589, 0.034...              22   \n",
       "40  [0.0319, 0.0302, 0.0195, 0.015, 0.0275, 0.019,...              22   \n",
       "41  [0.0492, 0.0295, 0.0428, 0.0977, 0.0311, 0.028...              22   \n",
       "42  [0.0145, 0.0226, 0.0212, 0.0784, 0.0064, 0.014...              23   \n",
       "43  [0.0033, 0.0128, 0.0065, 0.0643, 0.0015, 0.005...              23   \n",
       "44  [0.0033, 0.0095, 0.0054, 0.0426, 0.0014, 0.003...              23   \n",
       "45  [0.0075, 0.0155, 0.0144, 0.0982, 0.0037, 0.008...              23   \n",
       "46  [0.0534, 0.0296, 0.0624, 0.0148, 0.0322, 0.030...              24   \n",
       "47  [0.0802, 0.0283, 0.0644, 0.0384, 0.0268, 0.039...              24   \n",
       "48  [0.0508, 0.0683, 0.0132, 0.0048, 0.0796, 0.054...              25   \n",
       "49  [0.0736, 0.1498, 0.0158, 0.0056, 0.0486, 0.127...              25   \n",
       "50  [0.0562, 0.0417, 0.0206, 0.0067, 0.0272, 0.050...              26   \n",
       "51  [0.0675, 0.0506, 0.0302, 0.0159, 0.0313, 0.071...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.596154  0.108297  \n",
       "1                  0       NaN       NaN  \n",
       "2                 13       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                22       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                25       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                25       NaN       NaN  \n",
       "24                 9       NaN       NaN  \n",
       "25                 9       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 8       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361 : Training: loss:  0.13460353\n",
      "2362 : Training: loss:  0.08035517\n",
      "2363 : Training: loss:  0.08880998\n",
      "2364 : Training: loss:  0.109960556\n",
      "2365 : Training: loss:  0.07888128\n",
      "2366 : Training: loss:  0.09436648\n",
      "2367 : Training: loss:  0.1151464\n",
      "2368 : Training: loss:  0.087536775\n",
      "2369 : Training: loss:  0.12006377\n",
      "2370 : Training: loss:  0.04109339\n",
      "2371 : Training: loss:  0.09690735\n",
      "2372 : Training: loss:  0.10161423\n",
      "2373 : Training: loss:  0.13462605\n",
      "2374 : Training: loss:  0.10248045\n",
      "2375 : Training: loss:  0.081137024\n",
      "2376 : Training: loss:  0.095003255\n",
      "2377 : Training: loss:  0.107168704\n",
      "2378 : Training: loss:  0.1110169\n",
      "2379 : Training: loss:  0.12462943\n",
      "2380 : Training: loss:  0.110400304\n",
      "Validation: Loss:  0.10779638  Accuracy:  0.5769231\n",
      "2381 : Training: loss:  0.106778964\n",
      "2382 : Training: loss:  0.10264789\n",
      "2383 : Training: loss:  0.12032652\n",
      "2384 : Training: loss:  0.06996395\n",
      "2385 : Training: loss:  0.098767266\n",
      "2386 : Training: loss:  0.09290043\n",
      "2387 : Training: loss:  0.13056467\n",
      "2388 : Training: loss:  0.12381873\n",
      "2389 : Training: loss:  0.112628974\n",
      "2390 : Training: loss:  0.11161768\n",
      "2391 : Training: loss:  0.13365048\n",
      "2392 : Training: loss:  0.08114719\n",
      "2393 : Training: loss:  0.117134534\n",
      "2394 : Training: loss:  0.08514539\n",
      "2395 : Training: loss:  0.09277921\n",
      "2396 : Training: loss:  0.12018103\n",
      "2397 : Training: loss:  0.11455449\n",
      "2398 : Training: loss:  0.09616708\n",
      "2399 : Training: loss:  0.110660166\n",
      "2400 : Training: loss:  0.08457607\n",
      "Validation: Loss:  0.10721874  Accuracy:  0.5769231\n",
      "2401 : Training: loss:  0.09644565\n",
      "2402 : Training: loss:  0.089931294\n",
      "2403 : Training: loss:  0.14351767\n",
      "2404 : Training: loss:  0.10540439\n",
      "2405 : Training: loss:  0.0776944\n",
      "2406 : Training: loss:  0.12236619\n",
      "2407 : Training: loss:  0.08945719\n",
      "2408 : Training: loss:  0.10494809\n",
      "2409 : Training: loss:  0.097715005\n",
      "2410 : Training: loss:  0.120792836\n",
      "2411 : Training: loss:  0.12879668\n",
      "2412 : Training: loss:  0.119025655\n",
      "2413 : Training: loss:  0.064177565\n",
      "2414 : Training: loss:  0.10846089\n",
      "2415 : Training: loss:  0.073358804\n",
      "2416 : Training: loss:  0.07545451\n",
      "2417 : Training: loss:  0.100445755\n",
      "2418 : Training: loss:  0.12432953\n",
      "2419 : Training: loss:  0.044556856\n",
      "2420 : Training: loss:  0.09584804\n",
      "Validation: Loss:  0.1066559  Accuracy:  0.5769231\n",
      "2421 : Training: loss:  0.08920016\n",
      "2422 : Training: loss:  0.086211905\n",
      "2423 : Training: loss:  0.100061215\n",
      "2424 : Training: loss:  0.09007514\n",
      "2425 : Training: loss:  0.0728886\n",
      "2426 : Training: loss:  0.10305912\n",
      "2427 : Training: loss:  0.06819525\n",
      "2428 : Training: loss:  0.09608027\n",
      "2429 : Training: loss:  0.123320535\n",
      "2430 : Training: loss:  0.11487774\n",
      "2431 : Training: loss:  0.08386894\n",
      "2432 : Training: loss:  0.12834397\n",
      "2433 : Training: loss:  0.13331896\n",
      "2434 : Training: loss:  0.08533764\n",
      "2435 : Training: loss:  0.08890143\n",
      "2436 : Training: loss:  0.113805525\n",
      "2437 : Training: loss:  0.09835562\n",
      "2438 : Training: loss:  0.10596228\n",
      "2439 : Training: loss:  0.10964295\n",
      "2440 : Training: loss:  0.07906512\n",
      "Validation: Loss:  0.106008984  Accuracy:  0.61538464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1042, 0.0335, 0.0658, 0.0356, 0.0342, 0.044...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.106009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0878, 0.0408, 0.029, 0.0159, 0.0509, 0.0739...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0787, 0.0553, 0.0436, 0.0298, 0.0309, 0.066...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0456, 0.0251, 0.0186, 0.0083, 0.0315, 0.042...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0793, 0.0339, 0.0429, 0.0286, 0.0323, 0.047...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0483, 0.0225, 0.0704, 0.1148, 0.0201, 0.029...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0353, 0.0203, 0.0402, 0.336, 0.0279, 0.0223...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0352, 0.0265, 0.041, 0.3632, 0.038, 0.0273,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0886, 0.0269, 0.0868, 0.1159, 0.0468, 0.054...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.069, 0.0456, 0.0088, 0.0057, 0.1563, 0.0885...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0393, 0.08, 0.0053, 0.0066, 0.1321, 0.0864,...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0419, 0.0626, 0.013, 0.0079, 0.0273, 0.0999...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0176, 0.044, 0.0173, 0.0093, 0.0193, 0.0255...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.005, 0.0239, 0.0131, 0.0102, 0.0134, 0.009,...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0333, 0.0294, 0.0096, 0.008, 0.0567, 0.0364...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0847, 0.0663, 0.0184, 0.0031, 0.032, 0.0835...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0535, 0.0197, 0.063, 0.0729, 0.0255, 0.0309...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.034, 0.0167, 0.0331, 0.0302, 0.0223, 0.0247...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0673, 0.0456, 0.026, 0.0153, 0.0582, 0.0863...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0947, 0.0656, 0.0267, 0.0103, 0.08, 0.1228,...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0764, 0.0296, 0.0523, 0.0168, 0.0348, 0.055...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0203, 0.0212, 0.0162, 0.0341, 0.0162, 0.024...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0619, 0.0239, 0.0319, 0.011, 0.0425, 0.0558...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0691, 0.0702, 0.0256, 0.0056, 0.0631, 0.131...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0771, 0.0237, 0.059, 0.048, 0.0303, 0.0433,...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0649, 0.0237, 0.0389, 0.0301, 0.032, 0.0274...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0703, 0.0279, 0.0346, 0.0236, 0.0516, 0.033...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.034, 0.0178, 0.0185, 0.02, 0.0323, 0.027, 0...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.008, 0.015, 0.007, 0.0091, 0.0184, 0.0136, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0105, 0.0151, 0.005, 0.0051, 0.0191, 0.0151...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0206, 0.0511, 0.0189, 0.0013, 0.0726, 0.044...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0538, 0.0228, 0.0573, 0.0467, 0.026, 0.0347...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0337, 0.0161, 0.0318, 0.0439, 0.0162, 0.020...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0627, 0.0311, 0.0434, 0.0441, 0.0358, 0.037...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.064, 0.0404, 0.033, 0.013, 0.0361, 0.0648, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0509, 0.0182, 0.0255, 0.0274, 0.0186, 0.024...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0335, 0.027, 0.0391, 0.0925, 0.0155, 0.0317...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0034, 0.014, 0.006, 0.0281, 0.0024, 0.0105,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0462, 0.0423, 0.0224, 0.0691, 0.1246, 0.039...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0447, 0.0294, 0.0272, 0.0662, 0.0652, 0.035...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0315, 0.0247, 0.0185, 0.0135, 0.0276, 0.019...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0507, 0.0252, 0.0432, 0.0946, 0.0316, 0.029...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0141, 0.0188, 0.0199, 0.0742, 0.0059, 0.015...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0031, 0.0104, 0.0057, 0.0578, 0.0013, 0.005...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.003, 0.0076, 0.0047, 0.0373, 0.0012, 0.004,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0071, 0.0126, 0.013, 0.0905, 0.0033, 0.0084...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0527, 0.0245, 0.061, 0.0137, 0.03, 0.0307, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0847, 0.024, 0.0653, 0.037, 0.0258, 0.0424,...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0494, 0.0559, 0.0116, 0.0041, 0.0831, 0.059...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.071, 0.1247, 0.0134, 0.0047, 0.0475, 0.1436...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0562, 0.0334, 0.0187, 0.0058, 0.0256, 0.054...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0696, 0.0434, 0.0288, 0.0146, 0.0307, 0.082...</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1042, 0.0335, 0.0658, 0.0356, 0.0342, 0.044...               0   \n",
       "1   [0.0878, 0.0408, 0.029, 0.0159, 0.0509, 0.0739...               0   \n",
       "2   [0.0787, 0.0553, 0.0436, 0.0298, 0.0309, 0.066...               0   \n",
       "3   [0.0456, 0.0251, 0.0186, 0.0083, 0.0315, 0.042...               1   \n",
       "4   [0.0793, 0.0339, 0.0429, 0.0286, 0.0323, 0.047...               1   \n",
       "5   [0.0483, 0.0225, 0.0704, 0.1148, 0.0201, 0.029...               2   \n",
       "6   [0.0353, 0.0203, 0.0402, 0.336, 0.0279, 0.0223...               3   \n",
       "7   [0.0352, 0.0265, 0.041, 0.3632, 0.038, 0.0273,...               3   \n",
       "8   [0.0886, 0.0269, 0.0868, 0.1159, 0.0468, 0.054...               3   \n",
       "9   [0.069, 0.0456, 0.0088, 0.0057, 0.1563, 0.0885...               4   \n",
       "10  [0.0393, 0.08, 0.0053, 0.0066, 0.1321, 0.0864,...               4   \n",
       "11  [0.0419, 0.0626, 0.013, 0.0079, 0.0273, 0.0999...               5   \n",
       "12  [0.0176, 0.044, 0.0173, 0.0093, 0.0193, 0.0255...               6   \n",
       "13  [0.005, 0.0239, 0.0131, 0.0102, 0.0134, 0.009,...               7   \n",
       "14  [0.0333, 0.0294, 0.0096, 0.008, 0.0567, 0.0364...               8   \n",
       "15  [0.0847, 0.0663, 0.0184, 0.0031, 0.032, 0.0835...               8   \n",
       "16  [0.0535, 0.0197, 0.063, 0.0729, 0.0255, 0.0309...               9   \n",
       "17  [0.034, 0.0167, 0.0331, 0.0302, 0.0223, 0.0247...               9   \n",
       "18  [0.0673, 0.0456, 0.026, 0.0153, 0.0582, 0.0863...              10   \n",
       "19  [0.0947, 0.0656, 0.0267, 0.0103, 0.08, 0.1228,...              10   \n",
       "20  [0.0764, 0.0296, 0.0523, 0.0168, 0.0348, 0.055...              11   \n",
       "21  [0.0203, 0.0212, 0.0162, 0.0341, 0.0162, 0.024...              11   \n",
       "22  [0.0619, 0.0239, 0.0319, 0.011, 0.0425, 0.0558...              12   \n",
       "23  [0.0691, 0.0702, 0.0256, 0.0056, 0.0631, 0.131...              13   \n",
       "24  [0.0771, 0.0237, 0.059, 0.048, 0.0303, 0.0433,...              13   \n",
       "25  [0.0649, 0.0237, 0.0389, 0.0301, 0.032, 0.0274...              14   \n",
       "26  [0.0703, 0.0279, 0.0346, 0.0236, 0.0516, 0.033...              14   \n",
       "27  [0.034, 0.0178, 0.0185, 0.02, 0.0323, 0.027, 0...              15   \n",
       "28  [0.008, 0.015, 0.007, 0.0091, 0.0184, 0.0136, ...              15   \n",
       "29  [0.0105, 0.0151, 0.005, 0.0051, 0.0191, 0.0151...              15   \n",
       "30  [0.0206, 0.0511, 0.0189, 0.0013, 0.0726, 0.044...              16   \n",
       "31  [0.0538, 0.0228, 0.0573, 0.0467, 0.026, 0.0347...              17   \n",
       "32  [0.0337, 0.0161, 0.0318, 0.0439, 0.0162, 0.020...              17   \n",
       "33  [0.0627, 0.0311, 0.0434, 0.0441, 0.0358, 0.037...              18   \n",
       "34  [0.064, 0.0404, 0.033, 0.013, 0.0361, 0.0648, ...              19   \n",
       "35  [0.0509, 0.0182, 0.0255, 0.0274, 0.0186, 0.024...              20   \n",
       "36  [0.0335, 0.027, 0.0391, 0.0925, 0.0155, 0.0317...              21   \n",
       "37  [0.0034, 0.014, 0.006, 0.0281, 0.0024, 0.0105,...              21   \n",
       "38  [0.0462, 0.0423, 0.0224, 0.0691, 0.1246, 0.039...              22   \n",
       "39  [0.0447, 0.0294, 0.0272, 0.0662, 0.0652, 0.035...              22   \n",
       "40  [0.0315, 0.0247, 0.0185, 0.0135, 0.0276, 0.019...              22   \n",
       "41  [0.0507, 0.0252, 0.0432, 0.0946, 0.0316, 0.029...              22   \n",
       "42  [0.0141, 0.0188, 0.0199, 0.0742, 0.0059, 0.015...              23   \n",
       "43  [0.0031, 0.0104, 0.0057, 0.0578, 0.0013, 0.005...              23   \n",
       "44  [0.003, 0.0076, 0.0047, 0.0373, 0.0012, 0.004,...              23   \n",
       "45  [0.0071, 0.0126, 0.013, 0.0905, 0.0033, 0.0084...              23   \n",
       "46  [0.0527, 0.0245, 0.061, 0.0137, 0.03, 0.0307, ...              24   \n",
       "47  [0.0847, 0.024, 0.0653, 0.037, 0.0258, 0.0424,...              24   \n",
       "48  [0.0494, 0.0559, 0.0116, 0.0041, 0.0831, 0.059...              25   \n",
       "49  [0.071, 0.1247, 0.0134, 0.0047, 0.0475, 0.1436...              25   \n",
       "50  [0.0562, 0.0334, 0.0187, 0.0058, 0.0256, 0.054...              26   \n",
       "51  [0.0696, 0.0434, 0.0288, 0.0146, 0.0307, 0.082...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.615385  0.106009  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                25       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 4       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                25       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                25       NaN       NaN  \n",
       "24                 9       NaN       NaN  \n",
       "25                 9       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                25       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                 5       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2441 : Training: loss:  0.07826329\n",
      "2442 : Training: loss:  0.10571605\n",
      "2443 : Training: loss:  0.10644611\n",
      "2444 : Training: loss:  0.08207011\n",
      "2445 : Training: loss:  0.11823854\n",
      "2446 : Training: loss:  0.12192353\n",
      "2447 : Training: loss:  0.11910135\n",
      "2448 : Training: loss:  0.09720404\n",
      "2449 : Training: loss:  0.135759\n",
      "2450 : Training: loss:  0.11555172\n",
      "2451 : Training: loss:  0.12013635\n",
      "2452 : Training: loss:  0.083195336\n",
      "2453 : Training: loss:  0.11901408\n",
      "2454 : Training: loss:  0.10613432\n",
      "2455 : Training: loss:  0.09907484\n",
      "2456 : Training: loss:  0.10427512\n",
      "2457 : Training: loss:  0.080967814\n",
      "2458 : Training: loss:  0.122366\n",
      "2459 : Training: loss:  0.10525341\n",
      "2460 : Training: loss:  0.08532608\n",
      "Validation: Loss:  0.10546846  Accuracy:  0.61538464\n",
      "2461 : Training: loss:  0.082974225\n",
      "2462 : Training: loss:  0.10510858\n",
      "2463 : Training: loss:  0.09724666\n",
      "2464 : Training: loss:  0.1002213\n",
      "2465 : Training: loss:  0.08804985\n",
      "2466 : Training: loss:  0.10997178\n",
      "2467 : Training: loss:  0.11886263\n",
      "2468 : Training: loss:  0.09870226\n",
      "2469 : Training: loss:  0.10306393\n",
      "2470 : Training: loss:  0.11255372\n",
      "2471 : Training: loss:  0.09946961\n",
      "2472 : Training: loss:  0.09067965\n",
      "2473 : Training: loss:  0.099971905\n",
      "2474 : Training: loss:  0.11993399\n",
      "2475 : Training: loss:  0.11390954\n",
      "2476 : Training: loss:  0.11554674\n",
      "2477 : Training: loss:  0.12001151\n",
      "2478 : Training: loss:  0.11649009\n",
      "2479 : Training: loss:  0.11047691\n",
      "2480 : Training: loss:  0.120434225\n",
      "Validation: Loss:  0.105064295  Accuracy:  0.5769231\n",
      "2481 : Training: loss:  0.08430122\n",
      "2482 : Training: loss:  0.12636164\n",
      "2483 : Training: loss:  0.10530959\n",
      "2484 : Training: loss:  0.12747528\n",
      "2485 : Training: loss:  0.09252781\n",
      "2486 : Training: loss:  0.08300362\n",
      "2487 : Training: loss:  0.13051373\n",
      "2488 : Training: loss:  0.10050379\n",
      "2489 : Training: loss:  0.07226567\n",
      "2490 : Training: loss:  0.097247474\n",
      "2491 : Training: loss:  0.08187833\n",
      "2492 : Training: loss:  0.122362755\n",
      "2493 : Training: loss:  0.12413572\n",
      "2494 : Training: loss:  0.13081849\n",
      "2495 : Training: loss:  0.11914335\n",
      "2496 : Training: loss:  0.07880891\n",
      "2497 : Training: loss:  0.11646122\n",
      "2498 : Training: loss:  0.07797116\n",
      "2499 : Training: loss:  0.091390654\n",
      "2500 : Training: loss:  0.10813421\n",
      "Validation: Loss:  0.104711324  Accuracy:  0.5769231\n",
      "2501 : Training: loss:  0.103922404\n",
      "2502 : Training: loss:  0.09096289\n",
      "2503 : Training: loss:  0.09785945\n",
      "2504 : Training: loss:  0.102055274\n",
      "2505 : Training: loss:  0.10918036\n",
      "2506 : Training: loss:  0.12226389\n",
      "2507 : Training: loss:  0.12296949\n",
      "2508 : Training: loss:  0.03877224\n",
      "2509 : Training: loss:  0.11894494\n",
      "2510 : Training: loss:  0.092446394\n",
      "2511 : Training: loss:  0.11193314\n",
      "2512 : Training: loss:  0.11104384\n",
      "2513 : Training: loss:  0.1166256\n",
      "2514 : Training: loss:  0.061721493\n",
      "2515 : Training: loss:  0.13228588\n",
      "2516 : Training: loss:  0.11019761\n",
      "2517 : Training: loss:  0.10900259\n",
      "2518 : Training: loss:  0.10579593\n",
      "2519 : Training: loss:  0.11593779\n",
      "2520 : Training: loss:  0.12250989\n",
      "Validation: Loss:  0.10430574  Accuracy:  0.5769231\n",
      "2521 : Training: loss:  0.10802013\n",
      "2522 : Training: loss:  0.083856426\n",
      "2523 : Training: loss:  0.12706977\n",
      "2524 : Training: loss:  0.07283643\n",
      "2525 : Training: loss:  0.0737766\n",
      "2526 : Training: loss:  0.08372444\n",
      "2527 : Training: loss:  0.099063426\n",
      "2528 : Training: loss:  0.09189951\n",
      "2529 : Training: loss:  0.08398321\n",
      "2530 : Training: loss:  0.11940101\n",
      "2531 : Training: loss:  0.025403954\n",
      "2532 : Training: loss:  0.11344065\n",
      "2533 : Training: loss:  0.12831987\n",
      "2534 : Training: loss:  0.09643068\n",
      "2535 : Training: loss:  0.10591417\n",
      "2536 : Training: loss:  0.11095709\n",
      "2537 : Training: loss:  0.10618405\n",
      "2538 : Training: loss:  0.09665023\n",
      "2539 : Training: loss:  0.09453222\n",
      "2540 : Training: loss:  0.104765065\n",
      "Validation: Loss:  0.10383121  Accuracy:  0.59615386\n",
      "2541 : Training: loss:  0.12682062\n",
      "2542 : Training: loss:  0.108711965\n",
      "2543 : Training: loss:  0.06443387\n",
      "2544 : Training: loss:  0.055425785\n",
      "2545 : Training: loss:  0.08791801\n",
      "2546 : Training: loss:  0.13510399\n",
      "2547 : Training: loss:  0.102293506\n",
      "2548 : Training: loss:  0.055658158\n",
      "2549 : Training: loss:  0.091017485\n",
      "2550 : Training: loss:  0.07017759\n",
      "2551 : Training: loss:  0.09731269\n",
      "2552 : Training: loss:  0.09512813\n",
      "2553 : Training: loss:  0.10587596\n",
      "2554 : Training: loss:  0.121437676\n",
      "2555 : Training: loss:  0.09219564\n",
      "2556 : Training: loss:  0.13657957\n",
      "2557 : Training: loss:  0.10698095\n",
      "2558 : Training: loss:  0.10210353\n",
      "2559 : Training: loss:  0.1097567\n",
      "2560 : Training: loss:  0.09733962\n",
      "Validation: Loss:  0.10349606  Accuracy:  0.61538464\n",
      "2561 : Training: loss:  0.071160786\n",
      "2562 : Training: loss:  0.09755572\n",
      "2563 : Training: loss:  0.046113543\n",
      "2564 : Training: loss:  0.10224729\n",
      "2565 : Training: loss:  0.09203072\n",
      "2566 : Training: loss:  0.08870012\n",
      "2567 : Training: loss:  0.119622245\n",
      "2568 : Training: loss:  0.08910742\n",
      "2569 : Training: loss:  0.07638461\n",
      "2570 : Training: loss:  0.124403484\n",
      "2571 : Training: loss:  0.11005892\n",
      "2572 : Training: loss:  0.09674182\n",
      "2573 : Training: loss:  0.110360466\n",
      "2574 : Training: loss:  0.082118124\n",
      "2575 : Training: loss:  0.117192976\n",
      "2576 : Training: loss:  0.08658524\n",
      "2577 : Training: loss:  0.09527268\n",
      "2578 : Training: loss:  0.09036732\n",
      "2579 : Training: loss:  0.08205887\n",
      "2580 : Training: loss:  0.07659112\n",
      "Validation: Loss:  0.10319351  Accuracy:  0.61538464\n",
      "2581 : Training: loss:  0.08381054\n",
      "2582 : Training: loss:  0.102981426\n",
      "2583 : Training: loss:  0.11750677\n",
      "2584 : Training: loss:  0.110306635\n",
      "2585 : Training: loss:  0.107412115\n",
      "2586 : Training: loss:  0.09554182\n",
      "2587 : Training: loss:  0.058584906\n",
      "2588 : Training: loss:  0.093039274\n",
      "2589 : Training: loss:  0.06435481\n",
      "2590 : Training: loss:  0.11701469\n",
      "2591 : Training: loss:  0.11539556\n",
      "2592 : Training: loss:  0.104936965\n",
      "2593 : Training: loss:  0.10898811\n",
      "2594 : Training: loss:  0.052432314\n",
      "2595 : Training: loss:  0.05201774\n",
      "2596 : Training: loss:  0.08803218\n",
      "2597 : Training: loss:  0.12561819\n",
      "2598 : Training: loss:  0.13591442\n",
      "2599 : Training: loss:  0.13262783\n",
      "2600 : Training: loss:  0.123836555\n",
      "Validation: Loss:  0.102790244  Accuracy:  0.63461536\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1247, 0.0396, 0.0561, 0.0372, 0.0314, 0.033...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.10279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1039, 0.0535, 0.0216, 0.0152, 0.0508, 0.060...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0889, 0.0693, 0.0351, 0.0301, 0.0275, 0.052...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0498, 0.0315, 0.0127, 0.0071, 0.0283, 0.030...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.089, 0.0403, 0.0331, 0.0282, 0.0287, 0.0349...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0512, 0.023, 0.061, 0.1349, 0.0173, 0.0212,...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0367, 0.0209, 0.035, 0.409, 0.0255, 0.0159,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0356, 0.0272, 0.0353, 0.429, 0.0348, 0.0195...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0954, 0.0272, 0.0737, 0.1272, 0.0431, 0.041...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0758, 0.0619, 0.0056, 0.0047, 0.1654, 0.070...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.042, 0.1175, 0.0035, 0.0058, 0.1402, 0.0728...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0446, 0.087, 0.0087, 0.0071, 0.0248, 0.0841...</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0162, 0.0486, 0.0125, 0.0089, 0.0183, 0.019...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0041, 0.0234, 0.0103, 0.0112, 0.0113, 0.006...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0365, 0.0381, 0.0065, 0.0072, 0.0575, 0.026...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0996, 0.0896, 0.0127, 0.0026, 0.0291, 0.064...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0558, 0.0203, 0.0518, 0.079, 0.0226, 0.0226...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0333, 0.0171, 0.0251, 0.0307, 0.0197, 0.018...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0709, 0.0577, 0.0188, 0.014, 0.0553, 0.0697...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.1052, 0.0885, 0.0194, 0.0089, 0.0782, 0.105...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0851, 0.0331, 0.0411, 0.0155, 0.0302, 0.040...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0196, 0.0231, 0.0116, 0.0354, 0.0132, 0.016...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0673, 0.0265, 0.0233, 0.0096, 0.037, 0.0402...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0726, 0.0935, 0.018, 0.0046, 0.0588, 0.1127...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0881, 0.0266, 0.0495, 0.0509, 0.0274, 0.032...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0751, 0.0279, 0.0326, 0.0325, 0.03, 0.0203,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0756, 0.0319, 0.0261, 0.0228, 0.0496, 0.024...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0356, 0.0207, 0.0137, 0.02, 0.0306, 0.02, 0...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0071, 0.0161, 0.0052, 0.0096, 0.0166, 0.010...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.01, 0.0176, 0.0034, 0.0052, 0.0179, 0.0113,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0188, 0.0579, 0.0133, 0.0011, 0.0596, 0.034...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0556, 0.0239, 0.0459, 0.0487, 0.0233, 0.026...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0356, 0.0173, 0.0256, 0.0508, 0.0145, 0.015...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0698, 0.0362, 0.0346, 0.0459, 0.0328, 0.027...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0686, 0.0478, 0.0237, 0.0114, 0.0313, 0.048...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0605, 0.0215, 0.0206, 0.0302, 0.0164, 0.017...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0343, 0.0301, 0.0325, 0.1082, 0.0132, 0.024...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0029, 0.0155, 0.0044, 0.0321, 0.0018, 0.007...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0433, 0.0467, 0.0164, 0.0663, 0.1188, 0.028...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0442, 0.0322, 0.0201, 0.0669, 0.0608, 0.024...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0321, 0.0281, 0.0138, 0.0131, 0.0241, 0.012...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.054, 0.0275, 0.0357, 0.1055, 0.0282, 0.0215...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0135, 0.0198, 0.0152, 0.0869, 0.0047, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0028, 0.0106, 0.0039, 0.064, 0.001, 0.0039,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0028, 0.0074, 0.0031, 0.0409, 0.0009, 0.002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0065, 0.0127, 0.0096, 0.1058, 0.0026, 0.005...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.052, 0.024, 0.0479, 0.0124, 0.025, 0.0214, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1001, 0.0267, 0.0562, 0.0396, 0.0229, 0.032...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0517, 0.0765, 0.0075, 0.0033, 0.0821, 0.045...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0761, 0.174, 0.0086, 0.0038, 0.0433, 0.1198...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.066, 0.0424, 0.0139, 0.0054, 0.0231, 0.0431...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0789, 0.0542, 0.0217, 0.014, 0.0278, 0.0673...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1247, 0.0396, 0.0561, 0.0372, 0.0314, 0.033...               0   \n",
       "1   [0.1039, 0.0535, 0.0216, 0.0152, 0.0508, 0.060...               0   \n",
       "2   [0.0889, 0.0693, 0.0351, 0.0301, 0.0275, 0.052...               0   \n",
       "3   [0.0498, 0.0315, 0.0127, 0.0071, 0.0283, 0.030...               1   \n",
       "4   [0.089, 0.0403, 0.0331, 0.0282, 0.0287, 0.0349...               1   \n",
       "5   [0.0512, 0.023, 0.061, 0.1349, 0.0173, 0.0212,...               2   \n",
       "6   [0.0367, 0.0209, 0.035, 0.409, 0.0255, 0.0159,...               3   \n",
       "7   [0.0356, 0.0272, 0.0353, 0.429, 0.0348, 0.0195...               3   \n",
       "8   [0.0954, 0.0272, 0.0737, 0.1272, 0.0431, 0.041...               3   \n",
       "9   [0.0758, 0.0619, 0.0056, 0.0047, 0.1654, 0.070...               4   \n",
       "10  [0.042, 0.1175, 0.0035, 0.0058, 0.1402, 0.0728...               4   \n",
       "11  [0.0446, 0.087, 0.0087, 0.0071, 0.0248, 0.0841...               5   \n",
       "12  [0.0162, 0.0486, 0.0125, 0.0089, 0.0183, 0.019...               6   \n",
       "13  [0.0041, 0.0234, 0.0103, 0.0112, 0.0113, 0.006...               7   \n",
       "14  [0.0365, 0.0381, 0.0065, 0.0072, 0.0575, 0.026...               8   \n",
       "15  [0.0996, 0.0896, 0.0127, 0.0026, 0.0291, 0.064...               8   \n",
       "16  [0.0558, 0.0203, 0.0518, 0.079, 0.0226, 0.0226...               9   \n",
       "17  [0.0333, 0.0171, 0.0251, 0.0307, 0.0197, 0.018...               9   \n",
       "18  [0.0709, 0.0577, 0.0188, 0.014, 0.0553, 0.0697...              10   \n",
       "19  [0.1052, 0.0885, 0.0194, 0.0089, 0.0782, 0.105...              10   \n",
       "20  [0.0851, 0.0331, 0.0411, 0.0155, 0.0302, 0.040...              11   \n",
       "21  [0.0196, 0.0231, 0.0116, 0.0354, 0.0132, 0.016...              11   \n",
       "22  [0.0673, 0.0265, 0.0233, 0.0096, 0.037, 0.0402...              12   \n",
       "23  [0.0726, 0.0935, 0.018, 0.0046, 0.0588, 0.1127...              13   \n",
       "24  [0.0881, 0.0266, 0.0495, 0.0509, 0.0274, 0.032...              13   \n",
       "25  [0.0751, 0.0279, 0.0326, 0.0325, 0.03, 0.0203,...              14   \n",
       "26  [0.0756, 0.0319, 0.0261, 0.0228, 0.0496, 0.024...              14   \n",
       "27  [0.0356, 0.0207, 0.0137, 0.02, 0.0306, 0.02, 0...              15   \n",
       "28  [0.0071, 0.0161, 0.0052, 0.0096, 0.0166, 0.010...              15   \n",
       "29  [0.01, 0.0176, 0.0034, 0.0052, 0.0179, 0.0113,...              15   \n",
       "30  [0.0188, 0.0579, 0.0133, 0.0011, 0.0596, 0.034...              16   \n",
       "31  [0.0556, 0.0239, 0.0459, 0.0487, 0.0233, 0.026...              17   \n",
       "32  [0.0356, 0.0173, 0.0256, 0.0508, 0.0145, 0.015...              17   \n",
       "33  [0.0698, 0.0362, 0.0346, 0.0459, 0.0328, 0.027...              18   \n",
       "34  [0.0686, 0.0478, 0.0237, 0.0114, 0.0313, 0.048...              19   \n",
       "35  [0.0605, 0.0215, 0.0206, 0.0302, 0.0164, 0.017...              20   \n",
       "36  [0.0343, 0.0301, 0.0325, 0.1082, 0.0132, 0.024...              21   \n",
       "37  [0.0029, 0.0155, 0.0044, 0.0321, 0.0018, 0.007...              21   \n",
       "38  [0.0433, 0.0467, 0.0164, 0.0663, 0.1188, 0.028...              22   \n",
       "39  [0.0442, 0.0322, 0.0201, 0.0669, 0.0608, 0.024...              22   \n",
       "40  [0.0321, 0.0281, 0.0138, 0.0131, 0.0241, 0.012...              22   \n",
       "41  [0.054, 0.0275, 0.0357, 0.1055, 0.0282, 0.0215...              22   \n",
       "42  [0.0135, 0.0198, 0.0152, 0.0869, 0.0047, 0.011...              23   \n",
       "43  [0.0028, 0.0106, 0.0039, 0.064, 0.001, 0.0039,...              23   \n",
       "44  [0.0028, 0.0074, 0.0031, 0.0409, 0.0009, 0.002...              23   \n",
       "45  [0.0065, 0.0127, 0.0096, 0.1058, 0.0026, 0.005...              23   \n",
       "46  [0.052, 0.024, 0.0479, 0.0124, 0.025, 0.0214, ...              24   \n",
       "47  [0.1001, 0.0267, 0.0562, 0.0396, 0.0229, 0.032...              24   \n",
       "48  [0.0517, 0.0765, 0.0075, 0.0033, 0.0821, 0.045...              25   \n",
       "49  [0.0761, 0.174, 0.0086, 0.0038, 0.0433, 0.1198...              25   \n",
       "50  [0.066, 0.0424, 0.0139, 0.0054, 0.0231, 0.0431...              26   \n",
       "51  [0.0789, 0.0542, 0.0217, 0.014, 0.0278, 0.0673...              26   \n",
       "\n",
       "    Predicted labels  Accuracy     Loss  \n",
       "0                  0  0.634615  0.10279  \n",
       "1                  0       NaN      NaN  \n",
       "2                  0       NaN      NaN  \n",
       "3                  0       NaN      NaN  \n",
       "4                  0       NaN      NaN  \n",
       "5                  3       NaN      NaN  \n",
       "6                  3       NaN      NaN  \n",
       "7                  3       NaN      NaN  \n",
       "8                  9       NaN      NaN  \n",
       "9                  4       NaN      NaN  \n",
       "10                25       NaN      NaN  \n",
       "11                26       NaN      NaN  \n",
       "12                17       NaN      NaN  \n",
       "13                 7       NaN      NaN  \n",
       "14                 4       NaN      NaN  \n",
       "15                 8       NaN      NaN  \n",
       "16                 9       NaN      NaN  \n",
       "17                 9       NaN      NaN  \n",
       "18                10       NaN      NaN  \n",
       "19                25       NaN      NaN  \n",
       "20                 0       NaN      NaN  \n",
       "21                23       NaN      NaN  \n",
       "22                12       NaN      NaN  \n",
       "23                25       NaN      NaN  \n",
       "24                 0       NaN      NaN  \n",
       "25                14       NaN      NaN  \n",
       "26                 0       NaN      NaN  \n",
       "27                15       NaN      NaN  \n",
       "28                15       NaN      NaN  \n",
       "29                15       NaN      NaN  \n",
       "30                16       NaN      NaN  \n",
       "31                17       NaN      NaN  \n",
       "32                17       NaN      NaN  \n",
       "33                 0       NaN      NaN  \n",
       "34                12       NaN      NaN  \n",
       "35                 0       NaN      NaN  \n",
       "36                23       NaN      NaN  \n",
       "37                23       NaN      NaN  \n",
       "38                22       NaN      NaN  \n",
       "39                22       NaN      NaN  \n",
       "40                22       NaN      NaN  \n",
       "41                22       NaN      NaN  \n",
       "42                23       NaN      NaN  \n",
       "43                23       NaN      NaN  \n",
       "44                23       NaN      NaN  \n",
       "45                23       NaN      NaN  \n",
       "46                24       NaN      NaN  \n",
       "47                24       NaN      NaN  \n",
       "48                25       NaN      NaN  \n",
       "49                25       NaN      NaN  \n",
       "50                26       NaN      NaN  \n",
       "51                26       NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601 : Training: loss:  0.119245306\n",
      "2602 : Training: loss:  0.099639826\n",
      "2603 : Training: loss:  0.106843315\n",
      "2604 : Training: loss:  0.093527496\n",
      "2605 : Training: loss:  0.084549196\n",
      "2606 : Training: loss:  0.06790821\n",
      "2607 : Training: loss:  0.07769724\n",
      "2608 : Training: loss:  0.09033017\n",
      "2609 : Training: loss:  0.08080273\n",
      "2610 : Training: loss:  0.10879182\n",
      "2611 : Training: loss:  0.1170133\n",
      "2612 : Training: loss:  0.080733195\n",
      "2613 : Training: loss:  0.068389714\n",
      "2614 : Training: loss:  0.060745433\n",
      "2615 : Training: loss:  0.08626795\n",
      "2616 : Training: loss:  0.091156006\n",
      "2617 : Training: loss:  0.10730091\n",
      "2618 : Training: loss:  0.111585885\n",
      "2619 : Training: loss:  0.086823195\n",
      "2620 : Training: loss:  0.07193193\n",
      "Validation: Loss:  0.102379344  Accuracy:  0.5769231\n",
      "2621 : Training: loss:  0.10260054\n",
      "2622 : Training: loss:  0.10213315\n",
      "2623 : Training: loss:  0.11477279\n",
      "2624 : Training: loss:  0.11338513\n",
      "2625 : Training: loss:  0.08763255\n",
      "2626 : Training: loss:  0.07199434\n",
      "2627 : Training: loss:  0.099021845\n",
      "2628 : Training: loss:  0.10659586\n",
      "2629 : Training: loss:  0.109056346\n",
      "2630 : Training: loss:  0.09780644\n",
      "2631 : Training: loss:  0.09637371\n",
      "2632 : Training: loss:  0.09200301\n",
      "2633 : Training: loss:  0.07030969\n",
      "2634 : Training: loss:  0.123555034\n",
      "2635 : Training: loss:  0.109842606\n",
      "2636 : Training: loss:  0.11366385\n",
      "2637 : Training: loss:  0.10358053\n",
      "2638 : Training: loss:  0.073929034\n",
      "2639 : Training: loss:  0.080928594\n",
      "2640 : Training: loss:  0.102299504\n",
      "Validation: Loss:  0.10190078  Accuracy:  0.5769231\n",
      "2641 : Training: loss:  0.11824914\n",
      "2642 : Training: loss:  0.043690648\n",
      "2643 : Training: loss:  0.060938265\n",
      "2644 : Training: loss:  0.084701814\n",
      "2645 : Training: loss:  0.10548755\n",
      "2646 : Training: loss:  0.094257325\n",
      "2647 : Training: loss:  0.13698304\n",
      "2648 : Training: loss:  0.08962391\n",
      "2649 : Training: loss:  0.11193686\n",
      "2650 : Training: loss:  0.12777501\n",
      "2651 : Training: loss:  0.10447279\n",
      "2652 : Training: loss:  0.0835224\n",
      "2653 : Training: loss:  0.10698977\n",
      "2654 : Training: loss:  0.07733993\n",
      "2655 : Training: loss:  0.09520275\n",
      "2656 : Training: loss:  0.04917267\n",
      "2657 : Training: loss:  0.08282566\n",
      "2658 : Training: loss:  0.097559445\n",
      "2659 : Training: loss:  0.12671714\n",
      "2660 : Training: loss:  0.08213695\n",
      "Validation: Loss:  0.10153002  Accuracy:  0.61538464\n",
      "2661 : Training: loss:  0.11790706\n",
      "2662 : Training: loss:  0.09802571\n",
      "2663 : Training: loss:  0.08669672\n",
      "2664 : Training: loss:  0.08146958\n",
      "2665 : Training: loss:  0.07591406\n",
      "2666 : Training: loss:  0.103147514\n",
      "2667 : Training: loss:  0.07967981\n",
      "2668 : Training: loss:  0.06971566\n",
      "2669 : Training: loss:  0.11293738\n",
      "2670 : Training: loss:  0.09138337\n",
      "2671 : Training: loss:  0.09220228\n",
      "2672 : Training: loss:  0.10823702\n",
      "2673 : Training: loss:  0.05579489\n",
      "2674 : Training: loss:  0.08672619\n",
      "2675 : Training: loss:  0.09449322\n",
      "2676 : Training: loss:  0.10415764\n",
      "2677 : Training: loss:  0.06111099\n",
      "2678 : Training: loss:  0.13813905\n",
      "2679 : Training: loss:  0.1280146\n",
      "2680 : Training: loss:  0.100584224\n",
      "Validation: Loss:  0.10125647  Accuracy:  0.63461536\n",
      "2681 : Training: loss:  0.12129625\n",
      "2682 : Training: loss:  0.10243101\n",
      "2683 : Training: loss:  0.10653219\n",
      "2684 : Training: loss:  0.10633835\n",
      "2685 : Training: loss:  0.05669342\n",
      "2686 : Training: loss:  0.07832271\n",
      "2687 : Training: loss:  0.08955706\n",
      "2688 : Training: loss:  0.06051494\n",
      "2689 : Training: loss:  0.13503274\n",
      "2690 : Training: loss:  0.09969575\n",
      "2691 : Training: loss:  0.099221885\n",
      "2692 : Training: loss:  0.08042946\n",
      "2693 : Training: loss:  0.11395859\n",
      "2694 : Training: loss:  0.1066696\n",
      "2695 : Training: loss:  0.08805666\n",
      "2696 : Training: loss:  0.09471019\n",
      "2697 : Training: loss:  0.06981163\n",
      "2698 : Training: loss:  0.10581101\n",
      "2699 : Training: loss:  0.0911864\n",
      "2700 : Training: loss:  0.114518985\n",
      "Validation: Loss:  0.10085539  Accuracy:  0.61538464\n",
      "2701 : Training: loss:  0.11650007\n",
      "2702 : Training: loss:  0.06808342\n",
      "2703 : Training: loss:  0.09648322\n",
      "2704 : Training: loss:  0.11124249\n",
      "2705 : Training: loss:  0.109153606\n",
      "2706 : Training: loss:  0.09144193\n",
      "2707 : Training: loss:  0.07579313\n",
      "2708 : Training: loss:  0.09087286\n",
      "2709 : Training: loss:  0.11714161\n",
      "2710 : Training: loss:  0.1252186\n",
      "2711 : Training: loss:  0.06270838\n",
      "2712 : Training: loss:  0.10106917\n",
      "2713 : Training: loss:  0.11587416\n",
      "2714 : Training: loss:  0.061806645\n",
      "2715 : Training: loss:  0.11351379\n",
      "2716 : Training: loss:  0.089639336\n",
      "2717 : Training: loss:  0.11362075\n",
      "2718 : Training: loss:  0.09493708\n",
      "2719 : Training: loss:  0.059552476\n",
      "2720 : Training: loss:  0.11321623\n",
      "Validation: Loss:  0.10045308  Accuracy:  0.59615386\n",
      "2721 : Training: loss:  0.11646246\n",
      "2722 : Training: loss:  0.083236836\n",
      "2723 : Training: loss:  0.08299559\n",
      "2724 : Training: loss:  0.08181964\n",
      "2725 : Training: loss:  0.10647812\n",
      "2726 : Training: loss:  0.091881834\n",
      "2727 : Training: loss:  0.09646937\n",
      "2728 : Training: loss:  0.09221537\n",
      "2729 : Training: loss:  0.123225674\n",
      "2730 : Training: loss:  0.0856423\n",
      "2731 : Training: loss:  0.1434203\n",
      "2732 : Training: loss:  0.11391002\n",
      "2733 : Training: loss:  0.12492312\n",
      "2734 : Training: loss:  0.07182575\n",
      "2735 : Training: loss:  0.094778866\n",
      "2736 : Training: loss:  0.10521725\n",
      "2737 : Training: loss:  0.10406498\n",
      "2738 : Training: loss:  0.10887299\n",
      "2739 : Training: loss:  0.09258097\n",
      "2740 : Training: loss:  0.1119061\n",
      "Validation: Loss:  0.09993222  Accuracy:  0.61538464\n",
      "2741 : Training: loss:  0.10083049\n",
      "2742 : Training: loss:  0.09624535\n",
      "2743 : Training: loss:  0.093154065\n",
      "2744 : Training: loss:  0.08879635\n",
      "2745 : Training: loss:  0.11099805\n",
      "2746 : Training: loss:  0.081824474\n",
      "2747 : Training: loss:  0.11160204\n",
      "2748 : Training: loss:  0.09283299\n",
      "2749 : Training: loss:  0.0711799\n",
      "2750 : Training: loss:  0.1128019\n",
      "2751 : Training: loss:  0.084023386\n",
      "2752 : Training: loss:  0.11308319\n",
      "2753 : Training: loss:  0.084215134\n",
      "2754 : Training: loss:  0.118254706\n",
      "2755 : Training: loss:  0.108887635\n",
      "2756 : Training: loss:  0.102746926\n",
      "2757 : Training: loss:  0.12190217\n",
      "2758 : Training: loss:  0.11194958\n",
      "2759 : Training: loss:  0.07572912\n",
      "2760 : Training: loss:  0.12650707\n",
      "Validation: Loss:  0.099628754  Accuracy:  0.61538464\n",
      "2761 : Training: loss:  0.10064426\n",
      "2762 : Training: loss:  0.084059246\n",
      "2763 : Training: loss:  0.086668976\n",
      "2764 : Training: loss:  0.10854287\n",
      "2765 : Training: loss:  0.11348606\n",
      "2766 : Training: loss:  0.08632006\n",
      "2767 : Training: loss:  0.077389374\n",
      "2768 : Training: loss:  0.10794578\n",
      "2769 : Training: loss:  0.08071505\n",
      "2770 : Training: loss:  0.13160999\n",
      "2771 : Training: loss:  0.10287819\n",
      "2772 : Training: loss:  0.030142652\n",
      "2773 : Training: loss:  0.09484866\n",
      "2774 : Training: loss:  0.1011463\n",
      "2775 : Training: loss:  0.10750145\n",
      "2776 : Training: loss:  0.11512741\n",
      "2777 : Training: loss:  0.087300986\n",
      "2778 : Training: loss:  0.057472356\n",
      "2779 : Training: loss:  0.09654809\n",
      "2780 : Training: loss:  0.09587373\n",
      "Validation: Loss:  0.09924447  Accuracy:  0.61538464\n",
      "2781 : Training: loss:  0.053577993\n",
      "2782 : Training: loss:  0.08941316\n",
      "2783 : Training: loss:  0.03308276\n",
      "2784 : Training: loss:  0.07575404\n",
      "2785 : Training: loss:  0.10065845\n",
      "2786 : Training: loss:  0.10464831\n",
      "2787 : Training: loss:  0.092402354\n",
      "2788 : Training: loss:  0.09174878\n",
      "2789 : Training: loss:  0.11915631\n",
      "2790 : Training: loss:  0.087348655\n",
      "2791 : Training: loss:  0.12259201\n",
      "2792 : Training: loss:  0.057836954\n",
      "2793 : Training: loss:  0.08520546\n",
      "2794 : Training: loss:  0.08152217\n",
      "2795 : Training: loss:  0.09789126\n",
      "2796 : Training: loss:  0.06500171\n",
      "2797 : Training: loss:  0.111277014\n",
      "2798 : Training: loss:  0.10483282\n",
      "2799 : Training: loss:  0.07949582\n",
      "2800 : Training: loss:  0.10244249\n",
      "Validation: Loss:  0.0988031  Accuracy:  0.61538464\n",
      "2801 : Training: loss:  0.08641798\n",
      "2802 : Training: loss:  0.06922016\n",
      "2803 : Training: loss:  0.08391785\n",
      "2804 : Training: loss:  0.0495056\n",
      "2805 : Training: loss:  0.08641677\n",
      "2806 : Training: loss:  0.07458088\n",
      "2807 : Training: loss:  0.119361825\n",
      "2808 : Training: loss:  0.09887705\n",
      "2809 : Training: loss:  0.1173845\n",
      "2810 : Training: loss:  0.031579312\n",
      "2811 : Training: loss:  0.084727585\n",
      "2812 : Training: loss:  0.051634997\n",
      "2813 : Training: loss:  0.10621037\n",
      "2814 : Training: loss:  0.11506644\n",
      "2815 : Training: loss:  0.1167541\n",
      "2816 : Training: loss:  0.073445335\n",
      "2817 : Training: loss:  0.11850106\n",
      "2818 : Training: loss:  0.07523775\n",
      "2819 : Training: loss:  0.09406756\n",
      "2820 : Training: loss:  0.08482022\n",
      "Validation: Loss:  0.09837607  Accuracy:  0.61538464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821 : Training: loss:  0.08916332\n",
      "2822 : Training: loss:  0.078811914\n",
      "2823 : Training: loss:  0.09947688\n",
      "2824 : Training: loss:  0.102434576\n",
      "2825 : Training: loss:  0.11157635\n",
      "2826 : Training: loss:  0.12490093\n",
      "2827 : Training: loss:  0.099504456\n",
      "2828 : Training: loss:  0.104232274\n",
      "2829 : Training: loss:  0.124143735\n",
      "2830 : Training: loss:  0.08105311\n",
      "2831 : Training: loss:  0.047659572\n",
      "2832 : Training: loss:  0.064250685\n",
      "2833 : Training: loss:  0.094578795\n",
      "2834 : Training: loss:  0.075064436\n",
      "2835 : Training: loss:  0.05594831\n",
      "2836 : Training: loss:  0.09980109\n",
      "2837 : Training: loss:  0.06891423\n",
      "2838 : Training: loss:  0.099190705\n",
      "2839 : Training: loss:  0.109330334\n",
      "2840 : Training: loss:  0.081488274\n",
      "Validation: Loss:  0.09815086  Accuracy:  0.59615386\n",
      "2841 : Training: loss:  0.094434865\n",
      "2842 : Training: loss:  0.08790016\n",
      "2843 : Training: loss:  0.07031527\n",
      "2844 : Training: loss:  0.07832384\n",
      "2845 : Training: loss:  0.09003704\n",
      "2846 : Training: loss:  0.09739193\n",
      "2847 : Training: loss:  0.09978122\n",
      "2848 : Training: loss:  0.07145666\n",
      "2849 : Training: loss:  0.12535417\n",
      "2850 : Training: loss:  0.06281949\n",
      "2851 : Training: loss:  0.103523284\n",
      "2852 : Training: loss:  0.10519609\n",
      "2853 : Training: loss:  0.119795404\n",
      "2854 : Training: loss:  0.1253202\n",
      "2855 : Training: loss:  0.09480814\n",
      "2856 : Training: loss:  0.08653472\n",
      "2857 : Training: loss:  0.10175938\n",
      "2858 : Training: loss:  0.0837159\n",
      "2859 : Training: loss:  0.063456126\n",
      "2860 : Training: loss:  0.07978973\n",
      "Validation: Loss:  0.09771487  Accuracy:  0.61538464\n",
      "2861 : Training: loss:  0.07430264\n",
      "2862 : Training: loss:  0.084903635\n",
      "2863 : Training: loss:  0.09445901\n",
      "2864 : Training: loss:  0.10890222\n",
      "2865 : Training: loss:  0.10166396\n",
      "2866 : Training: loss:  0.09254779\n",
      "2867 : Training: loss:  0.109232016\n",
      "2868 : Training: loss:  0.09647137\n",
      "2869 : Training: loss:  0.10301591\n",
      "2870 : Training: loss:  0.096036024\n",
      "2871 : Training: loss:  0.11923354\n",
      "2872 : Training: loss:  0.07053443\n",
      "2873 : Training: loss:  0.084678784\n",
      "2874 : Training: loss:  0.1068051\n",
      "2875 : Training: loss:  0.06237722\n",
      "2876 : Training: loss:  0.10400211\n",
      "2877 : Training: loss:  0.06557249\n",
      "2878 : Training: loss:  0.08800028\n",
      "2879 : Training: loss:  0.08730869\n",
      "2880 : Training: loss:  0.08559874\n",
      "Validation: Loss:  0.09749949  Accuracy:  0.63461536\n",
      "2881 : Training: loss:  0.11165475\n",
      "2882 : Training: loss:  0.10515288\n",
      "2883 : Training: loss:  0.040431306\n",
      "2884 : Training: loss:  0.031161949\n",
      "2885 : Training: loss:  0.10354336\n",
      "2886 : Training: loss:  0.09842776\n",
      "2887 : Training: loss:  0.08368543\n",
      "2888 : Training: loss:  0.08296532\n",
      "2889 : Training: loss:  0.0680596\n",
      "2890 : Training: loss:  0.09751901\n",
      "2891 : Training: loss:  0.12575649\n",
      "2892 : Training: loss:  0.10862523\n",
      "2893 : Training: loss:  0.0824299\n",
      "2894 : Training: loss:  0.08135725\n",
      "2895 : Training: loss:  0.08372359\n",
      "2896 : Training: loss:  0.07729564\n",
      "2897 : Training: loss:  0.074572675\n",
      "2898 : Training: loss:  0.07443174\n",
      "2899 : Training: loss:  0.08137996\n",
      "2900 : Training: loss:  0.062370077\n",
      "Validation: Loss:  0.09702071  Accuracy:  0.63461536\n",
      "2901 : Training: loss:  0.081669345\n",
      "2902 : Training: loss:  0.07515761\n",
      "2903 : Training: loss:  0.104009\n",
      "2904 : Training: loss:  0.088174544\n",
      "2905 : Training: loss:  0.08421696\n",
      "2906 : Training: loss:  0.057618644\n",
      "2907 : Training: loss:  0.06956409\n",
      "2908 : Training: loss:  0.07697042\n",
      "2909 : Training: loss:  0.108998746\n",
      "2910 : Training: loss:  0.07113813\n",
      "2911 : Training: loss:  0.06658101\n",
      "2912 : Training: loss:  0.10335318\n",
      "2913 : Training: loss:  0.10867973\n",
      "2914 : Training: loss:  0.07949833\n",
      "2915 : Training: loss:  0.079296164\n",
      "2916 : Training: loss:  0.11749565\n",
      "2917 : Training: loss:  0.08518047\n",
      "2918 : Training: loss:  0.082108684\n",
      "2919 : Training: loss:  0.07344995\n",
      "2920 : Training: loss:  0.07394363\n",
      "Validation: Loss:  0.096429035  Accuracy:  0.65384614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1731, 0.0313, 0.0706, 0.0376, 0.0272, 0.027...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.096429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1476, 0.0455, 0.0223, 0.0139, 0.0552, 0.058...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1124, 0.0638, 0.0382, 0.0285, 0.0229, 0.048...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0605, 0.0256, 0.0134, 0.0056, 0.0274, 0.027...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1147, 0.033, 0.0383, 0.0272, 0.0253, 0.0292...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0575, 0.016, 0.0835, 0.1686, 0.0138, 0.0161...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0364, 0.0131, 0.0462, 0.5456, 0.0224, 0.010...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0333, 0.0173, 0.044, 0.5659, 0.0311, 0.0127...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1019, 0.0175, 0.0841, 0.1466, 0.0355, 0.031...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0993, 0.0542, 0.0042, 0.0036, 0.2479, 0.072...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0517, 0.1243, 0.0028, 0.0051, 0.2212, 0.085...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0485, 0.0895, 0.0064, 0.0053, 0.0231, 0.092...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0133, 0.0386, 0.0098, 0.006, 0.0187, 0.0165...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0025, 0.0169, 0.0097, 0.0102, 0.0087, 0.004...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0419, 0.0309, 0.0059, 0.006, 0.0689, 0.0222...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1384, 0.0811, 0.0101, 0.0017, 0.0261, 0.058...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0628, 0.0138, 0.067, 0.0874, 0.0199, 0.0179...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.033, 0.0111, 0.0266, 0.0295, 0.0178, 0.0142...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0772, 0.0509, 0.0177, 0.0114, 0.0555, 0.068...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.1287, 0.0842, 0.0179, 0.0068, 0.0807, 0.110...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.1013, 0.0244, 0.0433, 0.0123, 0.0245, 0.032...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0175, 0.0171, 0.0108, 0.0331, 0.0102, 0.011...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0708, 0.0188, 0.0208, 0.0071, 0.0299, 0.030...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.079, 0.0938, 0.0153, 0.0031, 0.0579, 0.1258...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1135, 0.0201, 0.0681, 0.0556, 0.0235, 0.027...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.1002, 0.0203, 0.045, 0.0353, 0.0275, 0.0154...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0877, 0.0228, 0.0279, 0.0196, 0.0495, 0.018...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0393, 0.0141, 0.0154, 0.0205, 0.0316, 0.016...</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0052, 0.0105, 0.0044, 0.0091, 0.0154, 0.007...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0082, 0.0114, 0.0027, 0.0043, 0.0181, 0.008...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0134, 0.0436, 0.0099, 0.0007, 0.0447, 0.025...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0581, 0.0161, 0.0487, 0.0459, 0.0198, 0.020...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0374, 0.0116, 0.0271, 0.0535, 0.012, 0.011,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0832, 0.0273, 0.0389, 0.048, 0.0306, 0.0209...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0759, 0.0395, 0.0212, 0.0086, 0.0265, 0.039...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0818, 0.0157, 0.0255, 0.0333, 0.0136, 0.012...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0343, 0.0235, 0.0377, 0.123, 0.0101, 0.0196...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0019, 0.012, 0.0036, 0.0293, 0.0012, 0.0063...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0377, 0.0326, 0.0183, 0.0664, 0.1239, 0.019...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0427, 0.0229, 0.0231, 0.0713, 0.0619, 0.017...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0332, 0.0203, 0.0152, 0.0113, 0.0202, 0.008...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0598, 0.0194, 0.0466, 0.1246, 0.0243, 0.015...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0121, 0.0145, 0.0145, 0.0928, 0.0033, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0021, 0.0078, 0.0031, 0.0597, 0.0007, 0.002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0022, 0.0049, 0.0024, 0.0368, 0.0006, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0052, 0.0086, 0.0088, 0.1147, 0.0017, 0.003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0474, 0.0153, 0.0504, 0.0097, 0.0175, 0.014...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1284, 0.0195, 0.0672, 0.0402, 0.0176, 0.024...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0574, 0.0674, 0.0062, 0.0023, 0.1013, 0.042...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.087, 0.184, 0.0056, 0.0026, 0.0411, 0.1254,...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0898, 0.0346, 0.0132, 0.0043, 0.0199, 0.038...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0979, 0.0489, 0.0204, 0.0116, 0.0236, 0.066...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1731, 0.0313, 0.0706, 0.0376, 0.0272, 0.027...               0   \n",
       "1   [0.1476, 0.0455, 0.0223, 0.0139, 0.0552, 0.058...               0   \n",
       "2   [0.1124, 0.0638, 0.0382, 0.0285, 0.0229, 0.048...               0   \n",
       "3   [0.0605, 0.0256, 0.0134, 0.0056, 0.0274, 0.027...               1   \n",
       "4   [0.1147, 0.033, 0.0383, 0.0272, 0.0253, 0.0292...               1   \n",
       "5   [0.0575, 0.016, 0.0835, 0.1686, 0.0138, 0.0161...               2   \n",
       "6   [0.0364, 0.0131, 0.0462, 0.5456, 0.0224, 0.010...               3   \n",
       "7   [0.0333, 0.0173, 0.044, 0.5659, 0.0311, 0.0127...               3   \n",
       "8   [0.1019, 0.0175, 0.0841, 0.1466, 0.0355, 0.031...               3   \n",
       "9   [0.0993, 0.0542, 0.0042, 0.0036, 0.2479, 0.072...               4   \n",
       "10  [0.0517, 0.1243, 0.0028, 0.0051, 0.2212, 0.085...               4   \n",
       "11  [0.0485, 0.0895, 0.0064, 0.0053, 0.0231, 0.092...               5   \n",
       "12  [0.0133, 0.0386, 0.0098, 0.006, 0.0187, 0.0165...               6   \n",
       "13  [0.0025, 0.0169, 0.0097, 0.0102, 0.0087, 0.004...               7   \n",
       "14  [0.0419, 0.0309, 0.0059, 0.006, 0.0689, 0.0222...               8   \n",
       "15  [0.1384, 0.0811, 0.0101, 0.0017, 0.0261, 0.058...               8   \n",
       "16  [0.0628, 0.0138, 0.067, 0.0874, 0.0199, 0.0179...               9   \n",
       "17  [0.033, 0.0111, 0.0266, 0.0295, 0.0178, 0.0142...               9   \n",
       "18  [0.0772, 0.0509, 0.0177, 0.0114, 0.0555, 0.068...              10   \n",
       "19  [0.1287, 0.0842, 0.0179, 0.0068, 0.0807, 0.110...              10   \n",
       "20  [0.1013, 0.0244, 0.0433, 0.0123, 0.0245, 0.032...              11   \n",
       "21  [0.0175, 0.0171, 0.0108, 0.0331, 0.0102, 0.011...              11   \n",
       "22  [0.0708, 0.0188, 0.0208, 0.0071, 0.0299, 0.030...              12   \n",
       "23  [0.079, 0.0938, 0.0153, 0.0031, 0.0579, 0.1258...              13   \n",
       "24  [0.1135, 0.0201, 0.0681, 0.0556, 0.0235, 0.027...              13   \n",
       "25  [0.1002, 0.0203, 0.045, 0.0353, 0.0275, 0.0154...              14   \n",
       "26  [0.0877, 0.0228, 0.0279, 0.0196, 0.0495, 0.018...              14   \n",
       "27  [0.0393, 0.0141, 0.0154, 0.0205, 0.0316, 0.016...              15   \n",
       "28  [0.0052, 0.0105, 0.0044, 0.0091, 0.0154, 0.007...              15   \n",
       "29  [0.0082, 0.0114, 0.0027, 0.0043, 0.0181, 0.008...              15   \n",
       "30  [0.0134, 0.0436, 0.0099, 0.0007, 0.0447, 0.025...              16   \n",
       "31  [0.0581, 0.0161, 0.0487, 0.0459, 0.0198, 0.020...              17   \n",
       "32  [0.0374, 0.0116, 0.0271, 0.0535, 0.012, 0.011,...              17   \n",
       "33  [0.0832, 0.0273, 0.0389, 0.048, 0.0306, 0.0209...              18   \n",
       "34  [0.0759, 0.0395, 0.0212, 0.0086, 0.0265, 0.039...              19   \n",
       "35  [0.0818, 0.0157, 0.0255, 0.0333, 0.0136, 0.012...              20   \n",
       "36  [0.0343, 0.0235, 0.0377, 0.123, 0.0101, 0.0196...              21   \n",
       "37  [0.0019, 0.012, 0.0036, 0.0293, 0.0012, 0.0063...              21   \n",
       "38  [0.0377, 0.0326, 0.0183, 0.0664, 0.1239, 0.019...              22   \n",
       "39  [0.0427, 0.0229, 0.0231, 0.0713, 0.0619, 0.017...              22   \n",
       "40  [0.0332, 0.0203, 0.0152, 0.0113, 0.0202, 0.008...              22   \n",
       "41  [0.0598, 0.0194, 0.0466, 0.1246, 0.0243, 0.015...              22   \n",
       "42  [0.0121, 0.0145, 0.0145, 0.0928, 0.0033, 0.008...              23   \n",
       "43  [0.0021, 0.0078, 0.0031, 0.0597, 0.0007, 0.002...              23   \n",
       "44  [0.0022, 0.0049, 0.0024, 0.0368, 0.0006, 0.001...              23   \n",
       "45  [0.0052, 0.0086, 0.0088, 0.1147, 0.0017, 0.003...              23   \n",
       "46  [0.0474, 0.0153, 0.0504, 0.0097, 0.0175, 0.014...              24   \n",
       "47  [0.1284, 0.0195, 0.0672, 0.0402, 0.0176, 0.024...              24   \n",
       "48  [0.0574, 0.0674, 0.0062, 0.0023, 0.1013, 0.042...              25   \n",
       "49  [0.087, 0.184, 0.0056, 0.0026, 0.0411, 0.1254,...              25   \n",
       "50  [0.0898, 0.0346, 0.0132, 0.0043, 0.0199, 0.038...              26   \n",
       "51  [0.0979, 0.0489, 0.0204, 0.0116, 0.0236, 0.066...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.653846  0.096429  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 4       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                25       NaN       NaN  \n",
       "24                 0       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                 9       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                23       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                22       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                25       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                 0       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2921 : Training: loss:  0.09416269\n",
      "2922 : Training: loss:  0.12030272\n",
      "2923 : Training: loss:  0.08273945\n",
      "2924 : Training: loss:  0.110285155\n",
      "2925 : Training: loss:  0.077732734\n",
      "2926 : Training: loss:  0.07510439\n",
      "2927 : Training: loss:  0.09368521\n",
      "2928 : Training: loss:  0.062332917\n",
      "2929 : Training: loss:  0.067427464\n",
      "2930 : Training: loss:  0.086651355\n",
      "2931 : Training: loss:  0.105010204\n",
      "2932 : Training: loss:  0.101592645\n",
      "2933 : Training: loss:  0.08323197\n",
      "2934 : Training: loss:  0.11217829\n",
      "2935 : Training: loss:  0.08509448\n",
      "2936 : Training: loss:  0.10225295\n",
      "2937 : Training: loss:  0.0863298\n",
      "2938 : Training: loss:  0.113202296\n",
      "2939 : Training: loss:  0.05592108\n",
      "2940 : Training: loss:  0.119184546\n",
      "Validation: Loss:  0.09601446  Accuracy:  0.61538464\n",
      "2941 : Training: loss:  0.08152507\n",
      "2942 : Training: loss:  0.039515942\n",
      "2943 : Training: loss:  0.11348535\n",
      "2944 : Training: loss:  0.071329944\n",
      "2945 : Training: loss:  0.078557454\n",
      "2946 : Training: loss:  0.11344412\n",
      "2947 : Training: loss:  0.062725134\n",
      "2948 : Training: loss:  0.102259636\n",
      "2949 : Training: loss:  0.11264053\n",
      "2950 : Training: loss:  0.08723351\n",
      "2951 : Training: loss:  0.120967254\n",
      "2952 : Training: loss:  0.053565297\n",
      "2953 : Training: loss:  0.095024645\n",
      "2954 : Training: loss:  0.10808463\n",
      "2955 : Training: loss:  0.13150407\n",
      "2956 : Training: loss:  0.06478747\n",
      "2957 : Training: loss:  0.065269075\n",
      "2958 : Training: loss:  0.067414105\n",
      "2959 : Training: loss:  0.089637935\n",
      "2960 : Training: loss:  0.08270829\n",
      "Validation: Loss:  0.0958048  Accuracy:  0.5769231\n",
      "2961 : Training: loss:  0.12179483\n",
      "2962 : Training: loss:  0.08395934\n",
      "2963 : Training: loss:  0.075563654\n",
      "2964 : Training: loss:  0.10213814\n",
      "2965 : Training: loss:  0.098895214\n",
      "2966 : Training: loss:  0.1021737\n",
      "2967 : Training: loss:  0.057421092\n",
      "2968 : Training: loss:  0.077620395\n",
      "2969 : Training: loss:  0.06358036\n",
      "2970 : Training: loss:  0.039103623\n",
      "2971 : Training: loss:  0.07084963\n",
      "2972 : Training: loss:  0.09316503\n",
      "2973 : Training: loss:  0.10331949\n",
      "2974 : Training: loss:  0.10110207\n",
      "2975 : Training: loss:  0.0936315\n",
      "2976 : Training: loss:  0.08381246\n",
      "2977 : Training: loss:  0.068635054\n",
      "2978 : Training: loss:  0.09433827\n",
      "2979 : Training: loss:  0.06528433\n",
      "2980 : Training: loss:  0.0882152\n",
      "Validation: Loss:  0.0954946  Accuracy:  0.5769231\n",
      "2981 : Training: loss:  0.120041505\n",
      "2982 : Training: loss:  0.06889063\n",
      "2983 : Training: loss:  0.09802939\n",
      "2984 : Training: loss:  0.09577937\n",
      "2985 : Training: loss:  0.10744237\n",
      "2986 : Training: loss:  0.10313422\n",
      "2987 : Training: loss:  0.11910072\n",
      "2988 : Training: loss:  0.061237667\n",
      "2989 : Training: loss:  0.07430415\n",
      "2990 : Training: loss:  0.11097845\n",
      "2991 : Training: loss:  0.13007265\n",
      "2992 : Training: loss:  0.08640951\n",
      "2993 : Training: loss:  0.093960576\n",
      "2994 : Training: loss:  0.08159297\n",
      "2995 : Training: loss:  0.08760572\n",
      "2996 : Training: loss:  0.09578723\n",
      "2997 : Training: loss:  0.100162975\n",
      "2998 : Training: loss:  0.0628829\n",
      "2999 : Training: loss:  0.10790173\n",
      "3000 : Training: loss:  0.08184541\n",
      "Validation: Loss:  0.09515071  Accuracy:  0.59615386\n",
      "3001 : Training: loss:  0.102797896\n",
      "3002 : Training: loss:  0.06317002\n",
      "3003 : Training: loss:  0.041421864\n",
      "3004 : Training: loss:  0.059327364\n",
      "3005 : Training: loss:  0.08156969\n",
      "3006 : Training: loss:  0.09284084\n",
      "3007 : Training: loss:  0.08325158\n",
      "3008 : Training: loss:  0.05845809\n",
      "3009 : Training: loss:  0.06569735\n",
      "3010 : Training: loss:  0.11211561\n",
      "3011 : Training: loss:  0.0933067\n",
      "3012 : Training: loss:  0.070976585\n",
      "3013 : Training: loss:  0.07872181\n",
      "3014 : Training: loss:  0.11963688\n",
      "3015 : Training: loss:  0.07752372\n",
      "3016 : Training: loss:  0.077653654\n",
      "3017 : Training: loss:  0.08722868\n",
      "3018 : Training: loss:  0.068827376\n",
      "3019 : Training: loss:  0.09763531\n",
      "3020 : Training: loss:  0.08363711\n",
      "Validation: Loss:  0.094699316  Accuracy:  0.61538464\n",
      "3021 : Training: loss:  0.10243028\n",
      "3022 : Training: loss:  0.097596\n",
      "3023 : Training: loss:  0.07352536\n",
      "3024 : Training: loss:  0.07833424\n",
      "3025 : Training: loss:  0.09227354\n",
      "3026 : Training: loss:  0.104338184\n",
      "3027 : Training: loss:  0.075126536\n",
      "3028 : Training: loss:  0.10457397\n",
      "3029 : Training: loss:  0.11752647\n",
      "3030 : Training: loss:  0.067496866\n",
      "3031 : Training: loss:  0.09910236\n",
      "3032 : Training: loss:  0.090240374\n",
      "3033 : Training: loss:  0.060302515\n",
      "3034 : Training: loss:  0.09310609\n",
      "3035 : Training: loss:  0.11203894\n",
      "3036 : Training: loss:  0.07804424\n",
      "3037 : Training: loss:  0.09220325\n",
      "3038 : Training: loss:  0.07984188\n",
      "3039 : Training: loss:  0.08479872\n",
      "3040 : Training: loss:  0.10144595\n",
      "Validation: Loss:  0.09410707  Accuracy:  0.61538464\n",
      "3041 : Training: loss:  0.08444095\n",
      "3042 : Training: loss:  0.06571134\n",
      "3043 : Training: loss:  0.1078888\n",
      "3044 : Training: loss:  0.08322243\n",
      "3045 : Training: loss:  0.09626083\n",
      "3046 : Training: loss:  0.09553329\n",
      "3047 : Training: loss:  0.091498576\n",
      "3048 : Training: loss:  0.048568673\n",
      "3049 : Training: loss:  0.08392706\n",
      "3050 : Training: loss:  0.08746673\n",
      "3051 : Training: loss:  0.09446892\n",
      "3052 : Training: loss:  0.05853557\n",
      "3053 : Training: loss:  0.1020621\n",
      "3054 : Training: loss:  0.09622183\n",
      "3055 : Training: loss:  0.09222168\n",
      "3056 : Training: loss:  0.06828533\n",
      "3057 : Training: loss:  0.06852969\n",
      "3058 : Training: loss:  0.08769036\n",
      "3059 : Training: loss:  0.10518015\n",
      "3060 : Training: loss:  0.07416483\n",
      "Validation: Loss:  0.09385286  Accuracy:  0.59615386\n",
      "3061 : Training: loss:  0.0675105\n",
      "3062 : Training: loss:  0.08384196\n",
      "3063 : Training: loss:  0.07431244\n",
      "3064 : Training: loss:  0.051392514\n",
      "3065 : Training: loss:  0.111479856\n",
      "3066 : Training: loss:  0.08798226\n",
      "3067 : Training: loss:  0.08274191\n",
      "3068 : Training: loss:  0.10586246\n",
      "3069 : Training: loss:  0.09922251\n",
      "3070 : Training: loss:  0.032319233\n",
      "3071 : Training: loss:  0.07572171\n",
      "3072 : Training: loss:  0.10734452\n",
      "3073 : Training: loss:  0.06988945\n",
      "3074 : Training: loss:  0.042415738\n",
      "3075 : Training: loss:  0.03649419\n",
      "3076 : Training: loss:  0.08958048\n",
      "3077 : Training: loss:  0.1187909\n",
      "3078 : Training: loss:  0.064892285\n",
      "3079 : Training: loss:  0.095373325\n",
      "3080 : Training: loss:  0.058634955\n",
      "Validation: Loss:  0.09359843  Accuracy:  0.61538464\n",
      "3081 : Training: loss:  0.08960679\n",
      "3082 : Training: loss:  0.05164667\n",
      "3083 : Training: loss:  0.037744705\n",
      "3084 : Training: loss:  0.092723794\n",
      "3085 : Training: loss:  0.071799904\n",
      "3086 : Training: loss:  0.09021777\n",
      "3087 : Training: loss:  0.084695324\n",
      "3088 : Training: loss:  0.08652677\n",
      "3089 : Training: loss:  0.07771578\n",
      "3090 : Training: loss:  0.053456157\n",
      "3091 : Training: loss:  0.08343797\n",
      "3092 : Training: loss:  0.061577335\n",
      "3093 : Training: loss:  0.03446907\n",
      "3094 : Training: loss:  0.10063928\n",
      "3095 : Training: loss:  0.115687035\n",
      "3096 : Training: loss:  0.103257716\n",
      "3097 : Training: loss:  0.057909224\n",
      "3098 : Training: loss:  0.06719283\n",
      "3099 : Training: loss:  0.08582483\n",
      "3100 : Training: loss:  0.110085815\n",
      "Validation: Loss:  0.09332172  Accuracy:  0.63461536\n",
      "3101 : Training: loss:  0.021611074\n",
      "3102 : Training: loss:  0.0838851\n",
      "3103 : Training: loss:  0.09645864\n",
      "3104 : Training: loss:  0.09259628\n",
      "3105 : Training: loss:  0.08936633\n",
      "3106 : Training: loss:  0.0724537\n",
      "3107 : Training: loss:  0.08465648\n",
      "3108 : Training: loss:  0.08243313\n",
      "3109 : Training: loss:  0.098879136\n",
      "3110 : Training: loss:  0.08683924\n",
      "3111 : Training: loss:  0.124713965\n",
      "3112 : Training: loss:  0.0893132\n",
      "3113 : Training: loss:  0.09323856\n",
      "3114 : Training: loss:  0.08361026\n",
      "3115 : Training: loss:  0.08306285\n",
      "3116 : Training: loss:  0.09646125\n",
      "3117 : Training: loss:  0.07764808\n",
      "3118 : Training: loss:  0.095205106\n",
      "3119 : Training: loss:  0.08839812\n",
      "3120 : Training: loss:  0.06780391\n",
      "Validation: Loss:  0.09314399  Accuracy:  0.63461536\n",
      "3121 : Training: loss:  0.11058416\n",
      "3122 : Training: loss:  0.075721994\n",
      "3123 : Training: loss:  0.09698754\n",
      "3124 : Training: loss:  0.054645304\n",
      "3125 : Training: loss:  0.040045757\n",
      "3126 : Training: loss:  0.11059043\n",
      "3127 : Training: loss:  0.06690512\n",
      "3128 : Training: loss:  0.10144405\n",
      "3129 : Training: loss:  0.07493259\n",
      "3130 : Training: loss:  0.08780009\n",
      "3131 : Training: loss:  0.07838524\n",
      "3132 : Training: loss:  0.038616017\n",
      "3133 : Training: loss:  0.07899043\n",
      "3134 : Training: loss:  0.063238226\n",
      "3135 : Training: loss:  0.08239193\n",
      "3136 : Training: loss:  0.1071497\n",
      "3137 : Training: loss:  0.08033928\n",
      "3138 : Training: loss:  0.07718693\n",
      "3139 : Training: loss:  0.07547161\n",
      "3140 : Training: loss:  0.06655161\n",
      "Validation: Loss:  0.09278287  Accuracy:  0.65384614\n",
      "3141 : Training: loss:  0.09737858\n",
      "3142 : Training: loss:  0.059528504\n",
      "3143 : Training: loss:  0.082885094\n",
      "3144 : Training: loss:  0.10100573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3145 : Training: loss:  0.109280854\n",
      "3146 : Training: loss:  0.08554434\n",
      "3147 : Training: loss:  0.100202166\n",
      "3148 : Training: loss:  0.07808982\n",
      "3149 : Training: loss:  0.044945\n",
      "3150 : Training: loss:  0.091475576\n",
      "3151 : Training: loss:  0.09525589\n",
      "3152 : Training: loss:  0.07403601\n",
      "3153 : Training: loss:  0.056149457\n",
      "3154 : Training: loss:  0.07435292\n",
      "3155 : Training: loss:  0.104421645\n",
      "3156 : Training: loss:  0.11074226\n",
      "3157 : Training: loss:  0.089224406\n",
      "3158 : Training: loss:  0.083484545\n",
      "3159 : Training: loss:  0.111851595\n",
      "3160 : Training: loss:  0.07325305\n",
      "Validation: Loss:  0.09241225  Accuracy:  0.61538464\n",
      "3161 : Training: loss:  0.10125776\n",
      "3162 : Training: loss:  0.093323596\n",
      "3163 : Training: loss:  0.06851756\n",
      "3164 : Training: loss:  0.044225734\n",
      "3165 : Training: loss:  0.0631476\n",
      "3166 : Training: loss:  0.0463739\n",
      "3167 : Training: loss:  0.04154591\n",
      "3168 : Training: loss:  0.07016918\n",
      "3169 : Training: loss:  0.12721205\n",
      "3170 : Training: loss:  0.08406447\n",
      "3171 : Training: loss:  0.10442157\n",
      "3172 : Training: loss:  0.0622659\n",
      "3173 : Training: loss:  0.110470116\n",
      "3174 : Training: loss:  0.08507361\n",
      "3175 : Training: loss:  0.08958649\n",
      "3176 : Training: loss:  0.1097211\n",
      "3177 : Training: loss:  0.077536836\n",
      "3178 : Training: loss:  0.045433287\n",
      "3179 : Training: loss:  0.13126084\n",
      "3180 : Training: loss:  0.06334104\n",
      "Validation: Loss:  0.09203199  Accuracy:  0.63461536\n",
      "3181 : Training: loss:  0.02507393\n",
      "3182 : Training: loss:  0.06978976\n",
      "3183 : Training: loss:  0.08284313\n",
      "3184 : Training: loss:  0.06693203\n",
      "3185 : Training: loss:  0.036582395\n",
      "3186 : Training: loss:  0.09481727\n",
      "3187 : Training: loss:  0.102133356\n",
      "3188 : Training: loss:  0.05908781\n",
      "3189 : Training: loss:  0.060025577\n",
      "3190 : Training: loss:  0.08782851\n",
      "3191 : Training: loss:  0.100531146\n",
      "3192 : Training: loss:  0.09278687\n",
      "3193 : Training: loss:  0.07473397\n",
      "3194 : Training: loss:  0.10688313\n",
      "3195 : Training: loss:  0.050155092\n",
      "3196 : Training: loss:  0.09564393\n",
      "3197 : Training: loss:  0.110986464\n",
      "3198 : Training: loss:  0.07952781\n",
      "3199 : Training: loss:  0.09929577\n",
      "3200 : Training: loss:  0.09393918\n",
      "Validation: Loss:  0.091595404  Accuracy:  0.65384614\n",
      "3201 : Training: loss:  0.102624066\n",
      "3202 : Training: loss:  0.05468381\n",
      "3203 : Training: loss:  0.07940541\n",
      "3204 : Training: loss:  0.091357745\n",
      "3205 : Training: loss:  0.06136581\n",
      "3206 : Training: loss:  0.05811933\n",
      "3207 : Training: loss:  0.07252249\n",
      "3208 : Training: loss:  0.051850364\n",
      "3209 : Training: loss:  0.054056697\n",
      "3210 : Training: loss:  0.08318157\n",
      "3211 : Training: loss:  0.118372075\n",
      "3212 : Training: loss:  0.096286505\n",
      "3213 : Training: loss:  0.11043564\n",
      "3214 : Training: loss:  0.0930065\n",
      "3215 : Training: loss:  0.098084174\n",
      "3216 : Training: loss:  0.09248677\n",
      "3217 : Training: loss:  0.07147628\n",
      "3218 : Training: loss:  0.07202918\n",
      "3219 : Training: loss:  0.059441637\n",
      "3220 : Training: loss:  0.12353795\n",
      "Validation: Loss:  0.09124319  Accuracy:  0.65384614\n",
      "3221 : Training: loss:  0.09550082\n",
      "3222 : Training: loss:  0.054614633\n",
      "3223 : Training: loss:  0.055482466\n",
      "3224 : Training: loss:  0.08557897\n",
      "3225 : Training: loss:  0.069931686\n",
      "3226 : Training: loss:  0.052888323\n",
      "3227 : Training: loss:  0.082448654\n",
      "3228 : Training: loss:  0.0492202\n",
      "3229 : Training: loss:  0.08670801\n",
      "3230 : Training: loss:  0.058838632\n",
      "3231 : Training: loss:  0.06628817\n",
      "3232 : Training: loss:  0.030257594\n",
      "3233 : Training: loss:  0.041516762\n",
      "3234 : Training: loss:  0.064945854\n",
      "3235 : Training: loss:  0.06462079\n",
      "3236 : Training: loss:  0.08835124\n",
      "3237 : Training: loss:  0.099444725\n",
      "3238 : Training: loss:  0.08329092\n",
      "3239 : Training: loss:  0.11402294\n",
      "3240 : Training: loss:  0.083608516\n",
      "Validation: Loss:  0.09115329  Accuracy:  0.65384614\n",
      "3241 : Training: loss:  0.09485021\n",
      "3242 : Training: loss:  0.111958444\n",
      "3243 : Training: loss:  0.06770503\n",
      "3244 : Training: loss:  0.09486898\n",
      "3245 : Training: loss:  0.0652628\n",
      "3246 : Training: loss:  0.072426885\n",
      "3247 : Training: loss:  0.059584226\n",
      "3248 : Training: loss:  0.09045397\n",
      "3249 : Training: loss:  0.07747645\n",
      "3250 : Training: loss:  0.06610339\n",
      "3251 : Training: loss:  0.09695225\n",
      "3252 : Training: loss:  0.07444739\n",
      "3253 : Training: loss:  0.04446445\n",
      "3254 : Training: loss:  0.09829927\n",
      "3255 : Training: loss:  0.08223101\n",
      "3256 : Training: loss:  0.080209635\n",
      "3257 : Training: loss:  0.07823673\n",
      "3258 : Training: loss:  0.08975322\n",
      "3259 : Training: loss:  0.09078293\n",
      "3260 : Training: loss:  0.09075006\n",
      "Validation: Loss:  0.09077753  Accuracy:  0.6730769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1814, 0.0381, 0.0843, 0.0296, 0.0221, 0.025...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.090778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1474, 0.0672, 0.0218, 0.01, 0.0536, 0.066, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1051, 0.0923, 0.0419, 0.0222, 0.0173, 0.052...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0486, 0.0359, 0.0139, 0.0035, 0.0234, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1048, 0.0425, 0.0417, 0.0197, 0.0204, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.049, 0.0159, 0.1058, 0.1551, 0.0104, 0.0146...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.029, 0.0127, 0.0599, 0.5763, 0.018, 0.0084,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0249, 0.0167, 0.054, 0.5912, 0.0247, 0.0102...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0899, 0.0164, 0.0957, 0.1355, 0.0297, 0.028...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0803, 0.0786, 0.0031, 0.0024, 0.3086, 0.083...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.037, 0.2182, 0.0022, 0.0036, 0.2662, 0.1093...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0348, 0.1491, 0.005, 0.0036, 0.0192, 0.1223...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0067, 0.0408, 0.0061, 0.0032, 0.0148, 0.013...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0011, 0.0152, 0.0078, 0.0064, 0.0053, 0.003...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0321, 0.0439, 0.0057, 0.0041, 0.0728, 0.021...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1215, 0.1156, 0.0085, 0.001, 0.0189, 0.0533...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0497, 0.0136, 0.0742, 0.0682, 0.0161, 0.016...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0211, 0.0104, 0.0227, 0.0185, 0.0144, 0.012...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.062, 0.0738, 0.0166, 0.0081, 0.0579, 0.0902...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.111, 0.1301, 0.0164, 0.0044, 0.0809, 0.1437...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0861, 0.0272, 0.044, 0.0079, 0.0193, 0.0306...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0108, 0.0197, 0.0097, 0.0222, 0.0076, 0.010...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0537, 0.0204, 0.0194, 0.0044, 0.0249, 0.029...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0572, 0.142, 0.013, 0.002, 0.0556, 0.1674, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1136, 0.0239, 0.0913, 0.0471, 0.0201, 0.029...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0853, 0.0216, 0.0515, 0.0276, 0.0201, 0.011...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0718, 0.0254, 0.0266, 0.0127, 0.0472, 0.016...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0288, 0.0168, 0.0157, 0.0149, 0.0289, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0026, 0.0103, 0.0037, 0.0068, 0.0121, 0.005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0043, 0.0124, 0.002, 0.0029, 0.0149, 0.0066...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0053, 0.0303, 0.0068, 0.0004, 0.021, 0.0152...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0425, 0.0155, 0.0433, 0.0304, 0.0156, 0.018...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0266, 0.0113, 0.0236, 0.0377, 0.0089, 0.008...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0716, 0.0342, 0.0415, 0.0367, 0.0273, 0.019...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.06, 0.0515, 0.02, 0.0056, 0.0218, 0.0401, 0...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0754, 0.0181, 0.0299, 0.0264, 0.0098, 0.010...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0254, 0.0283, 0.0401, 0.1007, 0.0075, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0008, 0.012, 0.0028, 0.0187, 0.0007, 0.0064...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0243, 0.0329, 0.0195, 0.0502, 0.1098, 0.014...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0315, 0.0246, 0.0263, 0.0584, 0.0587, 0.015...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0233, 0.0227, 0.016, 0.0075, 0.0145, 0.0066...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0509, 0.0218, 0.0589, 0.1096, 0.0197, 0.014...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0077, 0.0165, 0.0122, 0.0621, 0.0021, 0.007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0011, 0.0086, 0.0022, 0.0314, 0.0004, 0.002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0011, 0.0048, 0.0016, 0.018, 0.0003, 0.0012...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0028, 0.0087, 0.0067, 0.068, 0.001, 0.003, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0292, 0.0116, 0.0468, 0.0057, 0.0112, 0.010...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1286, 0.0215, 0.0796, 0.0318, 0.0135, 0.023...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.037, 0.0994, 0.0049, 0.0013, 0.0959, 0.0425...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0625, 0.286, 0.0038, 0.0015, 0.0317, 0.1387...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0779, 0.0419, 0.0127, 0.0028, 0.0136, 0.034...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0878, 0.066, 0.02, 0.0084, 0.0188, 0.078, 0...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1814, 0.0381, 0.0843, 0.0296, 0.0221, 0.025...               0   \n",
       "1   [0.1474, 0.0672, 0.0218, 0.01, 0.0536, 0.066, ...               0   \n",
       "2   [0.1051, 0.0923, 0.0419, 0.0222, 0.0173, 0.052...               0   \n",
       "3   [0.0486, 0.0359, 0.0139, 0.0035, 0.0234, 0.029...               1   \n",
       "4   [0.1048, 0.0425, 0.0417, 0.0197, 0.0204, 0.029...               1   \n",
       "5   [0.049, 0.0159, 0.1058, 0.1551, 0.0104, 0.0146...               2   \n",
       "6   [0.029, 0.0127, 0.0599, 0.5763, 0.018, 0.0084,...               3   \n",
       "7   [0.0249, 0.0167, 0.054, 0.5912, 0.0247, 0.0102...               3   \n",
       "8   [0.0899, 0.0164, 0.0957, 0.1355, 0.0297, 0.028...               3   \n",
       "9   [0.0803, 0.0786, 0.0031, 0.0024, 0.3086, 0.083...               4   \n",
       "10  [0.037, 0.2182, 0.0022, 0.0036, 0.2662, 0.1093...               4   \n",
       "11  [0.0348, 0.1491, 0.005, 0.0036, 0.0192, 0.1223...               5   \n",
       "12  [0.0067, 0.0408, 0.0061, 0.0032, 0.0148, 0.013...               6   \n",
       "13  [0.0011, 0.0152, 0.0078, 0.0064, 0.0053, 0.003...               7   \n",
       "14  [0.0321, 0.0439, 0.0057, 0.0041, 0.0728, 0.021...               8   \n",
       "15  [0.1215, 0.1156, 0.0085, 0.001, 0.0189, 0.0533...               8   \n",
       "16  [0.0497, 0.0136, 0.0742, 0.0682, 0.0161, 0.016...               9   \n",
       "17  [0.0211, 0.0104, 0.0227, 0.0185, 0.0144, 0.012...               9   \n",
       "18  [0.062, 0.0738, 0.0166, 0.0081, 0.0579, 0.0902...              10   \n",
       "19  [0.111, 0.1301, 0.0164, 0.0044, 0.0809, 0.1437...              10   \n",
       "20  [0.0861, 0.0272, 0.044, 0.0079, 0.0193, 0.0306...              11   \n",
       "21  [0.0108, 0.0197, 0.0097, 0.0222, 0.0076, 0.010...              11   \n",
       "22  [0.0537, 0.0204, 0.0194, 0.0044, 0.0249, 0.029...              12   \n",
       "23  [0.0572, 0.142, 0.013, 0.002, 0.0556, 0.1674, ...              13   \n",
       "24  [0.1136, 0.0239, 0.0913, 0.0471, 0.0201, 0.029...              13   \n",
       "25  [0.0853, 0.0216, 0.0515, 0.0276, 0.0201, 0.011...              14   \n",
       "26  [0.0718, 0.0254, 0.0266, 0.0127, 0.0472, 0.016...              14   \n",
       "27  [0.0288, 0.0168, 0.0157, 0.0149, 0.0289, 0.015...              15   \n",
       "28  [0.0026, 0.0103, 0.0037, 0.0068, 0.0121, 0.005...              15   \n",
       "29  [0.0043, 0.0124, 0.002, 0.0029, 0.0149, 0.0066...              15   \n",
       "30  [0.0053, 0.0303, 0.0068, 0.0004, 0.021, 0.0152...              16   \n",
       "31  [0.0425, 0.0155, 0.0433, 0.0304, 0.0156, 0.018...              17   \n",
       "32  [0.0266, 0.0113, 0.0236, 0.0377, 0.0089, 0.008...              17   \n",
       "33  [0.0716, 0.0342, 0.0415, 0.0367, 0.0273, 0.019...              18   \n",
       "34  [0.06, 0.0515, 0.02, 0.0056, 0.0218, 0.0401, 0...              19   \n",
       "35  [0.0754, 0.0181, 0.0299, 0.0264, 0.0098, 0.010...              20   \n",
       "36  [0.0254, 0.0283, 0.0401, 0.1007, 0.0075, 0.021...              21   \n",
       "37  [0.0008, 0.012, 0.0028, 0.0187, 0.0007, 0.0064...              21   \n",
       "38  [0.0243, 0.0329, 0.0195, 0.0502, 0.1098, 0.014...              22   \n",
       "39  [0.0315, 0.0246, 0.0263, 0.0584, 0.0587, 0.015...              22   \n",
       "40  [0.0233, 0.0227, 0.016, 0.0075, 0.0145, 0.0066...              22   \n",
       "41  [0.0509, 0.0218, 0.0589, 0.1096, 0.0197, 0.014...              22   \n",
       "42  [0.0077, 0.0165, 0.0122, 0.0621, 0.0021, 0.007...              23   \n",
       "43  [0.0011, 0.0086, 0.0022, 0.0314, 0.0004, 0.002...              23   \n",
       "44  [0.0011, 0.0048, 0.0016, 0.018, 0.0003, 0.0012...              23   \n",
       "45  [0.0028, 0.0087, 0.0067, 0.068, 0.001, 0.003, ...              23   \n",
       "46  [0.0292, 0.0116, 0.0468, 0.0057, 0.0112, 0.010...              24   \n",
       "47  [0.1286, 0.0215, 0.0796, 0.0318, 0.0135, 0.023...              24   \n",
       "48  [0.037, 0.0994, 0.0049, 0.0013, 0.0959, 0.0425...              25   \n",
       "49  [0.0625, 0.286, 0.0038, 0.0015, 0.0317, 0.1387...              25   \n",
       "50  [0.0779, 0.0419, 0.0127, 0.0028, 0.0136, 0.034...              26   \n",
       "51  [0.0878, 0.066, 0.02, 0.0084, 0.0188, 0.078, 0...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.673077  0.090778  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                 10       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  9       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 1       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                25       NaN       NaN  \n",
       "24                 0       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                10       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                14       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                10       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3261 : Training: loss:  0.09313163\n",
      "3262 : Training: loss:  0.075591736\n",
      "3263 : Training: loss:  0.09322275\n",
      "3264 : Training: loss:  0.106535465\n",
      "3265 : Training: loss:  0.12081458\n",
      "3266 : Training: loss:  0.07016392\n",
      "3267 : Training: loss:  0.09092647\n",
      "3268 : Training: loss:  0.10055317\n",
      "3269 : Training: loss:  0.10532089\n",
      "3270 : Training: loss:  0.08663092\n",
      "3271 : Training: loss:  0.07457659\n",
      "3272 : Training: loss:  0.068981536\n",
      "3273 : Training: loss:  0.086510375\n",
      "3274 : Training: loss:  0.060854938\n",
      "3275 : Training: loss:  0.07722694\n",
      "3276 : Training: loss:  0.10673837\n",
      "3277 : Training: loss:  0.069228575\n",
      "3278 : Training: loss:  0.0999364\n",
      "3279 : Training: loss:  0.061677095\n",
      "3280 : Training: loss:  0.07809974\n",
      "Validation: Loss:  0.0904878  Accuracy:  0.61538464\n",
      "3281 : Training: loss:  0.038295794\n",
      "3282 : Training: loss:  0.036316935\n",
      "3283 : Training: loss:  0.084297076\n",
      "3284 : Training: loss:  0.06547182\n",
      "3285 : Training: loss:  0.07317707\n",
      "3286 : Training: loss:  0.077708185\n",
      "3287 : Training: loss:  0.068975694\n",
      "3288 : Training: loss:  0.071846075\n",
      "3289 : Training: loss:  0.08268917\n",
      "3290 : Training: loss:  0.11335262\n",
      "3291 : Training: loss:  0.09334368\n",
      "3292 : Training: loss:  0.085718624\n",
      "3293 : Training: loss:  0.09639672\n",
      "3294 : Training: loss:  0.09022299\n",
      "3295 : Training: loss:  0.13738063\n",
      "3296 : Training: loss:  0.0496771\n",
      "3297 : Training: loss:  0.061096873\n",
      "3298 : Training: loss:  0.06299223\n",
      "3299 : Training: loss:  0.06051883\n",
      "3300 : Training: loss:  0.11635317\n",
      "Validation: Loss:  0.09024799  Accuracy:  0.63461536\n",
      "3301 : Training: loss:  0.07716692\n",
      "3302 : Training: loss:  0.11704191\n",
      "3303 : Training: loss:  0.09267592\n",
      "3304 : Training: loss:  0.08125993\n",
      "3305 : Training: loss:  0.09096509\n",
      "3306 : Training: loss:  0.094514884\n",
      "3307 : Training: loss:  0.09659705\n",
      "3308 : Training: loss:  0.07363332\n",
      "3309 : Training: loss:  0.069150366\n",
      "3310 : Training: loss:  0.075568385\n",
      "3311 : Training: loss:  0.11822809\n",
      "3312 : Training: loss:  0.080928914\n",
      "3313 : Training: loss:  0.06800197\n",
      "3314 : Training: loss:  0.07532007\n",
      "3315 : Training: loss:  0.10342063\n",
      "3316 : Training: loss:  0.11622904\n",
      "3317 : Training: loss:  0.06437182\n",
      "3318 : Training: loss:  0.062546715\n",
      "3319 : Training: loss:  0.071224794\n",
      "3320 : Training: loss:  0.10196778\n",
      "Validation: Loss:  0.09002122  Accuracy:  0.63461536\n",
      "3321 : Training: loss:  0.039177686\n",
      "3322 : Training: loss:  0.086262845\n",
      "3323 : Training: loss:  0.07225808\n",
      "3324 : Training: loss:  0.062878415\n",
      "3325 : Training: loss:  0.08357066\n",
      "3326 : Training: loss:  0.0673392\n",
      "3327 : Training: loss:  0.08620559\n",
      "3328 : Training: loss:  0.077318326\n",
      "3329 : Training: loss:  0.07024472\n",
      "3330 : Training: loss:  0.119490705\n",
      "3331 : Training: loss:  0.08143418\n",
      "3332 : Training: loss:  0.08122573\n",
      "3333 : Training: loss:  0.07329412\n",
      "3334 : Training: loss:  0.07252877\n",
      "3335 : Training: loss:  0.063617334\n",
      "3336 : Training: loss:  0.06497823\n",
      "3337 : Training: loss:  0.09744411\n",
      "3338 : Training: loss:  0.09254777\n",
      "3339 : Training: loss:  0.058980566\n",
      "3340 : Training: loss:  0.07142695\n",
      "Validation: Loss:  0.08962821  Accuracy:  0.61538464\n",
      "3341 : Training: loss:  0.076092534\n",
      "3342 : Training: loss:  0.101544425\n",
      "3343 : Training: loss:  0.07732035\n",
      "3344 : Training: loss:  0.09190859\n",
      "3345 : Training: loss:  0.028065667\n",
      "3346 : Training: loss:  0.06377478\n",
      "3347 : Training: loss:  0.08648087\n",
      "3348 : Training: loss:  0.12123886\n",
      "3349 : Training: loss:  0.1039725\n",
      "3350 : Training: loss:  0.0650324\n",
      "3351 : Training: loss:  0.05682978\n",
      "3352 : Training: loss:  0.11524048\n",
      "3353 : Training: loss:  0.110098\n",
      "3354 : Training: loss:  0.092279896\n",
      "3355 : Training: loss:  0.07088304\n",
      "3356 : Training: loss:  0.058022942\n",
      "3357 : Training: loss:  0.08939309\n",
      "3358 : Training: loss:  0.08449637\n",
      "3359 : Training: loss:  0.09483132\n",
      "3360 : Training: loss:  0.06160715\n",
      "Validation: Loss:  0.08908405  Accuracy:  0.63461536\n",
      "3361 : Training: loss:  0.05389243\n",
      "3362 : Training: loss:  0.07951053\n",
      "3363 : Training: loss:  0.09568177\n",
      "3364 : Training: loss:  0.07077013\n",
      "3365 : Training: loss:  0.03566857\n",
      "3366 : Training: loss:  0.057895612\n",
      "3367 : Training: loss:  0.04932723\n",
      "3368 : Training: loss:  0.08845053\n",
      "3369 : Training: loss:  0.044629965\n",
      "3370 : Training: loss:  0.0938353\n",
      "3371 : Training: loss:  0.0889732\n",
      "3372 : Training: loss:  0.07975152\n",
      "3373 : Training: loss:  0.10007586\n",
      "3374 : Training: loss:  0.08584803\n",
      "3375 : Training: loss:  0.051809642\n",
      "3376 : Training: loss:  0.08498411\n",
      "3377 : Training: loss:  0.07211719\n",
      "3378 : Training: loss:  0.087136336\n",
      "3379 : Training: loss:  0.071324624\n",
      "3380 : Training: loss:  0.09711537\n",
      "Validation: Loss:  0.088980176  Accuracy:  0.61538464\n",
      "3381 : Training: loss:  0.091828294\n",
      "3382 : Training: loss:  0.091488324\n",
      "3383 : Training: loss:  0.088730715\n",
      "3384 : Training: loss:  0.06777903\n",
      "3385 : Training: loss:  0.056930963\n",
      "3386 : Training: loss:  0.078741215\n",
      "3387 : Training: loss:  0.092361175\n",
      "3388 : Training: loss:  0.07538208\n",
      "3389 : Training: loss:  0.10728157\n",
      "3390 : Training: loss:  0.08394976\n",
      "3391 : Training: loss:  0.0630893\n",
      "3392 : Training: loss:  0.07362714\n",
      "3393 : Training: loss:  0.0858256\n",
      "3394 : Training: loss:  0.08361655\n",
      "3395 : Training: loss:  0.04975212\n",
      "3396 : Training: loss:  0.08612428\n",
      "3397 : Training: loss:  0.07459165\n",
      "3398 : Training: loss:  0.089352414\n",
      "3399 : Training: loss:  0.10400238\n",
      "3400 : Training: loss:  0.090289064\n",
      "Validation: Loss:  0.08896681  Accuracy:  0.63461536\n",
      "3401 : Training: loss:  0.08430122\n",
      "3402 : Training: loss:  0.094009675\n",
      "3403 : Training: loss:  0.051955767\n",
      "3404 : Training: loss:  0.04764339\n",
      "3405 : Training: loss:  0.05177929\n",
      "3406 : Training: loss:  0.09602891\n",
      "3407 : Training: loss:  0.069320716\n",
      "3408 : Training: loss:  0.051263217\n",
      "3409 : Training: loss:  0.05241906\n",
      "3410 : Training: loss:  0.098629266\n",
      "3411 : Training: loss:  0.034247052\n",
      "3412 : Training: loss:  0.06708273\n",
      "3413 : Training: loss:  0.04765683\n",
      "3414 : Training: loss:  0.089191675\n",
      "3415 : Training: loss:  0.093076974\n",
      "3416 : Training: loss:  0.07086305\n",
      "3417 : Training: loss:  0.060854938\n",
      "3418 : Training: loss:  0.081283286\n",
      "3419 : Training: loss:  0.05609097\n",
      "3420 : Training: loss:  0.08130164\n",
      "Validation: Loss:  0.088605106  Accuracy:  0.63461536\n",
      "3421 : Training: loss:  0.056952335\n",
      "3422 : Training: loss:  0.10504555\n",
      "3423 : Training: loss:  0.117120564\n",
      "3424 : Training: loss:  0.10829186\n",
      "3425 : Training: loss:  0.024306802\n",
      "3426 : Training: loss:  0.13580519\n",
      "3427 : Training: loss:  0.043343488\n",
      "3428 : Training: loss:  0.044862833\n",
      "3429 : Training: loss:  0.04538737\n",
      "3430 : Training: loss:  0.08760934\n",
      "3431 : Training: loss:  0.10415708\n",
      "3432 : Training: loss:  0.102617234\n",
      "3433 : Training: loss:  0.06380636\n",
      "3434 : Training: loss:  0.11517186\n",
      "3435 : Training: loss:  0.092896186\n",
      "3436 : Training: loss:  0.114988886\n",
      "3437 : Training: loss:  0.07113776\n",
      "3438 : Training: loss:  0.049420092\n",
      "3439 : Training: loss:  0.08831897\n",
      "3440 : Training: loss:  0.0811775\n",
      "Validation: Loss:  0.08803265  Accuracy:  0.63461536\n",
      "3441 : Training: loss:  0.067078315\n",
      "3442 : Training: loss:  0.066317715\n",
      "3443 : Training: loss:  0.07819596\n",
      "3444 : Training: loss:  0.08601402\n",
      "3445 : Training: loss:  0.063489646\n",
      "3446 : Training: loss:  0.1269178\n",
      "3447 : Training: loss:  0.11685722\n",
      "3448 : Training: loss:  0.055336423\n",
      "3449 : Training: loss:  0.048705444\n",
      "3450 : Training: loss:  0.04507942\n",
      "3451 : Training: loss:  0.07469807\n",
      "3452 : Training: loss:  0.1074416\n",
      "3453 : Training: loss:  0.084555626\n",
      "3454 : Training: loss:  0.08182012\n",
      "3455 : Training: loss:  0.10822636\n",
      "3456 : Training: loss:  0.09146303\n",
      "3457 : Training: loss:  0.106996424\n",
      "3458 : Training: loss:  0.10723071\n",
      "3459 : Training: loss:  0.080503434\n",
      "3460 : Training: loss:  0.05384857\n",
      "Validation: Loss:  0.08780537  Accuracy:  0.63461536\n",
      "3461 : Training: loss:  0.07095453\n",
      "3462 : Training: loss:  0.046064224\n",
      "3463 : Training: loss:  0.050552025\n",
      "3464 : Training: loss:  0.04486602\n",
      "3465 : Training: loss:  0.112096556\n",
      "3466 : Training: loss:  0.07047529\n",
      "3467 : Training: loss:  0.078598164\n",
      "3468 : Training: loss:  0.048497353\n",
      "3469 : Training: loss:  0.12641136\n",
      "3470 : Training: loss:  0.10120516\n",
      "3471 : Training: loss:  0.08548059\n",
      "3472 : Training: loss:  0.103265956\n",
      "3473 : Training: loss:  0.09376063\n",
      "3474 : Training: loss:  0.0558218\n",
      "3475 : Training: loss:  0.08708452\n",
      "3476 : Training: loss:  0.09305335\n",
      "3477 : Training: loss:  0.09097236\n",
      "3478 : Training: loss:  0.08411002\n",
      "3479 : Training: loss:  0.06952597\n",
      "3480 : Training: loss:  0.072925195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss:  0.08781533  Accuracy:  0.61538464\n",
      "3481 : Training: loss:  0.08398417\n",
      "3482 : Training: loss:  0.08412008\n",
      "3483 : Training: loss:  0.029035496\n",
      "3484 : Training: loss:  0.07254149\n",
      "3485 : Training: loss:  0.116132624\n",
      "3486 : Training: loss:  0.08772585\n",
      "3487 : Training: loss:  0.10660566\n",
      "3488 : Training: loss:  0.065268\n",
      "3489 : Training: loss:  0.08144037\n",
      "3490 : Training: loss:  0.08404814\n",
      "3491 : Training: loss:  0.041656986\n",
      "3492 : Training: loss:  0.12249818\n",
      "3493 : Training: loss:  0.051475424\n",
      "3494 : Training: loss:  0.04509786\n",
      "3495 : Training: loss:  0.070604734\n",
      "3496 : Training: loss:  0.081441164\n",
      "3497 : Training: loss:  0.07575426\n",
      "3498 : Training: loss:  0.09788977\n",
      "3499 : Training: loss:  0.09737171\n",
      "3500 : Training: loss:  0.07267777\n",
      "Validation: Loss:  0.08775719  Accuracy:  0.61538464\n",
      "3501 : Training: loss:  0.097454645\n",
      "3502 : Training: loss:  0.07029126\n",
      "3503 : Training: loss:  0.058054727\n",
      "3504 : Training: loss:  0.07346338\n",
      "3505 : Training: loss:  0.06551713\n",
      "3506 : Training: loss:  0.06341573\n",
      "3507 : Training: loss:  0.06977419\n",
      "3508 : Training: loss:  0.080374256\n",
      "3509 : Training: loss:  0.08251716\n",
      "3510 : Training: loss:  0.07821682\n",
      "3511 : Training: loss:  0.027922003\n",
      "3512 : Training: loss:  0.08082842\n",
      "3513 : Training: loss:  0.043472286\n",
      "3514 : Training: loss:  0.08693285\n",
      "3515 : Training: loss:  0.09710002\n",
      "3516 : Training: loss:  0.07800074\n",
      "3517 : Training: loss:  0.11963329\n",
      "3518 : Training: loss:  0.06496406\n",
      "3519 : Training: loss:  0.0457788\n",
      "3520 : Training: loss:  0.04133466\n",
      "Validation: Loss:  0.08763162  Accuracy:  0.63461536\n",
      "3521 : Training: loss:  0.08058745\n",
      "3522 : Training: loss:  0.0576569\n",
      "3523 : Training: loss:  0.056755453\n",
      "3524 : Training: loss:  0.035335142\n",
      "3525 : Training: loss:  0.118531056\n",
      "3526 : Training: loss:  0.08502018\n",
      "3527 : Training: loss:  0.08691405\n",
      "3528 : Training: loss:  0.053159043\n",
      "3529 : Training: loss:  0.06167923\n",
      "3530 : Training: loss:  0.051534083\n",
      "3531 : Training: loss:  0.08877732\n",
      "3532 : Training: loss:  0.08927204\n",
      "3533 : Training: loss:  0.0855585\n",
      "3534 : Training: loss:  0.08912779\n",
      "3535 : Training: loss:  0.045442507\n",
      "3536 : Training: loss:  0.05895656\n",
      "3537 : Training: loss:  0.08033075\n",
      "3538 : Training: loss:  0.070934236\n",
      "3539 : Training: loss:  0.11444889\n",
      "3540 : Training: loss:  0.09063318\n",
      "Validation: Loss:  0.08754379  Accuracy:  0.61538464\n",
      "3541 : Training: loss:  0.06908608\n",
      "3542 : Training: loss:  0.09438831\n",
      "3543 : Training: loss:  0.06766646\n",
      "3544 : Training: loss:  0.0713571\n",
      "3545 : Training: loss:  0.07118848\n",
      "3546 : Training: loss:  0.09208384\n",
      "3547 : Training: loss:  0.059177253\n",
      "3548 : Training: loss:  0.055347566\n",
      "3549 : Training: loss:  0.08711524\n",
      "3550 : Training: loss:  0.09762071\n",
      "3551 : Training: loss:  0.07712835\n",
      "3552 : Training: loss:  0.074448675\n",
      "3553 : Training: loss:  0.08809867\n",
      "3554 : Training: loss:  0.086020134\n",
      "3555 : Training: loss:  0.07509845\n",
      "3556 : Training: loss:  0.05337973\n",
      "3557 : Training: loss:  0.096946344\n",
      "3558 : Training: loss:  0.07385883\n",
      "3559 : Training: loss:  0.05102517\n",
      "3560 : Training: loss:  0.095848046\n",
      "Validation: Loss:  0.08716905  Accuracy:  0.63461536\n",
      "3561 : Training: loss:  0.10097355\n",
      "3562 : Training: loss:  0.09382362\n",
      "3563 : Training: loss:  0.06912673\n",
      "3564 : Training: loss:  0.055745717\n",
      "3565 : Training: loss:  0.07819644\n",
      "3566 : Training: loss:  0.05939148\n",
      "3567 : Training: loss:  0.086378984\n",
      "3568 : Training: loss:  0.06950159\n",
      "3569 : Training: loss:  0.081002064\n",
      "3570 : Training: loss:  0.06881246\n",
      "3571 : Training: loss:  0.04573746\n",
      "3572 : Training: loss:  0.09223183\n",
      "3573 : Training: loss:  0.09601372\n",
      "3574 : Training: loss:  0.08746074\n",
      "3575 : Training: loss:  0.071985215\n",
      "3576 : Training: loss:  0.07194144\n",
      "3577 : Training: loss:  0.072598904\n",
      "3578 : Training: loss:  0.06925045\n",
      "3579 : Training: loss:  0.0769847\n",
      "3580 : Training: loss:  0.053169005\n",
      "Validation: Loss:  0.086653166  Accuracy:  0.65384614\n",
      "3581 : Training: loss:  0.055309914\n",
      "3582 : Training: loss:  0.062228803\n",
      "3583 : Training: loss:  0.12019755\n",
      "3584 : Training: loss:  0.115781754\n",
      "3585 : Training: loss:  0.08917479\n",
      "3586 : Training: loss:  0.06674291\n",
      "3587 : Training: loss:  0.0574438\n",
      "3588 : Training: loss:  0.06605055\n",
      "3589 : Training: loss:  0.11214558\n",
      "3590 : Training: loss:  0.04937944\n",
      "3591 : Training: loss:  0.0749969\n",
      "3592 : Training: loss:  0.09914616\n",
      "3593 : Training: loss:  0.07321394\n",
      "3594 : Training: loss:  0.09677757\n",
      "3595 : Training: loss:  0.08107825\n",
      "3596 : Training: loss:  0.03647992\n",
      "3597 : Training: loss:  0.084053785\n",
      "3598 : Training: loss:  0.12144019\n",
      "3599 : Training: loss:  0.058426276\n",
      "3600 : Training: loss:  0.07528401\n",
      "Validation: Loss:  0.08624297  Accuracy:  0.6730769\n",
      "3601 : Training: loss:  0.062180042\n",
      "3602 : Training: loss:  0.082763515\n",
      "3603 : Training: loss:  0.05360474\n",
      "3604 : Training: loss:  0.087717086\n",
      "3605 : Training: loss:  0.069901146\n",
      "3606 : Training: loss:  0.11323117\n",
      "3607 : Training: loss:  0.058022358\n",
      "3608 : Training: loss:  0.06565045\n",
      "3609 : Training: loss:  0.09122205\n",
      "3610 : Training: loss:  0.06407244\n",
      "3611 : Training: loss:  0.052145127\n",
      "3612 : Training: loss:  0.08550905\n",
      "3613 : Training: loss:  0.06187727\n",
      "3614 : Training: loss:  0.124128245\n",
      "3615 : Training: loss:  0.09755314\n",
      "3616 : Training: loss:  0.08200898\n",
      "3617 : Training: loss:  0.03827214\n",
      "3618 : Training: loss:  0.07583504\n",
      "3619 : Training: loss:  0.105274\n",
      "3620 : Training: loss:  0.08244019\n",
      "Validation: Loss:  0.08603275  Accuracy:  0.65384614\n",
      "3621 : Training: loss:  0.070043735\n",
      "3622 : Training: loss:  0.13156769\n",
      "3623 : Training: loss:  0.070798345\n",
      "3624 : Training: loss:  0.0723827\n",
      "3625 : Training: loss:  0.09817449\n",
      "3626 : Training: loss:  0.044732127\n",
      "3627 : Training: loss:  0.08995367\n",
      "3628 : Training: loss:  0.0816051\n",
      "3629 : Training: loss:  0.05822943\n",
      "3630 : Training: loss:  0.091464214\n",
      "3631 : Training: loss:  0.09064234\n",
      "3632 : Training: loss:  0.057526443\n",
      "3633 : Training: loss:  0.07867351\n",
      "3634 : Training: loss:  0.06594719\n",
      "3635 : Training: loss:  0.06232941\n",
      "3636 : Training: loss:  0.06650399\n",
      "3637 : Training: loss:  0.047739383\n",
      "3638 : Training: loss:  0.11278619\n",
      "3639 : Training: loss:  0.0653277\n",
      "3640 : Training: loss:  0.09579658\n",
      "Validation: Loss:  0.08563922  Accuracy:  0.65384614\n",
      "3641 : Training: loss:  0.03843601\n",
      "3642 : Training: loss:  0.072497874\n",
      "3643 : Training: loss:  0.086707726\n",
      "3644 : Training: loss:  0.1093042\n",
      "3645 : Training: loss:  0.06717967\n",
      "3646 : Training: loss:  0.07841191\n",
      "3647 : Training: loss:  0.056303196\n",
      "3648 : Training: loss:  0.08864897\n",
      "3649 : Training: loss:  0.04928641\n",
      "3650 : Training: loss:  0.0635511\n",
      "3651 : Training: loss:  0.090898894\n",
      "3652 : Training: loss:  0.046384726\n",
      "3653 : Training: loss:  0.03442506\n",
      "3654 : Training: loss:  0.08899208\n",
      "3655 : Training: loss:  0.03509896\n",
      "3656 : Training: loss:  0.041481957\n",
      "3657 : Training: loss:  0.080379486\n",
      "3658 : Training: loss:  0.079743974\n",
      "3659 : Training: loss:  0.06513809\n",
      "3660 : Training: loss:  0.051379275\n",
      "Validation: Loss:  0.08513134  Accuracy:  0.65384614\n",
      "3661 : Training: loss:  0.08642427\n",
      "3662 : Training: loss:  0.11499701\n",
      "3663 : Training: loss:  0.0967091\n",
      "3664 : Training: loss:  0.07610729\n",
      "3665 : Training: loss:  0.06889501\n",
      "3666 : Training: loss:  0.07222471\n",
      "3667 : Training: loss:  0.07880369\n",
      "3668 : Training: loss:  0.085465066\n",
      "3669 : Training: loss:  0.05728216\n",
      "3670 : Training: loss:  0.074637406\n",
      "3671 : Training: loss:  0.10106617\n",
      "3672 : Training: loss:  0.06898969\n",
      "3673 : Training: loss:  0.079018794\n",
      "3674 : Training: loss:  0.053582095\n",
      "3675 : Training: loss:  0.09379012\n",
      "3676 : Training: loss:  0.060229722\n",
      "3677 : Training: loss:  0.051673166\n",
      "3678 : Training: loss:  0.06161832\n",
      "3679 : Training: loss:  0.09464102\n",
      "3680 : Training: loss:  0.084262095\n",
      "Validation: Loss:  0.084957995  Accuracy:  0.6730769\n",
      "3681 : Training: loss:  0.07466922\n",
      "3682 : Training: loss:  0.12264354\n",
      "3683 : Training: loss:  0.10061973\n",
      "3684 : Training: loss:  0.039269853\n",
      "3685 : Training: loss:  0.07636171\n",
      "3686 : Training: loss:  0.06093357\n",
      "3687 : Training: loss:  0.055488653\n",
      "3688 : Training: loss:  0.058790978\n",
      "3689 : Training: loss:  0.10147455\n",
      "3690 : Training: loss:  0.037997298\n",
      "3691 : Training: loss:  0.08884605\n",
      "3692 : Training: loss:  0.06899965\n",
      "3693 : Training: loss:  0.108070165\n",
      "3694 : Training: loss:  0.032798998\n",
      "3695 : Training: loss:  0.062147405\n",
      "3696 : Training: loss:  0.092888646\n",
      "3697 : Training: loss:  0.049521018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3698 : Training: loss:  0.06592599\n",
      "3699 : Training: loss:  0.053409573\n",
      "3700 : Training: loss:  0.05549149\n",
      "Validation: Loss:  0.08461364  Accuracy:  0.65384614\n",
      "3701 : Training: loss:  0.033876784\n",
      "3702 : Training: loss:  0.081950285\n",
      "3703 : Training: loss:  0.072096065\n",
      "3704 : Training: loss:  0.08603927\n",
      "3705 : Training: loss:  0.10015095\n",
      "3706 : Training: loss:  0.026543558\n",
      "3707 : Training: loss:  0.09968028\n",
      "3708 : Training: loss:  0.059127405\n",
      "3709 : Training: loss:  0.0764747\n",
      "3710 : Training: loss:  0.06077349\n",
      "3711 : Training: loss:  0.060291283\n",
      "3712 : Training: loss:  0.077813424\n",
      "3713 : Training: loss:  0.083068185\n",
      "3714 : Training: loss:  0.0679983\n",
      "3715 : Training: loss:  0.08176821\n",
      "3716 : Training: loss:  0.049283\n",
      "3717 : Training: loss:  0.08189907\n",
      "3718 : Training: loss:  0.057123046\n",
      "3719 : Training: loss:  0.092839606\n",
      "3720 : Training: loss:  0.04020216\n",
      "Validation: Loss:  0.08415952  Accuracy:  0.65384614\n",
      "3721 : Training: loss:  0.12572083\n",
      "3722 : Training: loss:  0.08485965\n",
      "3723 : Training: loss:  0.054293327\n",
      "3724 : Training: loss:  0.09000817\n",
      "3725 : Training: loss:  0.05723284\n",
      "3726 : Training: loss:  0.05180828\n",
      "3727 : Training: loss:  0.06041847\n",
      "3728 : Training: loss:  0.06791889\n",
      "3729 : Training: loss:  0.1032044\n",
      "3730 : Training: loss:  0.040188067\n",
      "3731 : Training: loss:  0.06891788\n",
      "3732 : Training: loss:  0.057711832\n",
      "3733 : Training: loss:  0.03928101\n",
      "3734 : Training: loss:  0.07337106\n",
      "3735 : Training: loss:  0.07516974\n",
      "3736 : Training: loss:  0.031688575\n",
      "3737 : Training: loss:  0.09116188\n",
      "3738 : Training: loss:  0.061317984\n",
      "3739 : Training: loss:  0.07634366\n",
      "3740 : Training: loss:  0.05440738\n",
      "Validation: Loss:  0.08377913  Accuracy:  0.6730769\n",
      "3741 : Training: loss:  0.06630114\n",
      "3742 : Training: loss:  0.05264707\n",
      "3743 : Training: loss:  0.061556987\n",
      "3744 : Training: loss:  0.07546937\n",
      "3745 : Training: loss:  0.044977717\n",
      "3746 : Training: loss:  0.058509793\n",
      "3747 : Training: loss:  0.044706468\n",
      "3748 : Training: loss:  0.07464447\n",
      "3749 : Training: loss:  0.06334165\n",
      "3750 : Training: loss:  0.060480207\n",
      "3751 : Training: loss:  0.07096411\n",
      "3752 : Training: loss:  0.08110244\n",
      "3753 : Training: loss:  0.045339413\n",
      "3754 : Training: loss:  0.08200821\n",
      "3755 : Training: loss:  0.12579273\n",
      "3756 : Training: loss:  0.110978715\n",
      "3757 : Training: loss:  0.08155074\n",
      "3758 : Training: loss:  0.09358246\n",
      "3759 : Training: loss:  0.0591682\n",
      "3760 : Training: loss:  0.050939944\n",
      "Validation: Loss:  0.08375866  Accuracy:  0.65384614\n",
      "3761 : Training: loss:  0.065177076\n",
      "3762 : Training: loss:  0.09374811\n",
      "3763 : Training: loss:  0.09152028\n",
      "3764 : Training: loss:  0.068990305\n",
      "3765 : Training: loss:  0.09046406\n",
      "3766 : Training: loss:  0.07622198\n",
      "3767 : Training: loss:  0.06284476\n",
      "3768 : Training: loss:  0.062128946\n",
      "3769 : Training: loss:  0.086420394\n",
      "3770 : Training: loss:  0.047962613\n",
      "3771 : Training: loss:  0.09387921\n",
      "3772 : Training: loss:  0.043355465\n",
      "3773 : Training: loss:  0.021951832\n",
      "3774 : Training: loss:  0.089073926\n",
      "3775 : Training: loss:  0.08229174\n",
      "3776 : Training: loss:  0.07490049\n",
      "3777 : Training: loss:  0.068697326\n",
      "3778 : Training: loss:  0.075352035\n",
      "3779 : Training: loss:  0.039172433\n",
      "3780 : Training: loss:  0.097257726\n",
      "Validation: Loss:  0.08327457  Accuracy:  0.6923077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2098, 0.029, 0.083, 0.0275, 0.0133, 0.0219,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.083275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1601, 0.048, 0.0138, 0.0082, 0.0383, 0.0702...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1097, 0.0863, 0.0359, 0.0198, 0.01, 0.056, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0401, 0.026, 0.0106, 0.0022, 0.0141, 0.0309...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1061, 0.0346, 0.0367, 0.0173, 0.0121, 0.027...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0385, 0.0107, 0.1164, 0.1972, 0.0056, 0.011...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0197, 0.0071, 0.0615, 0.7281, 0.0101, 0.005...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0161, 0.0092, 0.0533, 0.7294, 0.014, 0.0066...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0697, 0.0093, 0.0833, 0.1654, 0.0179, 0.023...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0799, 0.0457, 0.0015, 0.0016, 0.3067, 0.090...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0299, 0.1608, 0.0011, 0.0028, 0.2503, 0.131...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0254, 0.1269, 0.0024, 0.0023, 0.0118, 0.168...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0035, 0.0281, 0.0029, 0.0016, 0.0102, 0.012...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0004, 0.009, 0.0059, 0.0046, 0.0025, 0.0021...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0257, 0.0256, 0.0039, 0.0029, 0.0522, 0.017...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1505, 0.0714, 0.0052, 0.0006, 0.011, 0.0472...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0352, 0.0086, 0.0606, 0.0627, 0.0095, 0.014...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0125, 0.0061, 0.0133, 0.013, 0.0087, 0.0104...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0439, 0.0626, 0.0099, 0.0051, 0.0434, 0.112...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0954, 0.1096, 0.0094, 0.0026, 0.0615, 0.176...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0776, 0.0186, 0.0355, 0.005, 0.0105, 0.0278...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0064, 0.0148, 0.0065, 0.0177, 0.0036, 0.008...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0433, 0.0138, 0.0143, 0.0026, 0.0143, 0.027...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0413, 0.1298, 0.0072, 0.001, 0.0394, 0.223,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.106, 0.0177, 0.0946, 0.0472, 0.0118, 0.0279...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0809, 0.0135, 0.051, 0.0266, 0.0115, 0.008,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0633, 0.0174, 0.0218, 0.0089, 0.0341, 0.013...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0192, 0.0096, 0.0102, 0.012, 0.0179, 0.0125...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0011, 0.0046, 0.002, 0.0049, 0.0057, 0.0032...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0021, 0.0057, 0.0009, 0.002, 0.008, 0.0043,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0023, 0.0143, 0.0046, 0.0002, 0.0072, 0.009...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0285, 0.0094, 0.0289, 0.0242, 0.0093, 0.015...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0165, 0.0064, 0.0149, 0.0326, 0.0047, 0.006...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0616, 0.025, 0.0341, 0.0335, 0.0167, 0.0161...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0506, 0.0405, 0.014, 0.0036, 0.0126, 0.0409...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0719, 0.0109, 0.0276, 0.0285, 0.0047, 0.006...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0171, 0.0244, 0.0332, 0.106, 0.0042, 0.0232...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0003, 0.008, 0.0018, 0.0153, 0.0003, 0.0069...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0146, 0.0202, 0.0203, 0.046, 0.073, 0.0098,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0209, 0.0158, 0.026, 0.0577, 0.0399, 0.0122...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0181, 0.0154, 0.0148, 0.0055, 0.008, 0.0044...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0409, 0.0153, 0.0633, 0.1307, 0.0113, 0.011...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0046, 0.0129, 0.0078, 0.0639, 0.001, 0.0066...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0006, 0.0072, 0.0012, 0.0271, 0.0002, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0006, 0.0035, 0.0008, 0.0152, 1e-04, 0.0009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0015, 0.0063, 0.004, 0.0674, 0.0004, 0.0022...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0173, 0.0063, 0.0435, 0.0032, 0.0055, 0.007...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.134, 0.015, 0.0761, 0.031, 0.0074, 0.0206, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0258, 0.0637, 0.0027, 0.0007, 0.0687, 0.038...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.053, 0.2335, 0.0016, 0.0008, 0.0185, 0.1566...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0856, 0.023, 0.0096, 0.0021, 0.0069, 0.0288...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0811, 0.0501, 0.0139, 0.0063, 0.0109, 0.088...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.2098, 0.029, 0.083, 0.0275, 0.0133, 0.0219,...               0   \n",
       "1   [0.1601, 0.048, 0.0138, 0.0082, 0.0383, 0.0702...               0   \n",
       "2   [0.1097, 0.0863, 0.0359, 0.0198, 0.01, 0.056, ...               0   \n",
       "3   [0.0401, 0.026, 0.0106, 0.0022, 0.0141, 0.0309...               1   \n",
       "4   [0.1061, 0.0346, 0.0367, 0.0173, 0.0121, 0.027...               1   \n",
       "5   [0.0385, 0.0107, 0.1164, 0.1972, 0.0056, 0.011...               2   \n",
       "6   [0.0197, 0.0071, 0.0615, 0.7281, 0.0101, 0.005...               3   \n",
       "7   [0.0161, 0.0092, 0.0533, 0.7294, 0.014, 0.0066...               3   \n",
       "8   [0.0697, 0.0093, 0.0833, 0.1654, 0.0179, 0.023...               3   \n",
       "9   [0.0799, 0.0457, 0.0015, 0.0016, 0.3067, 0.090...               4   \n",
       "10  [0.0299, 0.1608, 0.0011, 0.0028, 0.2503, 0.131...               4   \n",
       "11  [0.0254, 0.1269, 0.0024, 0.0023, 0.0118, 0.168...               5   \n",
       "12  [0.0035, 0.0281, 0.0029, 0.0016, 0.0102, 0.012...               6   \n",
       "13  [0.0004, 0.009, 0.0059, 0.0046, 0.0025, 0.0021...               7   \n",
       "14  [0.0257, 0.0256, 0.0039, 0.0029, 0.0522, 0.017...               8   \n",
       "15  [0.1505, 0.0714, 0.0052, 0.0006, 0.011, 0.0472...               8   \n",
       "16  [0.0352, 0.0086, 0.0606, 0.0627, 0.0095, 0.014...               9   \n",
       "17  [0.0125, 0.0061, 0.0133, 0.013, 0.0087, 0.0104...               9   \n",
       "18  [0.0439, 0.0626, 0.0099, 0.0051, 0.0434, 0.112...              10   \n",
       "19  [0.0954, 0.1096, 0.0094, 0.0026, 0.0615, 0.176...              10   \n",
       "20  [0.0776, 0.0186, 0.0355, 0.005, 0.0105, 0.0278...              11   \n",
       "21  [0.0064, 0.0148, 0.0065, 0.0177, 0.0036, 0.008...              11   \n",
       "22  [0.0433, 0.0138, 0.0143, 0.0026, 0.0143, 0.027...              12   \n",
       "23  [0.0413, 0.1298, 0.0072, 0.001, 0.0394, 0.223,...              13   \n",
       "24  [0.106, 0.0177, 0.0946, 0.0472, 0.0118, 0.0279...              13   \n",
       "25  [0.0809, 0.0135, 0.051, 0.0266, 0.0115, 0.008,...              14   \n",
       "26  [0.0633, 0.0174, 0.0218, 0.0089, 0.0341, 0.013...              14   \n",
       "27  [0.0192, 0.0096, 0.0102, 0.012, 0.0179, 0.0125...              15   \n",
       "28  [0.0011, 0.0046, 0.002, 0.0049, 0.0057, 0.0032...              15   \n",
       "29  [0.0021, 0.0057, 0.0009, 0.002, 0.008, 0.0043,...              15   \n",
       "30  [0.0023, 0.0143, 0.0046, 0.0002, 0.0072, 0.009...              16   \n",
       "31  [0.0285, 0.0094, 0.0289, 0.0242, 0.0093, 0.015...              17   \n",
       "32  [0.0165, 0.0064, 0.0149, 0.0326, 0.0047, 0.006...              17   \n",
       "33  [0.0616, 0.025, 0.0341, 0.0335, 0.0167, 0.0161...              18   \n",
       "34  [0.0506, 0.0405, 0.014, 0.0036, 0.0126, 0.0409...              19   \n",
       "35  [0.0719, 0.0109, 0.0276, 0.0285, 0.0047, 0.006...              20   \n",
       "36  [0.0171, 0.0244, 0.0332, 0.106, 0.0042, 0.0232...              21   \n",
       "37  [0.0003, 0.008, 0.0018, 0.0153, 0.0003, 0.0069...              21   \n",
       "38  [0.0146, 0.0202, 0.0203, 0.046, 0.073, 0.0098,...              22   \n",
       "39  [0.0209, 0.0158, 0.026, 0.0577, 0.0399, 0.0122...              22   \n",
       "40  [0.0181, 0.0154, 0.0148, 0.0055, 0.008, 0.0044...              22   \n",
       "41  [0.0409, 0.0153, 0.0633, 0.1307, 0.0113, 0.011...              22   \n",
       "42  [0.0046, 0.0129, 0.0078, 0.0639, 0.001, 0.0066...              23   \n",
       "43  [0.0006, 0.0072, 0.0012, 0.0271, 0.0002, 0.001...              23   \n",
       "44  [0.0006, 0.0035, 0.0008, 0.0152, 1e-04, 0.0009...              23   \n",
       "45  [0.0015, 0.0063, 0.004, 0.0674, 0.0004, 0.0022...              23   \n",
       "46  [0.0173, 0.0063, 0.0435, 0.0032, 0.0055, 0.007...              24   \n",
       "47  [0.134, 0.015, 0.0761, 0.031, 0.0074, 0.0206, ...              24   \n",
       "48  [0.0258, 0.0637, 0.0027, 0.0007, 0.0687, 0.038...              25   \n",
       "49  [0.053, 0.2335, 0.0016, 0.0008, 0.0185, 0.1566...              25   \n",
       "50  [0.0856, 0.023, 0.0096, 0.0021, 0.0069, 0.0288...              26   \n",
       "51  [0.0811, 0.0501, 0.0139, 0.0063, 0.0109, 0.088...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.692308  0.083275  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                 10       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                10       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                11       NaN       NaN  \n",
       "34                12       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                14       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3781 : Training: loss:  0.07687883\n",
      "3782 : Training: loss:  0.08099518\n",
      "3783 : Training: loss:  0.062297136\n",
      "3784 : Training: loss:  0.09588943\n",
      "3785 : Training: loss:  0.07647847\n",
      "3786 : Training: loss:  0.06616076\n",
      "3787 : Training: loss:  0.08576997\n",
      "3788 : Training: loss:  0.01916387\n",
      "3789 : Training: loss:  0.032538738\n",
      "3790 : Training: loss:  0.077730455\n",
      "3791 : Training: loss:  0.06547514\n",
      "3792 : Training: loss:  0.073295675\n",
      "3793 : Training: loss:  0.075587615\n",
      "3794 : Training: loss:  0.10190978\n",
      "3795 : Training: loss:  0.09748283\n",
      "3796 : Training: loss:  0.10176048\n",
      "3797 : Training: loss:  0.071925946\n",
      "3798 : Training: loss:  0.0764423\n",
      "3799 : Training: loss:  0.06374024\n",
      "3800 : Training: loss:  0.038286995\n",
      "Validation: Loss:  0.0832637  Accuracy:  0.6923077\n",
      "3801 : Training: loss:  0.055535607\n",
      "3802 : Training: loss:  0.043168142\n",
      "3803 : Training: loss:  0.05759029\n",
      "3804 : Training: loss:  0.035415594\n",
      "3805 : Training: loss:  0.09837756\n",
      "3806 : Training: loss:  0.036976345\n",
      "3807 : Training: loss:  0.059651036\n",
      "3808 : Training: loss:  0.071323164\n",
      "3809 : Training: loss:  0.11457219\n",
      "3810 : Training: loss:  0.07704511\n",
      "3811 : Training: loss:  0.052781202\n",
      "3812 : Training: loss:  0.042051036\n",
      "3813 : Training: loss:  0.06566271\n",
      "3814 : Training: loss:  0.065593764\n",
      "3815 : Training: loss:  0.059248284\n",
      "3816 : Training: loss:  0.08327724\n",
      "3817 : Training: loss:  0.08852138\n",
      "3818 : Training: loss:  0.11799777\n",
      "3819 : Training: loss:  0.10839439\n",
      "3820 : Training: loss:  0.07299711\n",
      "Validation: Loss:  0.08339272  Accuracy:  0.6923077\n",
      "3821 : Training: loss:  0.082639426\n",
      "3822 : Training: loss:  0.086174384\n",
      "3823 : Training: loss:  0.040537044\n",
      "3824 : Training: loss:  0.095315896\n",
      "3825 : Training: loss:  0.07295481\n",
      "3826 : Training: loss:  0.08208374\n",
      "3827 : Training: loss:  0.027814317\n",
      "3828 : Training: loss:  0.07075265\n",
      "3829 : Training: loss:  0.036571465\n",
      "3830 : Training: loss:  0.0653499\n",
      "3831 : Training: loss:  0.09009743\n",
      "3832 : Training: loss:  0.028514767\n",
      "3833 : Training: loss:  0.13771039\n",
      "3834 : Training: loss:  0.06136783\n",
      "3835 : Training: loss:  0.09532877\n",
      "3836 : Training: loss:  0.06682694\n",
      "3837 : Training: loss:  0.08721366\n",
      "3838 : Training: loss:  0.080024034\n",
      "3839 : Training: loss:  0.059301358\n",
      "3840 : Training: loss:  0.081521526\n",
      "Validation: Loss:  0.083133616  Accuracy:  0.6923077\n",
      "3841 : Training: loss:  0.049466558\n",
      "3842 : Training: loss:  0.10446538\n",
      "3843 : Training: loss:  0.06716055\n",
      "3844 : Training: loss:  0.056615666\n",
      "3845 : Training: loss:  0.046962764\n",
      "3846 : Training: loss:  0.050781917\n",
      "3847 : Training: loss:  0.09475198\n",
      "3848 : Training: loss:  0.07109725\n",
      "3849 : Training: loss:  0.09198476\n",
      "3850 : Training: loss:  0.092373736\n",
      "3851 : Training: loss:  0.072896324\n",
      "3852 : Training: loss:  0.07499579\n",
      "3853 : Training: loss:  0.04118615\n",
      "3854 : Training: loss:  0.051779393\n",
      "3855 : Training: loss:  0.06573585\n",
      "3856 : Training: loss:  0.06413662\n",
      "3857 : Training: loss:  0.110005446\n",
      "3858 : Training: loss:  0.09711456\n",
      "3859 : Training: loss:  0.09918374\n",
      "3860 : Training: loss:  0.101274215\n",
      "Validation: Loss:  0.083039664  Accuracy:  0.6730769\n",
      "3861 : Training: loss:  0.10497719\n",
      "3862 : Training: loss:  0.11028493\n",
      "3863 : Training: loss:  0.027719224\n",
      "3864 : Training: loss:  0.052618075\n",
      "3865 : Training: loss:  0.068347804\n",
      "3866 : Training: loss:  0.028637467\n",
      "3867 : Training: loss:  0.09484086\n",
      "3868 : Training: loss:  0.04155862\n",
      "3869 : Training: loss:  0.08465583\n",
      "3870 : Training: loss:  0.063691825\n",
      "3871 : Training: loss:  0.11141812\n",
      "3872 : Training: loss:  0.046130322\n",
      "3873 : Training: loss:  0.08456958\n",
      "3874 : Training: loss:  0.044726614\n",
      "3875 : Training: loss:  0.046736613\n",
      "3876 : Training: loss:  0.092952095\n",
      "3877 : Training: loss:  0.044209175\n",
      "3878 : Training: loss:  0.05218942\n",
      "3879 : Training: loss:  0.08346577\n",
      "3880 : Training: loss:  0.06444773\n",
      "Validation: Loss:  0.08290699  Accuracy:  0.6730769\n",
      "3881 : Training: loss:  0.044477277\n",
      "3882 : Training: loss:  0.09308263\n",
      "3883 : Training: loss:  0.08286628\n",
      "3884 : Training: loss:  0.05055224\n",
      "3885 : Training: loss:  0.061983254\n",
      "3886 : Training: loss:  0.035515174\n",
      "3887 : Training: loss:  0.049374685\n",
      "3888 : Training: loss:  0.06959524\n",
      "3889 : Training: loss:  0.035629597\n",
      "3890 : Training: loss:  0.10646631\n",
      "3891 : Training: loss:  0.06990242\n",
      "3892 : Training: loss:  0.08634557\n",
      "3893 : Training: loss:  0.06782463\n",
      "3894 : Training: loss:  0.08039593\n",
      "3895 : Training: loss:  0.06293097\n",
      "3896 : Training: loss:  0.046493307\n",
      "3897 : Training: loss:  0.07770491\n",
      "3898 : Training: loss:  0.081537515\n",
      "3899 : Training: loss:  0.06421857\n",
      "3900 : Training: loss:  0.078190744\n",
      "Validation: Loss:  0.08266896  Accuracy:  0.6923077\n",
      "3901 : Training: loss:  0.05053685\n",
      "3902 : Training: loss:  0.083879195\n",
      "3903 : Training: loss:  0.07205498\n",
      "3904 : Training: loss:  0.047023933\n",
      "3905 : Training: loss:  0.08431292\n",
      "3906 : Training: loss:  0.079780474\n",
      "3907 : Training: loss:  0.054543946\n",
      "3908 : Training: loss:  0.05559785\n",
      "3909 : Training: loss:  0.050233394\n",
      "3910 : Training: loss:  0.064966045\n",
      "3911 : Training: loss:  0.05469199\n",
      "3912 : Training: loss:  0.03602422\n",
      "3913 : Training: loss:  0.05647271\n",
      "3914 : Training: loss:  0.04051041\n",
      "3915 : Training: loss:  0.05131219\n",
      "3916 : Training: loss:  0.07714434\n",
      "3917 : Training: loss:  0.13106085\n",
      "3918 : Training: loss:  0.053623173\n",
      "3919 : Training: loss:  0.080699675\n",
      "3920 : Training: loss:  0.07216019\n",
      "Validation: Loss:  0.082124054  Accuracy:  0.6923077\n",
      "3921 : Training: loss:  0.04183394\n",
      "3922 : Training: loss:  0.019958708\n",
      "3923 : Training: loss:  0.05131581\n",
      "3924 : Training: loss:  0.07437538\n",
      "3925 : Training: loss:  0.0643332\n",
      "3926 : Training: loss:  0.10225806\n",
      "3927 : Training: loss:  0.041406717\n",
      "3928 : Training: loss:  0.06980177\n",
      "3929 : Training: loss:  0.060149092\n",
      "3930 : Training: loss:  0.07931295\n",
      "3931 : Training: loss:  0.023132546\n",
      "3932 : Training: loss:  0.04735496\n",
      "3933 : Training: loss:  0.038175687\n",
      "3934 : Training: loss:  0.04640366\n",
      "3935 : Training: loss:  0.092927106\n",
      "3936 : Training: loss:  0.10575047\n",
      "3937 : Training: loss:  0.06030271\n",
      "3938 : Training: loss:  0.042376287\n",
      "3939 : Training: loss:  0.11137246\n",
      "3940 : Training: loss:  0.092089795\n",
      "Validation: Loss:  0.08186574  Accuracy:  0.6923077\n",
      "3941 : Training: loss:  0.047201533\n",
      "3942 : Training: loss:  0.05561786\n",
      "3943 : Training: loss:  0.031493176\n",
      "3944 : Training: loss:  0.03602394\n",
      "3945 : Training: loss:  0.057000965\n",
      "3946 : Training: loss:  0.0729176\n",
      "3947 : Training: loss:  0.09711928\n",
      "3948 : Training: loss:  0.09029191\n",
      "3949 : Training: loss:  0.03617845\n",
      "3950 : Training: loss:  0.0728837\n",
      "3951 : Training: loss:  0.06813583\n",
      "3952 : Training: loss:  0.053061772\n",
      "3953 : Training: loss:  0.08236226\n",
      "3954 : Training: loss:  0.07697796\n",
      "3955 : Training: loss:  0.06078903\n",
      "3956 : Training: loss:  0.073539935\n",
      "3957 : Training: loss:  0.037102576\n",
      "3958 : Training: loss:  0.06886855\n",
      "3959 : Training: loss:  0.05118617\n",
      "3960 : Training: loss:  0.09132756\n",
      "Validation: Loss:  0.08174893  Accuracy:  0.6730769\n",
      "3961 : Training: loss:  0.08063992\n",
      "3962 : Training: loss:  0.07506913\n",
      "3963 : Training: loss:  0.06779877\n",
      "3964 : Training: loss:  0.0797264\n",
      "3965 : Training: loss:  0.09841588\n",
      "3966 : Training: loss:  0.08529871\n",
      "3967 : Training: loss:  0.071019\n",
      "3968 : Training: loss:  0.04729559\n",
      "3969 : Training: loss:  0.091999054\n",
      "3970 : Training: loss:  0.0642399\n",
      "3971 : Training: loss:  0.09559755\n",
      "3972 : Training: loss:  0.06907534\n",
      "3973 : Training: loss:  0.05693129\n",
      "3974 : Training: loss:  0.049414758\n",
      "3975 : Training: loss:  0.09236479\n",
      "3976 : Training: loss:  0.08582703\n",
      "3977 : Training: loss:  0.055250235\n",
      "3978 : Training: loss:  0.040343713\n",
      "3979 : Training: loss:  0.08883583\n",
      "3980 : Training: loss:  0.077530056\n",
      "Validation: Loss:  0.081451245  Accuracy:  0.6923077\n",
      "3981 : Training: loss:  0.102179185\n",
      "3982 : Training: loss:  0.092120476\n",
      "3983 : Training: loss:  0.07380192\n",
      "3984 : Training: loss:  0.08892175\n",
      "3985 : Training: loss:  0.08604766\n",
      "3986 : Training: loss:  0.06860836\n",
      "3987 : Training: loss:  0.069557264\n",
      "3988 : Training: loss:  0.051974718\n",
      "3989 : Training: loss:  0.11203278\n",
      "3990 : Training: loss:  0.0726021\n",
      "3991 : Training: loss:  0.06980985\n",
      "3992 : Training: loss:  0.06969178\n",
      "3993 : Training: loss:  0.029397404\n",
      "3994 : Training: loss:  0.07284859\n",
      "3995 : Training: loss:  0.09548959\n",
      "3996 : Training: loss:  0.07008485\n",
      "3997 : Training: loss:  0.07049149\n",
      "3998 : Training: loss:  0.0609819\n",
      "3999 : Training: loss:  0.064598255\n",
      "4000 : Training: loss:  0.06900859\n",
      "Validation: Loss:  0.08120905  Accuracy:  0.6730769\n",
      "4001 : Training: loss:  0.033673596\n",
      "4002 : Training: loss:  0.06418229\n",
      "4003 : Training: loss:  0.06957435\n",
      "4004 : Training: loss:  0.08927137\n",
      "4005 : Training: loss:  0.097407304\n",
      "4006 : Training: loss:  0.09067239\n",
      "4007 : Training: loss:  0.04634498\n",
      "4008 : Training: loss:  0.1185918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4009 : Training: loss:  0.052969832\n",
      "4010 : Training: loss:  0.062178917\n",
      "4011 : Training: loss:  0.07066776\n",
      "4012 : Training: loss:  0.061363053\n",
      "4013 : Training: loss:  0.056223527\n",
      "4014 : Training: loss:  0.08959594\n",
      "4015 : Training: loss:  0.07919084\n",
      "4016 : Training: loss:  0.04935367\n",
      "4017 : Training: loss:  0.106469095\n",
      "4018 : Training: loss:  0.049747154\n",
      "4019 : Training: loss:  0.083126135\n",
      "4020 : Training: loss:  0.024764325\n",
      "Validation: Loss:  0.08098923  Accuracy:  0.6923077\n",
      "4021 : Training: loss:  0.09043932\n",
      "4022 : Training: loss:  0.08734414\n",
      "4023 : Training: loss:  0.06384276\n",
      "4024 : Training: loss:  0.06981238\n",
      "4025 : Training: loss:  0.071585186\n",
      "4026 : Training: loss:  0.09277037\n",
      "4027 : Training: loss:  0.07389918\n",
      "4028 : Training: loss:  0.054960664\n",
      "4029 : Training: loss:  0.044767585\n",
      "4030 : Training: loss:  0.08989882\n",
      "4031 : Training: loss:  0.072436824\n",
      "4032 : Training: loss:  0.053782552\n",
      "4033 : Training: loss:  0.09912454\n",
      "4034 : Training: loss:  0.07950223\n",
      "4035 : Training: loss:  0.06419456\n",
      "4036 : Training: loss:  0.0702938\n",
      "4037 : Training: loss:  0.07672519\n",
      "4038 : Training: loss:  0.092688546\n",
      "4039 : Training: loss:  0.04857998\n",
      "4040 : Training: loss:  0.08463969\n",
      "Validation: Loss:  0.08091991  Accuracy:  0.6923077\n",
      "4041 : Training: loss:  0.08904581\n",
      "4042 : Training: loss:  0.07830862\n",
      "4043 : Training: loss:  0.051716033\n",
      "4044 : Training: loss:  0.07092105\n",
      "4045 : Training: loss:  0.045549627\n",
      "4046 : Training: loss:  0.08313584\n",
      "4047 : Training: loss:  0.03341052\n",
      "4048 : Training: loss:  0.06544937\n",
      "4049 : Training: loss:  0.0798944\n",
      "4050 : Training: loss:  0.0837959\n",
      "4051 : Training: loss:  0.019647768\n",
      "4052 : Training: loss:  0.0764088\n",
      "4053 : Training: loss:  0.014850422\n",
      "4054 : Training: loss:  0.056752566\n",
      "4055 : Training: loss:  0.08650417\n",
      "4056 : Training: loss:  0.06968533\n",
      "4057 : Training: loss:  0.096260265\n",
      "4058 : Training: loss:  0.06225715\n",
      "4059 : Training: loss:  0.047969837\n",
      "4060 : Training: loss:  0.086773485\n",
      "Validation: Loss:  0.08058995  Accuracy:  0.6923077\n",
      "4061 : Training: loss:  0.051289063\n",
      "4062 : Training: loss:  0.086188704\n",
      "4063 : Training: loss:  0.08048509\n",
      "4064 : Training: loss:  0.057532188\n",
      "4065 : Training: loss:  0.07925136\n",
      "4066 : Training: loss:  0.109385975\n",
      "4067 : Training: loss:  0.03769638\n",
      "4068 : Training: loss:  0.03261255\n",
      "4069 : Training: loss:  0.06459853\n",
      "4070 : Training: loss:  0.04216357\n",
      "4071 : Training: loss:  0.12351197\n",
      "4072 : Training: loss:  0.08652306\n",
      "4073 : Training: loss:  0.082626015\n",
      "4074 : Training: loss:  0.04337066\n",
      "4075 : Training: loss:  0.09237398\n",
      "4076 : Training: loss:  0.046303935\n",
      "4077 : Training: loss:  0.055630874\n",
      "4078 : Training: loss:  0.07168731\n",
      "4079 : Training: loss:  0.06615129\n",
      "4080 : Training: loss:  0.0575671\n",
      "Validation: Loss:  0.08031124  Accuracy:  0.6730769\n",
      "4081 : Training: loss:  0.061112072\n",
      "4082 : Training: loss:  0.03612965\n",
      "4083 : Training: loss:  0.10665284\n",
      "4084 : Training: loss:  0.13542259\n",
      "4085 : Training: loss:  0.06893148\n",
      "4086 : Training: loss:  0.07317911\n",
      "4087 : Training: loss:  0.07429715\n",
      "4088 : Training: loss:  0.09764246\n",
      "4089 : Training: loss:  0.06826299\n",
      "4090 : Training: loss:  0.044356246\n",
      "4091 : Training: loss:  0.045701917\n",
      "4092 : Training: loss:  0.05391935\n",
      "4093 : Training: loss:  0.07361729\n",
      "4094 : Training: loss:  0.05709085\n",
      "4095 : Training: loss:  0.091884896\n",
      "4096 : Training: loss:  0.07497701\n",
      "4097 : Training: loss:  0.05128205\n",
      "4098 : Training: loss:  0.05842851\n",
      "4099 : Training: loss:  0.067201324\n",
      "4100 : Training: loss:  0.04468378\n",
      "Validation: Loss:  0.080119655  Accuracy:  0.6923077\n",
      "4101 : Training: loss:  0.067650445\n",
      "4102 : Training: loss:  0.081941895\n",
      "4103 : Training: loss:  0.061741915\n",
      "4104 : Training: loss:  0.062138695\n",
      "4105 : Training: loss:  0.0674601\n",
      "4106 : Training: loss:  0.06439136\n",
      "4107 : Training: loss:  0.10576319\n",
      "4108 : Training: loss:  0.100165404\n",
      "4109 : Training: loss:  0.039730664\n",
      "4110 : Training: loss:  0.03363756\n",
      "4111 : Training: loss:  0.04856109\n",
      "4112 : Training: loss:  0.046925873\n",
      "4113 : Training: loss:  0.045265373\n",
      "4114 : Training: loss:  0.07543014\n",
      "4115 : Training: loss:  0.04704291\n",
      "4116 : Training: loss:  0.11221339\n",
      "4117 : Training: loss:  0.040156946\n",
      "4118 : Training: loss:  0.08883838\n",
      "4119 : Training: loss:  0.06431969\n",
      "4120 : Training: loss:  0.08490913\n",
      "Validation: Loss:  0.079878256  Accuracy:  0.71153843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3417, 0.0293, 0.0988, 0.0274, 0.0116, 0.017...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.079878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.3109, 0.0558, 0.0145, 0.0084, 0.0429, 0.073...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1703, 0.0964, 0.0369, 0.0184, 0.0078, 0.050...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0615, 0.0306, 0.0111, 0.0017, 0.0143, 0.033...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1667, 0.0374, 0.0403, 0.0161, 0.0108, 0.023...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0503, 0.0097, 0.159, 0.233, 0.0048, 0.0098,...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.026, 0.0063, 0.0854, 0.8034, 0.0099, 0.0041...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0201, 0.0083, 0.0699, 0.7973, 0.0138, 0.005...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0921, 0.0079, 0.0997, 0.1908, 0.017, 0.02, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1372, 0.0555, 0.0011, 0.0014, 0.4232, 0.111...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0481, 0.2371, 0.0008, 0.0025, 0.354, 0.1909...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0356, 0.1621, 0.0017, 0.0019, 0.0118, 0.219...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0026, 0.0334, 0.0021, 0.0011, 0.0108, 0.012...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0003, 0.0094, 0.0056, 0.0034, 0.0018, 0.001...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0437, 0.0324, 0.004, 0.0027, 0.0805, 0.0187...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.2899, 0.0737, 0.0046, 0.0004, 0.01, 0.039, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0435, 0.008, 0.0721, 0.0635, 0.009, 0.0126,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0132, 0.0058, 0.0127, 0.0117, 0.0087, 0.009...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0582, 0.077, 0.0091, 0.0041, 0.0486, 0.1412...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.1592, 0.1397, 0.0092, 0.002, 0.0705, 0.2288...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.1198, 0.0183, 0.0395, 0.0041, 0.0093, 0.025...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0073, 0.0164, 0.0062, 0.0162, 0.0033, 0.007...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.061, 0.0141, 0.0141, 0.0019, 0.0141, 0.0274...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0551, 0.1693, 0.0059, 0.0007, 0.0437, 0.306...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1641, 0.0177, 0.1237, 0.0487, 0.0109, 0.025...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.1196, 0.0129, 0.0615, 0.0264, 0.0099, 0.005...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0842, 0.0184, 0.0223, 0.0072, 0.0383, 0.011...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0259, 0.0104, 0.0111, 0.0115, 0.019, 0.0121...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0009, 0.0047, 0.0017, 0.0046, 0.005, 0.0026...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.002, 0.0064, 0.0007, 0.0017, 0.0079, 0.0037...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0019, 0.0144, 0.0039, 0.0002, 0.0069, 0.008...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0319, 0.0084, 0.028, 0.0214, 0.0087, 0.0133...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0188, 0.006, 0.0144, 0.0312, 0.0043, 0.0051...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0976, 0.0275, 0.0394, 0.0349, 0.0176, 0.014...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0714, 0.0451, 0.0127, 0.0027, 0.0119, 0.040...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.1281, 0.0107, 0.0356, 0.0305, 0.0041, 0.005...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.019, 0.0259, 0.0362, 0.1074, 0.0035, 0.0235...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0002, 0.0078, 0.0016, 0.0124, 0.0002, 0.007...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0136, 0.02, 0.0222, 0.0405, 0.0764, 0.0076,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0233, 0.0158, 0.0302, 0.0582, 0.0448, 0.010...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0225, 0.0164, 0.0161, 0.0046, 0.0068, 0.003...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0567, 0.0148, 0.084, 0.1471, 0.0102, 0.0094...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0053, 0.014, 0.0076, 0.0636, 0.0008, 0.0061...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0006, 0.0086, 0.0011, 0.0227, 1e-04, 0.0017...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0006, 0.004, 0.0007, 0.0123, 1e-04, 0.0008,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0015, 0.0067, 0.0039, 0.064, 0.0003, 0.0018...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0141, 0.005, 0.0429, 0.0022, 0.0044, 0.0059...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2291, 0.0145, 0.096, 0.0324, 0.0064, 0.0174...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0313, 0.0799, 0.0021, 0.0005, 0.0816, 0.040...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0798, 0.2908, 0.001, 0.0006, 0.0174, 0.1706...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1566, 0.0216, 0.0099, 0.0018, 0.0056, 0.023...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.1241, 0.0531, 0.013, 0.0054, 0.0096, 0.0918...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3417, 0.0293, 0.0988, 0.0274, 0.0116, 0.017...               0   \n",
       "1   [0.3109, 0.0558, 0.0145, 0.0084, 0.0429, 0.073...               0   \n",
       "2   [0.1703, 0.0964, 0.0369, 0.0184, 0.0078, 0.050...               0   \n",
       "3   [0.0615, 0.0306, 0.0111, 0.0017, 0.0143, 0.033...               1   \n",
       "4   [0.1667, 0.0374, 0.0403, 0.0161, 0.0108, 0.023...               1   \n",
       "5   [0.0503, 0.0097, 0.159, 0.233, 0.0048, 0.0098,...               2   \n",
       "6   [0.026, 0.0063, 0.0854, 0.8034, 0.0099, 0.0041...               3   \n",
       "7   [0.0201, 0.0083, 0.0699, 0.7973, 0.0138, 0.005...               3   \n",
       "8   [0.0921, 0.0079, 0.0997, 0.1908, 0.017, 0.02, ...               3   \n",
       "9   [0.1372, 0.0555, 0.0011, 0.0014, 0.4232, 0.111...               4   \n",
       "10  [0.0481, 0.2371, 0.0008, 0.0025, 0.354, 0.1909...               4   \n",
       "11  [0.0356, 0.1621, 0.0017, 0.0019, 0.0118, 0.219...               5   \n",
       "12  [0.0026, 0.0334, 0.0021, 0.0011, 0.0108, 0.012...               6   \n",
       "13  [0.0003, 0.0094, 0.0056, 0.0034, 0.0018, 0.001...               7   \n",
       "14  [0.0437, 0.0324, 0.004, 0.0027, 0.0805, 0.0187...               8   \n",
       "15  [0.2899, 0.0737, 0.0046, 0.0004, 0.01, 0.039, ...               8   \n",
       "16  [0.0435, 0.008, 0.0721, 0.0635, 0.009, 0.0126,...               9   \n",
       "17  [0.0132, 0.0058, 0.0127, 0.0117, 0.0087, 0.009...               9   \n",
       "18  [0.0582, 0.077, 0.0091, 0.0041, 0.0486, 0.1412...              10   \n",
       "19  [0.1592, 0.1397, 0.0092, 0.002, 0.0705, 0.2288...              10   \n",
       "20  [0.1198, 0.0183, 0.0395, 0.0041, 0.0093, 0.025...              11   \n",
       "21  [0.0073, 0.0164, 0.0062, 0.0162, 0.0033, 0.007...              11   \n",
       "22  [0.061, 0.0141, 0.0141, 0.0019, 0.0141, 0.0274...              12   \n",
       "23  [0.0551, 0.1693, 0.0059, 0.0007, 0.0437, 0.306...              13   \n",
       "24  [0.1641, 0.0177, 0.1237, 0.0487, 0.0109, 0.025...              13   \n",
       "25  [0.1196, 0.0129, 0.0615, 0.0264, 0.0099, 0.005...              14   \n",
       "26  [0.0842, 0.0184, 0.0223, 0.0072, 0.0383, 0.011...              14   \n",
       "27  [0.0259, 0.0104, 0.0111, 0.0115, 0.019, 0.0121...              15   \n",
       "28  [0.0009, 0.0047, 0.0017, 0.0046, 0.005, 0.0026...              15   \n",
       "29  [0.002, 0.0064, 0.0007, 0.0017, 0.0079, 0.0037...              15   \n",
       "30  [0.0019, 0.0144, 0.0039, 0.0002, 0.0069, 0.008...              16   \n",
       "31  [0.0319, 0.0084, 0.028, 0.0214, 0.0087, 0.0133...              17   \n",
       "32  [0.0188, 0.006, 0.0144, 0.0312, 0.0043, 0.0051...              17   \n",
       "33  [0.0976, 0.0275, 0.0394, 0.0349, 0.0176, 0.014...              18   \n",
       "34  [0.0714, 0.0451, 0.0127, 0.0027, 0.0119, 0.040...              19   \n",
       "35  [0.1281, 0.0107, 0.0356, 0.0305, 0.0041, 0.005...              20   \n",
       "36  [0.019, 0.0259, 0.0362, 0.1074, 0.0035, 0.0235...              21   \n",
       "37  [0.0002, 0.0078, 0.0016, 0.0124, 0.0002, 0.007...              21   \n",
       "38  [0.0136, 0.02, 0.0222, 0.0405, 0.0764, 0.0076,...              22   \n",
       "39  [0.0233, 0.0158, 0.0302, 0.0582, 0.0448, 0.010...              22   \n",
       "40  [0.0225, 0.0164, 0.0161, 0.0046, 0.0068, 0.003...              22   \n",
       "41  [0.0567, 0.0148, 0.084, 0.1471, 0.0102, 0.0094...              22   \n",
       "42  [0.0053, 0.014, 0.0076, 0.0636, 0.0008, 0.0061...              23   \n",
       "43  [0.0006, 0.0086, 0.0011, 0.0227, 1e-04, 0.0017...              23   \n",
       "44  [0.0006, 0.004, 0.0007, 0.0123, 1e-04, 0.0008,...              23   \n",
       "45  [0.0015, 0.0067, 0.0039, 0.064, 0.0003, 0.0018...              23   \n",
       "46  [0.0141, 0.005, 0.0429, 0.0022, 0.0044, 0.0059...              24   \n",
       "47  [0.2291, 0.0145, 0.096, 0.0324, 0.0064, 0.0174...              24   \n",
       "48  [0.0313, 0.0799, 0.0021, 0.0005, 0.0816, 0.040...              25   \n",
       "49  [0.0798, 0.2908, 0.001, 0.0006, 0.0174, 0.1706...              25   \n",
       "50  [0.1566, 0.0216, 0.0099, 0.0018, 0.0056, 0.023...              26   \n",
       "51  [0.1241, 0.0531, 0.013, 0.0054, 0.0096, 0.0918...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.711538  0.079878  \n",
       "1                  0       NaN       NaN  \n",
       "2                  0       NaN       NaN  \n",
       "3                  0       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                 0       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                 0       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                 0       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                14       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4121 : Training: loss:  0.09390337\n",
      "4122 : Training: loss:  0.07516106\n",
      "4123 : Training: loss:  0.03662041\n",
      "4124 : Training: loss:  0.03382739\n",
      "4125 : Training: loss:  0.040991712\n",
      "4126 : Training: loss:  0.07909433\n",
      "4127 : Training: loss:  0.08959241\n",
      "4128 : Training: loss:  0.09059878\n",
      "4129 : Training: loss:  0.083799906\n",
      "4130 : Training: loss:  0.0786975\n",
      "4131 : Training: loss:  0.102722496\n",
      "4132 : Training: loss:  0.05479374\n",
      "4133 : Training: loss:  0.052552648\n",
      "4134 : Training: loss:  0.066776365\n",
      "4135 : Training: loss:  0.07104639\n",
      "4136 : Training: loss:  0.05402497\n",
      "4137 : Training: loss:  0.05249701\n",
      "4138 : Training: loss:  0.021818409\n",
      "4139 : Training: loss:  0.05235784\n",
      "4140 : Training: loss:  0.096050456\n",
      "Validation: Loss:  0.07954589  Accuracy:  0.6923077\n",
      "4141 : Training: loss:  0.07442955\n",
      "4142 : Training: loss:  0.03166995\n",
      "4143 : Training: loss:  0.061427608\n",
      "4144 : Training: loss:  0.055762358\n",
      "4145 : Training: loss:  0.083163016\n",
      "4146 : Training: loss:  0.06808578\n",
      "4147 : Training: loss:  0.059552256\n",
      "4148 : Training: loss:  0.067717314\n",
      "4149 : Training: loss:  0.046494715\n",
      "4150 : Training: loss:  0.08034981\n",
      "4151 : Training: loss:  0.09295135\n",
      "4152 : Training: loss:  0.08396182\n",
      "4153 : Training: loss:  0.085413344\n",
      "4154 : Training: loss:  0.09541909\n",
      "4155 : Training: loss:  0.06790865\n",
      "4156 : Training: loss:  0.06588665\n",
      "4157 : Training: loss:  0.053250685\n",
      "4158 : Training: loss:  0.070893794\n",
      "4159 : Training: loss:  0.10315972\n",
      "4160 : Training: loss:  0.050802413\n",
      "Validation: Loss:  0.07925357  Accuracy:  0.6730769\n",
      "4161 : Training: loss:  0.028507492\n",
      "4162 : Training: loss:  0.051936347\n",
      "4163 : Training: loss:  0.09992666\n",
      "4164 : Training: loss:  0.03568421\n",
      "4165 : Training: loss:  0.113887794\n",
      "4166 : Training: loss:  0.05896608\n",
      "4167 : Training: loss:  0.076491445\n",
      "4168 : Training: loss:  0.0735393\n",
      "4169 : Training: loss:  0.0738437\n",
      "4170 : Training: loss:  0.07969738\n",
      "4171 : Training: loss:  0.10086276\n",
      "4172 : Training: loss:  0.05745439\n",
      "4173 : Training: loss:  0.0503297\n",
      "4174 : Training: loss:  0.048835926\n",
      "4175 : Training: loss:  0.061521944\n",
      "4176 : Training: loss:  0.06663116\n",
      "4177 : Training: loss:  0.038752336\n",
      "4178 : Training: loss:  0.1031777\n",
      "4179 : Training: loss:  0.09633213\n",
      "4180 : Training: loss:  0.046739668\n",
      "Validation: Loss:  0.07896551  Accuracy:  0.6730769\n",
      "4181 : Training: loss:  0.031210594\n",
      "4182 : Training: loss:  0.05348156\n",
      "4183 : Training: loss:  0.038936663\n",
      "4184 : Training: loss:  0.08980821\n",
      "4185 : Training: loss:  0.084329404\n",
      "4186 : Training: loss:  0.06686357\n",
      "4187 : Training: loss:  0.062382266\n",
      "4188 : Training: loss:  0.05946882\n",
      "4189 : Training: loss:  0.071519814\n",
      "4190 : Training: loss:  0.061305042\n",
      "4191 : Training: loss:  0.049317077\n",
      "4192 : Training: loss:  0.030469967\n",
      "4193 : Training: loss:  0.07040637\n",
      "4194 : Training: loss:  0.0552521\n",
      "4195 : Training: loss:  0.059043333\n",
      "4196 : Training: loss:  0.06469375\n",
      "4197 : Training: loss:  0.029287653\n",
      "4198 : Training: loss:  0.030231778\n",
      "4199 : Training: loss:  0.10789996\n",
      "4200 : Training: loss:  0.098690666\n",
      "Validation: Loss:  0.07854292  Accuracy:  0.71153843\n",
      "4201 : Training: loss:  0.056823723\n",
      "4202 : Training: loss:  0.08490613\n",
      "4203 : Training: loss:  0.085670516\n",
      "4204 : Training: loss:  0.0950801\n",
      "4205 : Training: loss:  0.04665826\n",
      "4206 : Training: loss:  0.06880704\n",
      "4207 : Training: loss:  0.057262227\n",
      "4208 : Training: loss:  0.06965036\n",
      "4209 : Training: loss:  0.103910066\n",
      "4210 : Training: loss:  0.04594853\n",
      "4211 : Training: loss:  0.07727777\n",
      "4212 : Training: loss:  0.034499127\n",
      "4213 : Training: loss:  0.046958152\n",
      "4214 : Training: loss:  0.027060902\n",
      "4215 : Training: loss:  0.04047729\n",
      "4216 : Training: loss:  0.03870673\n",
      "4217 : Training: loss:  0.10116085\n",
      "4218 : Training: loss:  0.030277278\n",
      "4219 : Training: loss:  0.09332694\n",
      "4220 : Training: loss:  0.087690525\n",
      "Validation: Loss:  0.07842134  Accuracy:  0.6923077\n",
      "4221 : Training: loss:  0.06363965\n",
      "4222 : Training: loss:  0.08526523\n",
      "4223 : Training: loss:  0.08519445\n",
      "4224 : Training: loss:  0.045244034\n",
      "4225 : Training: loss:  0.06305381\n",
      "4226 : Training: loss:  0.049678065\n",
      "4227 : Training: loss:  0.077425726\n",
      "4228 : Training: loss:  0.077014774\n",
      "4229 : Training: loss:  0.08341183\n",
      "4230 : Training: loss:  0.10959925\n",
      "4231 : Training: loss:  0.04732767\n",
      "4232 : Training: loss:  0.04426458\n",
      "4233 : Training: loss:  0.047761653\n",
      "4234 : Training: loss:  0.087105684\n",
      "4235 : Training: loss:  0.072717324\n",
      "4236 : Training: loss:  0.07613788\n",
      "4237 : Training: loss:  0.09270856\n",
      "4238 : Training: loss:  0.06378039\n",
      "4239 : Training: loss:  0.06477874\n",
      "4240 : Training: loss:  0.03995876\n",
      "Validation: Loss:  0.078228205  Accuracy:  0.6923077\n",
      "4241 : Training: loss:  0.028607903\n",
      "4242 : Training: loss:  0.07922751\n",
      "4243 : Training: loss:  0.077516995\n",
      "4244 : Training: loss:  0.08585991\n",
      "4245 : Training: loss:  0.073458165\n",
      "4246 : Training: loss:  0.08196525\n",
      "4247 : Training: loss:  0.056665245\n",
      "4248 : Training: loss:  0.07733006\n",
      "4249 : Training: loss:  0.039260294\n",
      "4250 : Training: loss:  0.04226287\n",
      "4251 : Training: loss:  0.03467175\n",
      "4252 : Training: loss:  0.05194287\n",
      "4253 : Training: loss:  0.031828918\n",
      "4254 : Training: loss:  0.025373796\n",
      "4255 : Training: loss:  0.05561551\n",
      "4256 : Training: loss:  0.103875816\n",
      "4257 : Training: loss:  0.085672535\n",
      "4258 : Training: loss:  0.07390861\n",
      "4259 : Training: loss:  0.088178046\n",
      "4260 : Training: loss:  0.037576374\n",
      "Validation: Loss:  0.07809218  Accuracy:  0.71153843\n",
      "4261 : Training: loss:  0.058130316\n",
      "4262 : Training: loss:  0.08117648\n",
      "4263 : Training: loss:  0.083345555\n",
      "4264 : Training: loss:  0.064293995\n",
      "4265 : Training: loss:  0.061953932\n",
      "4266 : Training: loss:  0.04480302\n",
      "4267 : Training: loss:  0.09124662\n",
      "4268 : Training: loss:  0.042516034\n",
      "4269 : Training: loss:  0.05277668\n",
      "4270 : Training: loss:  0.1061461\n",
      "4271 : Training: loss:  0.12105785\n",
      "4272 : Training: loss:  0.066310376\n",
      "4273 : Training: loss:  0.032371275\n",
      "4274 : Training: loss:  0.072701655\n",
      "4275 : Training: loss:  0.07742147\n",
      "4276 : Training: loss:  0.03538269\n",
      "4277 : Training: loss:  0.069243185\n",
      "4278 : Training: loss:  0.089572206\n",
      "4279 : Training: loss:  0.036436733\n",
      "4280 : Training: loss:  0.10311462\n",
      "Validation: Loss:  0.07783536  Accuracy:  0.6923077\n",
      "4281 : Training: loss:  0.082761236\n",
      "4282 : Training: loss:  0.034629688\n",
      "4283 : Training: loss:  0.06196414\n",
      "4284 : Training: loss:  0.025323668\n",
      "4285 : Training: loss:  0.06616162\n",
      "4286 : Training: loss:  0.076651275\n",
      "4287 : Training: loss:  0.0930863\n",
      "4288 : Training: loss:  0.05010391\n",
      "4289 : Training: loss:  0.033088338\n",
      "4290 : Training: loss:  0.050506122\n",
      "4291 : Training: loss:  0.082023755\n",
      "4292 : Training: loss:  0.030611917\n",
      "4293 : Training: loss:  0.07690701\n",
      "4294 : Training: loss:  0.072910994\n",
      "4295 : Training: loss:  0.08900267\n",
      "4296 : Training: loss:  0.04503326\n",
      "4297 : Training: loss:  0.040445615\n",
      "4298 : Training: loss:  0.046990775\n",
      "4299 : Training: loss:  0.07546912\n",
      "4300 : Training: loss:  0.05952596\n",
      "Validation: Loss:  0.0777611  Accuracy:  0.6730769\n",
      "4301 : Training: loss:  0.06135961\n",
      "4302 : Training: loss:  0.052865125\n",
      "4303 : Training: loss:  0.06246471\n",
      "4304 : Training: loss:  0.11175394\n",
      "4305 : Training: loss:  0.057704095\n",
      "4306 : Training: loss:  0.07447463\n",
      "4307 : Training: loss:  0.06987459\n",
      "4308 : Training: loss:  0.06826984\n",
      "4309 : Training: loss:  0.043394525\n",
      "4310 : Training: loss:  0.06937121\n",
      "4311 : Training: loss:  0.09896503\n",
      "4312 : Training: loss:  0.07092013\n",
      "4313 : Training: loss:  0.08022684\n",
      "4314 : Training: loss:  0.048954107\n",
      "4315 : Training: loss:  0.038316797\n",
      "4316 : Training: loss:  0.0738641\n",
      "4317 : Training: loss:  0.085454166\n",
      "4318 : Training: loss:  0.05728573\n",
      "4319 : Training: loss:  0.088347636\n",
      "4320 : Training: loss:  0.04843399\n",
      "Validation: Loss:  0.07761597  Accuracy:  0.6730769\n",
      "4321 : Training: loss:  0.08777816\n",
      "4322 : Training: loss:  0.030174203\n",
      "4323 : Training: loss:  0.07194999\n",
      "4324 : Training: loss:  0.08291407\n",
      "4325 : Training: loss:  0.048680328\n",
      "4326 : Training: loss:  0.05871849\n",
      "4327 : Training: loss:  0.05463662\n",
      "4328 : Training: loss:  0.035580084\n",
      "4329 : Training: loss:  0.07280507\n",
      "4330 : Training: loss:  0.100754626\n",
      "4331 : Training: loss:  0.059508465\n",
      "4332 : Training: loss:  0.064454824\n",
      "4333 : Training: loss:  0.038627416\n",
      "4334 : Training: loss:  0.042091545\n",
      "4335 : Training: loss:  0.091351144\n",
      "4336 : Training: loss:  0.10833195\n",
      "4337 : Training: loss:  0.056585517\n",
      "4338 : Training: loss:  0.081232496\n",
      "4339 : Training: loss:  0.09789931\n",
      "4340 : Training: loss:  0.04033951\n",
      "Validation: Loss:  0.0774007  Accuracy:  0.6730769\n",
      "4341 : Training: loss:  0.07443875\n",
      "4342 : Training: loss:  0.06325138\n",
      "4343 : Training: loss:  0.03481427\n",
      "4344 : Training: loss:  0.10644277\n",
      "4345 : Training: loss:  0.07384077\n",
      "4346 : Training: loss:  0.026054263\n",
      "4347 : Training: loss:  0.07464612\n",
      "4348 : Training: loss:  0.114622675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4349 : Training: loss:  0.06854964\n",
      "4350 : Training: loss:  0.07411199\n",
      "4351 : Training: loss:  0.058399364\n",
      "4352 : Training: loss:  0.04854894\n",
      "4353 : Training: loss:  0.09945926\n",
      "4354 : Training: loss:  0.055841394\n",
      "4355 : Training: loss:  0.056157798\n",
      "4356 : Training: loss:  0.07581859\n",
      "4357 : Training: loss:  0.06488871\n",
      "4358 : Training: loss:  0.058572635\n",
      "4359 : Training: loss:  0.07097238\n",
      "4360 : Training: loss:  0.06845552\n",
      "Validation: Loss:  0.07712821  Accuracy:  0.6923077\n",
      "4361 : Training: loss:  0.08509656\n",
      "4362 : Training: loss:  0.055213068\n",
      "4363 : Training: loss:  0.06824964\n",
      "4364 : Training: loss:  0.051531784\n",
      "4365 : Training: loss:  0.034332722\n",
      "4366 : Training: loss:  0.033288438\n",
      "4367 : Training: loss:  0.05219565\n",
      "4368 : Training: loss:  0.07155863\n",
      "4369 : Training: loss:  0.052703045\n",
      "4370 : Training: loss:  0.066301\n",
      "4371 : Training: loss:  0.091794856\n",
      "4372 : Training: loss:  0.048583165\n",
      "4373 : Training: loss:  0.06056776\n",
      "4374 : Training: loss:  0.05697171\n",
      "4375 : Training: loss:  0.062481005\n",
      "4376 : Training: loss:  0.0474405\n",
      "4377 : Training: loss:  0.058143266\n",
      "4378 : Training: loss:  0.0840065\n",
      "4379 : Training: loss:  0.08290609\n",
      "4380 : Training: loss:  0.09042325\n",
      "Validation: Loss:  0.07699952  Accuracy:  0.6923077\n",
      "4381 : Training: loss:  0.10236392\n",
      "4382 : Training: loss:  0.07460383\n",
      "4383 : Training: loss:  0.079195485\n",
      "4384 : Training: loss:  0.04001658\n",
      "4385 : Training: loss:  0.023197122\n",
      "4386 : Training: loss:  0.09866096\n",
      "4387 : Training: loss:  0.08021198\n",
      "4388 : Training: loss:  0.077094644\n",
      "4389 : Training: loss:  0.054635905\n",
      "4390 : Training: loss:  0.10117178\n",
      "4391 : Training: loss:  0.11806091\n",
      "4392 : Training: loss:  0.06729226\n",
      "4393 : Training: loss:  0.064790994\n",
      "4394 : Training: loss:  0.009671209\n",
      "4395 : Training: loss:  0.054414406\n",
      "4396 : Training: loss:  0.05267406\n",
      "4397 : Training: loss:  0.04100056\n",
      "4398 : Training: loss:  0.09801421\n",
      "4399 : Training: loss:  0.039924473\n",
      "4400 : Training: loss:  0.0810647\n",
      "Validation: Loss:  0.07696306  Accuracy:  0.63461536\n",
      "4401 : Training: loss:  0.084090225\n",
      "4402 : Training: loss:  0.07230161\n",
      "4403 : Training: loss:  0.025502266\n",
      "4404 : Training: loss:  0.08574894\n",
      "4405 : Training: loss:  0.04919644\n",
      "4406 : Training: loss:  0.07060769\n",
      "4407 : Training: loss:  0.0866192\n",
      "4408 : Training: loss:  0.060496062\n",
      "4409 : Training: loss:  0.075188816\n",
      "4410 : Training: loss:  0.09854433\n",
      "4411 : Training: loss:  0.021090364\n",
      "4412 : Training: loss:  0.04441943\n",
      "4413 : Training: loss:  0.10252614\n",
      "4414 : Training: loss:  0.05436974\n",
      "4415 : Training: loss:  0.07326337\n",
      "4416 : Training: loss:  0.06833535\n",
      "4417 : Training: loss:  0.0981351\n",
      "4418 : Training: loss:  0.035322607\n",
      "4419 : Training: loss:  0.052943062\n",
      "4420 : Training: loss:  0.041089885\n",
      "Validation: Loss:  0.07691573  Accuracy:  0.65384614\n",
      "4421 : Training: loss:  0.10330831\n",
      "4422 : Training: loss:  0.08303842\n",
      "4423 : Training: loss:  0.09331154\n",
      "4424 : Training: loss:  0.10178598\n",
      "4425 : Training: loss:  0.076165006\n",
      "4426 : Training: loss:  0.08681488\n",
      "4427 : Training: loss:  0.043750778\n",
      "4428 : Training: loss:  0.06389386\n",
      "4429 : Training: loss:  0.07471257\n",
      "4430 : Training: loss:  0.090258494\n",
      "4431 : Training: loss:  0.04553077\n",
      "4432 : Training: loss:  0.077310145\n",
      "4433 : Training: loss:  0.09135105\n",
      "4434 : Training: loss:  0.070454955\n",
      "4435 : Training: loss:  0.07335174\n",
      "4436 : Training: loss:  0.08487054\n",
      "4437 : Training: loss:  0.06378793\n",
      "4438 : Training: loss:  0.024397578\n",
      "4439 : Training: loss:  0.038978834\n",
      "4440 : Training: loss:  0.10245622\n",
      "Validation: Loss:  0.07660735  Accuracy:  0.6730769\n",
      "4441 : Training: loss:  0.027143171\n",
      "4442 : Training: loss:  0.05525307\n",
      "4443 : Training: loss:  0.03393809\n",
      "4444 : Training: loss:  0.051687248\n",
      "4445 : Training: loss:  0.08795959\n",
      "4446 : Training: loss:  0.04501968\n",
      "4447 : Training: loss:  0.04237251\n",
      "4448 : Training: loss:  0.03786916\n",
      "4449 : Training: loss:  0.07507548\n",
      "4450 : Training: loss:  0.06642542\n",
      "4451 : Training: loss:  0.04894259\n",
      "4452 : Training: loss:  0.060140975\n",
      "4453 : Training: loss:  0.07274417\n",
      "4454 : Training: loss:  0.025078652\n",
      "4455 : Training: loss:  0.03958089\n",
      "4456 : Training: loss:  0.092567936\n",
      "4457 : Training: loss:  0.05404892\n",
      "4458 : Training: loss:  0.063028485\n",
      "4459 : Training: loss:  0.06545635\n",
      "4460 : Training: loss:  0.072294086\n",
      "Validation: Loss:  0.07635286  Accuracy:  0.6730769\n",
      "4461 : Training: loss:  0.058346454\n",
      "4462 : Training: loss:  0.107433684\n",
      "4463 : Training: loss:  0.06840041\n",
      "4464 : Training: loss:  0.0512266\n",
      "4465 : Training: loss:  0.07753759\n",
      "4466 : Training: loss:  0.05351925\n",
      "4467 : Training: loss:  0.09037798\n",
      "4468 : Training: loss:  0.061082885\n",
      "4469 : Training: loss:  0.06624362\n",
      "4470 : Training: loss:  0.03553681\n",
      "4471 : Training: loss:  0.06725395\n",
      "4472 : Training: loss:  0.06007759\n",
      "4473 : Training: loss:  0.03836165\n",
      "4474 : Training: loss:  0.05169037\n",
      "4475 : Training: loss:  0.02890835\n",
      "4476 : Training: loss:  0.08920809\n",
      "4477 : Training: loss:  0.078628495\n",
      "4478 : Training: loss:  0.06905016\n",
      "4479 : Training: loss:  0.06009765\n",
      "4480 : Training: loss:  0.0767839\n",
      "Validation: Loss:  0.07608625  Accuracy:  0.6923077\n",
      "4481 : Training: loss:  0.0938364\n",
      "4482 : Training: loss:  0.06650852\n",
      "4483 : Training: loss:  0.04904232\n",
      "4484 : Training: loss:  0.07572698\n",
      "4485 : Training: loss:  0.057840224\n",
      "4486 : Training: loss:  0.06738113\n",
      "4487 : Training: loss:  0.08531012\n",
      "4488 : Training: loss:  0.05552965\n",
      "4489 : Training: loss:  0.03057493\n",
      "4490 : Training: loss:  0.09783901\n",
      "4491 : Training: loss:  0.054768994\n",
      "4492 : Training: loss:  0.048759274\n",
      "4493 : Training: loss:  0.04334877\n",
      "4494 : Training: loss:  0.04879865\n",
      "4495 : Training: loss:  0.04295681\n",
      "4496 : Training: loss:  0.037192587\n",
      "4497 : Training: loss:  0.06575135\n",
      "4498 : Training: loss:  0.060022797\n",
      "4499 : Training: loss:  0.09929633\n",
      "4500 : Training: loss:  0.073438175\n",
      "Validation: Loss:  0.07591195  Accuracy:  0.6923077\n",
      "4501 : Training: loss:  0.050783303\n",
      "4502 : Training: loss:  0.10403859\n",
      "4503 : Training: loss:  0.06897357\n",
      "4504 : Training: loss:  0.053993076\n",
      "4505 : Training: loss:  0.1051386\n",
      "4506 : Training: loss:  0.08791162\n",
      "4507 : Training: loss:  0.038019206\n",
      "4508 : Training: loss:  0.12052764\n",
      "4509 : Training: loss:  0.09303778\n",
      "4510 : Training: loss:  0.05701191\n",
      "4511 : Training: loss:  0.024665317\n",
      "4512 : Training: loss:  0.041924648\n",
      "4513 : Training: loss:  0.044577677\n",
      "4514 : Training: loss:  0.03715787\n",
      "4515 : Training: loss:  0.08519223\n",
      "4516 : Training: loss:  0.07971047\n",
      "4517 : Training: loss:  0.07538512\n",
      "4518 : Training: loss:  0.06542303\n",
      "4519 : Training: loss:  0.043427225\n",
      "4520 : Training: loss:  0.11529987\n",
      "Validation: Loss:  0.07594628  Accuracy:  0.71153843\n",
      "4521 : Training: loss:  0.029181225\n",
      "4522 : Training: loss:  0.042682238\n",
      "4523 : Training: loss:  0.056141112\n",
      "4524 : Training: loss:  0.049134687\n",
      "4525 : Training: loss:  0.06688071\n",
      "4526 : Training: loss:  0.07128445\n",
      "4527 : Training: loss:  0.08399762\n",
      "4528 : Training: loss:  0.066947\n",
      "4529 : Training: loss:  0.07260793\n",
      "4530 : Training: loss:  0.013831275\n",
      "4531 : Training: loss:  0.069558084\n",
      "4532 : Training: loss:  0.07254836\n",
      "4533 : Training: loss:  0.032937746\n",
      "4534 : Training: loss:  0.06603995\n",
      "4535 : Training: loss:  0.03626044\n",
      "4536 : Training: loss:  0.058709163\n",
      "4537 : Training: loss:  0.067030795\n",
      "4538 : Training: loss:  0.08403206\n",
      "4539 : Training: loss:  0.06196733\n",
      "4540 : Training: loss:  0.028143428\n",
      "Validation: Loss:  0.07588122  Accuracy:  0.71153843\n",
      "4541 : Training: loss:  0.060028013\n",
      "4542 : Training: loss:  0.05565381\n",
      "4543 : Training: loss:  0.060294632\n",
      "4544 : Training: loss:  0.069204144\n",
      "4545 : Training: loss:  0.08374633\n",
      "4546 : Training: loss:  0.058258984\n",
      "4547 : Training: loss:  0.040431328\n",
      "4548 : Training: loss:  0.03650973\n",
      "4549 : Training: loss:  0.03883155\n",
      "4550 : Training: loss:  0.07055796\n",
      "4551 : Training: loss:  0.09785853\n",
      "4552 : Training: loss:  0.1253604\n",
      "4553 : Training: loss:  0.04236514\n",
      "4554 : Training: loss:  0.051204983\n",
      "4555 : Training: loss:  0.07468853\n",
      "4556 : Training: loss:  0.0847236\n",
      "4557 : Training: loss:  0.094231576\n",
      "4558 : Training: loss:  0.05752106\n",
      "4559 : Training: loss:  0.058050413\n",
      "4560 : Training: loss:  0.04047565\n",
      "Validation: Loss:  0.07565371  Accuracy:  0.7307692\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2608, 0.0253, 0.0759, 0.0177, 0.0082, 0.014...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.075654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.2372, 0.0496, 0.0088, 0.0056, 0.0397, 0.084...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1009, 0.0867, 0.0237, 0.0111, 0.0047, 0.049...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0307, 0.0251, 0.0081, 0.0008, 0.0121, 0.036...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1036, 0.0339, 0.0283, 0.009, 0.008, 0.0216,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0305, 0.0075, 0.1618, 0.2407, 0.0036, 0.008...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0158, 0.0047, 0.0826, 0.8066, 0.0075, 0.002...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.012, 0.0058, 0.0668, 0.7861, 0.0104, 0.0033...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0574, 0.0054, 0.0828, 0.1661, 0.0137, 0.016...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0996, 0.0424, 0.0007, 0.0009, 0.5249, 0.122...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0291, 0.2026, 0.0005, 0.0016, 0.4136, 0.212...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.018, 0.1313, 0.001, 0.0011, 0.0089, 0.2925,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0014, 0.0326, 0.0014, 0.0007, 0.0143, 0.012...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1e-04, 0.0063, 0.0055, 0.0037, 0.0015, 0.0012...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0253, 0.0253, 0.0032, 0.0015, 0.0857, 0.014...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.2074, 0.0572, 0.0029, 0.0002, 0.007, 0.0338...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0228, 0.0061, 0.0541, 0.0466, 0.0076, 0.011...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0065, 0.0045, 0.0082, 0.008, 0.0081, 0.0089...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0301, 0.0639, 0.0056, 0.0023, 0.0468, 0.178...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0999, 0.1175, 0.0059, 0.0011, 0.0689, 0.281...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0705, 0.0139, 0.0276, 0.0022, 0.0071, 0.024...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0035, 0.0133, 0.0042, 0.0093, 0.0024, 0.006...</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0317, 0.0102, 0.0088, 0.0009, 0.0112, 0.026...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0274, 0.1368, 0.0036, 0.0004, 0.039, 0.3885...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.114, 0.0146, 0.1158, 0.0342, 0.0084, 0.0264...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0762, 0.011, 0.0516, 0.0185, 0.0072, 0.0038...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0491, 0.0157, 0.0154, 0.0038, 0.0388, 0.009...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.013, 0.0084, 0.0075, 0.007, 0.0173, 0.0107,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0005, 0.0038, 0.0014, 0.0045, 0.0045, 0.001...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.001, 0.005, 0.0005, 0.0013, 0.0073, 0.0026,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0007, 0.0072, 0.0032, 1e-04, 0.0032, 0.0053...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0159, 0.0063, 0.0171, 0.014, 0.0076, 0.0122...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.009, 0.0046, 0.0086, 0.0233, 0.0035, 0.0038...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0562, 0.0243, 0.0257, 0.0193, 0.015, 0.0128...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0375, 0.038, 0.008, 0.0014, 0.0089, 0.041, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0861, 0.0088, 0.0299, 0.0218, 0.003, 0.0036...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0086, 0.0212, 0.0247, 0.0795, 0.0025, 0.025...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[1e-04, 0.0039, 0.001, 0.0074, 1e-04, 0.0056, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.006, 0.0144, 0.0191, 0.0185, 0.0578, 0.0041...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0116, 0.0119, 0.0271, 0.0345, 0.0403, 0.007...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0119, 0.0147, 0.0138, 0.0029, 0.0049, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0321, 0.0118, 0.0727, 0.1008, 0.0075, 0.007...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0029, 0.0126, 0.0056, 0.0581, 0.0006, 0.006...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0003, 0.0074, 0.0009, 0.0206, 1e-04, 0.0013...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0003, 0.0032, 0.0006, 0.0118, 1e-04, 0.0006...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0008, 0.0055, 0.003, 0.0584, 0.0002, 0.0014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0067, 0.0031, 0.0354, 0.0012, 0.0034, 0.004...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1719, 0.0116, 0.0781, 0.0231, 0.0046, 0.016...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0147, 0.0674, 0.0014, 0.0002, 0.0814, 0.037...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.044, 0.2584, 0.0005, 0.0003, 0.0124, 0.1917...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1082, 0.0161, 0.0075, 0.0012, 0.0039, 0.020...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0761, 0.0419, 0.0087, 0.0034, 0.0069, 0.105...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.2608, 0.0253, 0.0759, 0.0177, 0.0082, 0.014...               0   \n",
       "1   [0.2372, 0.0496, 0.0088, 0.0056, 0.0397, 0.084...               0   \n",
       "2   [0.1009, 0.0867, 0.0237, 0.0111, 0.0047, 0.049...               0   \n",
       "3   [0.0307, 0.0251, 0.0081, 0.0008, 0.0121, 0.036...               1   \n",
       "4   [0.1036, 0.0339, 0.0283, 0.009, 0.008, 0.0216,...               1   \n",
       "5   [0.0305, 0.0075, 0.1618, 0.2407, 0.0036, 0.008...               2   \n",
       "6   [0.0158, 0.0047, 0.0826, 0.8066, 0.0075, 0.002...               3   \n",
       "7   [0.012, 0.0058, 0.0668, 0.7861, 0.0104, 0.0033...               3   \n",
       "8   [0.0574, 0.0054, 0.0828, 0.1661, 0.0137, 0.016...               3   \n",
       "9   [0.0996, 0.0424, 0.0007, 0.0009, 0.5249, 0.122...               4   \n",
       "10  [0.0291, 0.2026, 0.0005, 0.0016, 0.4136, 0.212...               4   \n",
       "11  [0.018, 0.1313, 0.001, 0.0011, 0.0089, 0.2925,...               5   \n",
       "12  [0.0014, 0.0326, 0.0014, 0.0007, 0.0143, 0.012...               6   \n",
       "13  [1e-04, 0.0063, 0.0055, 0.0037, 0.0015, 0.0012...               7   \n",
       "14  [0.0253, 0.0253, 0.0032, 0.0015, 0.0857, 0.014...               8   \n",
       "15  [0.2074, 0.0572, 0.0029, 0.0002, 0.007, 0.0338...               8   \n",
       "16  [0.0228, 0.0061, 0.0541, 0.0466, 0.0076, 0.011...               9   \n",
       "17  [0.0065, 0.0045, 0.0082, 0.008, 0.0081, 0.0089...               9   \n",
       "18  [0.0301, 0.0639, 0.0056, 0.0023, 0.0468, 0.178...              10   \n",
       "19  [0.0999, 0.1175, 0.0059, 0.0011, 0.0689, 0.281...              10   \n",
       "20  [0.0705, 0.0139, 0.0276, 0.0022, 0.0071, 0.024...              11   \n",
       "21  [0.0035, 0.0133, 0.0042, 0.0093, 0.0024, 0.006...              11   \n",
       "22  [0.0317, 0.0102, 0.0088, 0.0009, 0.0112, 0.026...              12   \n",
       "23  [0.0274, 0.1368, 0.0036, 0.0004, 0.039, 0.3885...              13   \n",
       "24  [0.114, 0.0146, 0.1158, 0.0342, 0.0084, 0.0264...              13   \n",
       "25  [0.0762, 0.011, 0.0516, 0.0185, 0.0072, 0.0038...              14   \n",
       "26  [0.0491, 0.0157, 0.0154, 0.0038, 0.0388, 0.009...              14   \n",
       "27  [0.013, 0.0084, 0.0075, 0.007, 0.0173, 0.0107,...              15   \n",
       "28  [0.0005, 0.0038, 0.0014, 0.0045, 0.0045, 0.001...              15   \n",
       "29  [0.001, 0.005, 0.0005, 0.0013, 0.0073, 0.0026,...              15   \n",
       "30  [0.0007, 0.0072, 0.0032, 1e-04, 0.0032, 0.0053...              16   \n",
       "31  [0.0159, 0.0063, 0.0171, 0.014, 0.0076, 0.0122...              17   \n",
       "32  [0.009, 0.0046, 0.0086, 0.0233, 0.0035, 0.0038...              17   \n",
       "33  [0.0562, 0.0243, 0.0257, 0.0193, 0.015, 0.0128...              18   \n",
       "34  [0.0375, 0.038, 0.008, 0.0014, 0.0089, 0.041, ...              19   \n",
       "35  [0.0861, 0.0088, 0.0299, 0.0218, 0.003, 0.0036...              20   \n",
       "36  [0.0086, 0.0212, 0.0247, 0.0795, 0.0025, 0.025...              21   \n",
       "37  [1e-04, 0.0039, 0.001, 0.0074, 1e-04, 0.0056, ...              21   \n",
       "38  [0.006, 0.0144, 0.0191, 0.0185, 0.0578, 0.0041...              22   \n",
       "39  [0.0116, 0.0119, 0.0271, 0.0345, 0.0403, 0.007...              22   \n",
       "40  [0.0119, 0.0147, 0.0138, 0.0029, 0.0049, 0.001...              22   \n",
       "41  [0.0321, 0.0118, 0.0727, 0.1008, 0.0075, 0.007...              22   \n",
       "42  [0.0029, 0.0126, 0.0056, 0.0581, 0.0006, 0.006...              23   \n",
       "43  [0.0003, 0.0074, 0.0009, 0.0206, 1e-04, 0.0013...              23   \n",
       "44  [0.0003, 0.0032, 0.0006, 0.0118, 1e-04, 0.0006...              23   \n",
       "45  [0.0008, 0.0055, 0.003, 0.0584, 0.0002, 0.0014...              23   \n",
       "46  [0.0067, 0.0031, 0.0354, 0.0012, 0.0034, 0.004...              24   \n",
       "47  [0.1719, 0.0116, 0.0781, 0.0231, 0.0046, 0.016...              24   \n",
       "48  [0.0147, 0.0674, 0.0014, 0.0002, 0.0814, 0.037...              25   \n",
       "49  [0.044, 0.2584, 0.0005, 0.0003, 0.0124, 0.1917...              25   \n",
       "50  [0.1082, 0.0161, 0.0075, 0.0012, 0.0039, 0.020...              26   \n",
       "51  [0.0761, 0.0419, 0.0087, 0.0034, 0.0069, 0.105...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.730769  0.075654  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                 19       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                21       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                11       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                21       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                14       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4561 : Training: loss:  0.046253804\n",
      "4562 : Training: loss:  0.04746513\n",
      "4563 : Training: loss:  0.058810815\n",
      "4564 : Training: loss:  0.074347734\n",
      "4565 : Training: loss:  0.024142066\n",
      "4566 : Training: loss:  0.08353973\n",
      "4567 : Training: loss:  0.07025164\n",
      "4568 : Training: loss:  0.03797876\n",
      "4569 : Training: loss:  0.06114957\n",
      "4570 : Training: loss:  0.0391254\n",
      "4571 : Training: loss:  0.0526088\n",
      "4572 : Training: loss:  0.06088211\n",
      "4573 : Training: loss:  0.020606864\n",
      "4574 : Training: loss:  0.039268263\n",
      "4575 : Training: loss:  0.05833201\n",
      "4576 : Training: loss:  0.04283493\n",
      "4577 : Training: loss:  0.06922622\n",
      "4578 : Training: loss:  0.1084929\n",
      "4579 : Training: loss:  0.0777312\n",
      "4580 : Training: loss:  0.067570224\n",
      "Validation: Loss:  0.07525828  Accuracy:  0.7307692\n",
      "4581 : Training: loss:  0.055798355\n",
      "4582 : Training: loss:  0.09042284\n",
      "4583 : Training: loss:  0.071292244\n",
      "4584 : Training: loss:  0.058007725\n",
      "4585 : Training: loss:  0.07824966\n",
      "4586 : Training: loss:  0.0742671\n",
      "4587 : Training: loss:  0.04972965\n",
      "4588 : Training: loss:  0.056641243\n",
      "4589 : Training: loss:  0.052439388\n",
      "4590 : Training: loss:  0.033848852\n",
      "4591 : Training: loss:  0.05465718\n",
      "4592 : Training: loss:  0.090925686\n",
      "4593 : Training: loss:  0.055574797\n",
      "4594 : Training: loss:  0.019305896\n",
      "4595 : Training: loss:  0.044915535\n",
      "4596 : Training: loss:  0.052172408\n",
      "4597 : Training: loss:  0.036579613\n",
      "4598 : Training: loss:  0.041598663\n",
      "4599 : Training: loss:  0.057772573\n",
      "4600 : Training: loss:  0.033205483\n",
      "Validation: Loss:  0.07495839  Accuracy:  0.7307692\n",
      "4601 : Training: loss:  0.0942068\n",
      "4602 : Training: loss:  0.04625146\n",
      "4603 : Training: loss:  0.07971985\n",
      "4604 : Training: loss:  0.07148598\n",
      "4605 : Training: loss:  0.09295047\n",
      "4606 : Training: loss:  0.101934515\n",
      "4607 : Training: loss:  0.039402343\n",
      "4608 : Training: loss:  0.060835823\n",
      "4609 : Training: loss:  0.058573548\n",
      "4610 : Training: loss:  0.037885856\n",
      "4611 : Training: loss:  0.07817804\n",
      "4612 : Training: loss:  0.05503907\n",
      "4613 : Training: loss:  0.08239296\n",
      "4614 : Training: loss:  0.08383508\n",
      "4615 : Training: loss:  0.06760566\n",
      "4616 : Training: loss:  0.07212259\n",
      "4617 : Training: loss:  0.06697449\n",
      "4618 : Training: loss:  0.087825984\n",
      "4619 : Training: loss:  0.09139225\n",
      "4620 : Training: loss:  0.08381528\n",
      "Validation: Loss:  0.074724466  Accuracy:  0.7307692\n",
      "4621 : Training: loss:  0.06488242\n",
      "4622 : Training: loss:  0.05100229\n",
      "4623 : Training: loss:  0.052372392\n",
      "4624 : Training: loss:  0.065404944\n",
      "4625 : Training: loss:  0.06854504\n",
      "4626 : Training: loss:  0.07015563\n",
      "4627 : Training: loss:  0.06538528\n",
      "4628 : Training: loss:  0.04133978\n",
      "4629 : Training: loss:  0.032464165\n",
      "4630 : Training: loss:  0.022447191\n",
      "4631 : Training: loss:  0.051911764\n",
      "4632 : Training: loss:  0.08063703\n",
      "4633 : Training: loss:  0.088533916\n",
      "4634 : Training: loss:  0.081749186\n",
      "4635 : Training: loss:  0.06476614\n",
      "4636 : Training: loss:  0.045122884\n",
      "4637 : Training: loss:  0.060575705\n",
      "4638 : Training: loss:  0.04802703\n",
      "4639 : Training: loss:  0.07630493\n",
      "4640 : Training: loss:  0.054310992\n",
      "Validation: Loss:  0.07430366  Accuracy:  0.7307692\n",
      "4641 : Training: loss:  0.033032298\n",
      "4642 : Training: loss:  0.061502654\n",
      "4643 : Training: loss:  0.036784265\n",
      "4644 : Training: loss:  0.10814449\n",
      "4645 : Training: loss:  0.076625384\n",
      "4646 : Training: loss:  0.044963222\n",
      "4647 : Training: loss:  0.048335433\n",
      "4648 : Training: loss:  0.033907134\n",
      "4649 : Training: loss:  0.03651888\n",
      "4650 : Training: loss:  0.07304894\n",
      "4651 : Training: loss:  0.055174246\n",
      "4652 : Training: loss:  0.04501225\n",
      "4653 : Training: loss:  0.038092826\n",
      "4654 : Training: loss:  0.0421189\n",
      "4655 : Training: loss:  0.04866551\n",
      "4656 : Training: loss:  0.116179414\n",
      "4657 : Training: loss:  0.07526527\n",
      "4658 : Training: loss:  0.07924146\n",
      "4659 : Training: loss:  0.06915856\n",
      "4660 : Training: loss:  0.06438705\n",
      "Validation: Loss:  0.07437878  Accuracy:  0.71153843\n",
      "4661 : Training: loss:  0.050515912\n",
      "4662 : Training: loss:  0.059717752\n",
      "4663 : Training: loss:  0.07393752\n",
      "4664 : Training: loss:  0.058435503\n",
      "4665 : Training: loss:  0.03477077\n",
      "4666 : Training: loss:  0.06406731\n",
      "4667 : Training: loss:  0.1179828\n",
      "4668 : Training: loss:  0.024866674\n",
      "4669 : Training: loss:  0.03375529\n",
      "4670 : Training: loss:  0.052339483\n",
      "4671 : Training: loss:  0.10006571\n",
      "4672 : Training: loss:  0.05986469\n",
      "4673 : Training: loss:  0.0848048\n",
      "4674 : Training: loss:  0.053085092\n",
      "4675 : Training: loss:  0.041631173\n",
      "4676 : Training: loss:  0.08410147\n",
      "4677 : Training: loss:  0.07983003\n",
      "4678 : Training: loss:  0.09112767\n",
      "4679 : Training: loss:  0.065477304\n",
      "4680 : Training: loss:  0.047114223\n",
      "Validation: Loss:  0.074258976  Accuracy:  0.7307692\n",
      "4681 : Training: loss:  0.060776703\n",
      "4682 : Training: loss:  0.07157403\n",
      "4683 : Training: loss:  0.049918834\n",
      "4684 : Training: loss:  0.057319455\n",
      "4685 : Training: loss:  0.06968337\n",
      "4686 : Training: loss:  0.076071486\n",
      "4687 : Training: loss:  0.07411783\n",
      "4688 : Training: loss:  0.050770156\n",
      "4689 : Training: loss:  0.043168712\n",
      "4690 : Training: loss:  0.05217476\n",
      "4691 : Training: loss:  0.074769706\n",
      "4692 : Training: loss:  0.066081226\n",
      "4693 : Training: loss:  0.060370546\n",
      "4694 : Training: loss:  0.09988225\n",
      "4695 : Training: loss:  0.069842994\n",
      "4696 : Training: loss:  0.06230011\n",
      "4697 : Training: loss:  0.082020275\n",
      "4698 : Training: loss:  0.075242765\n",
      "4699 : Training: loss:  0.05017541\n",
      "4700 : Training: loss:  0.053926095\n",
      "Validation: Loss:  0.07412584  Accuracy:  0.7307692\n",
      "4701 : Training: loss:  0.04416568\n",
      "4702 : Training: loss:  0.06319951\n",
      "4703 : Training: loss:  0.053316884\n",
      "4704 : Training: loss:  0.096178144\n",
      "4705 : Training: loss:  0.06005704\n",
      "4706 : Training: loss:  0.08012954\n",
      "4707 : Training: loss:  0.0893498\n",
      "4708 : Training: loss:  0.10720339\n",
      "4709 : Training: loss:  0.06759227\n",
      "4710 : Training: loss:  0.0435064\n",
      "4711 : Training: loss:  0.044119786\n",
      "4712 : Training: loss:  0.062282458\n",
      "4713 : Training: loss:  0.06358569\n",
      "4714 : Training: loss:  0.06750144\n",
      "4715 : Training: loss:  0.040616624\n",
      "4716 : Training: loss:  0.031724144\n",
      "4717 : Training: loss:  0.037484393\n",
      "4718 : Training: loss:  0.059648838\n",
      "4719 : Training: loss:  0.078747205\n",
      "4720 : Training: loss:  0.07321181\n",
      "Validation: Loss:  0.073935606  Accuracy:  0.7307692\n",
      "4721 : Training: loss:  0.033411402\n",
      "4722 : Training: loss:  0.06841584\n",
      "4723 : Training: loss:  0.031138528\n",
      "4724 : Training: loss:  0.09420572\n",
      "4725 : Training: loss:  0.056252792\n",
      "4726 : Training: loss:  0.060303047\n",
      "4727 : Training: loss:  0.04213601\n",
      "4728 : Training: loss:  0.065244816\n",
      "4729 : Training: loss:  0.04513311\n",
      "4730 : Training: loss:  0.051038608\n",
      "4731 : Training: loss:  0.030898146\n",
      "4732 : Training: loss:  0.018195948\n",
      "4733 : Training: loss:  0.045535468\n",
      "4734 : Training: loss:  0.07494584\n",
      "4735 : Training: loss:  0.07094673\n",
      "4736 : Training: loss:  0.048369646\n",
      "4737 : Training: loss:  0.039352134\n",
      "4738 : Training: loss:  0.031387232\n",
      "4739 : Training: loss:  0.02497699\n",
      "4740 : Training: loss:  0.05830408\n",
      "Validation: Loss:  0.07365123  Accuracy:  0.71153843\n",
      "4741 : Training: loss:  0.058636907\n",
      "4742 : Training: loss:  0.09124185\n",
      "4743 : Training: loss:  0.054156337\n",
      "4744 : Training: loss:  0.025838722\n",
      "4745 : Training: loss:  0.06219727\n",
      "4746 : Training: loss:  0.021555929\n",
      "4747 : Training: loss:  0.063300356\n",
      "4748 : Training: loss:  0.053909786\n",
      "4749 : Training: loss:  0.04857965\n",
      "4750 : Training: loss:  0.061539404\n",
      "4751 : Training: loss:  0.07183278\n",
      "4752 : Training: loss:  0.038625427\n",
      "4753 : Training: loss:  0.041440584\n",
      "4754 : Training: loss:  0.030637596\n",
      "4755 : Training: loss:  0.06507603\n",
      "4756 : Training: loss:  0.07938021\n",
      "4757 : Training: loss:  0.07244228\n",
      "4758 : Training: loss:  0.05959823\n",
      "4759 : Training: loss:  0.055241037\n",
      "4760 : Training: loss:  0.01908773\n",
      "Validation: Loss:  0.073607884  Accuracy:  0.71153843\n",
      "4761 : Training: loss:  0.03319644\n",
      "4762 : Training: loss:  0.07235167\n",
      "4763 : Training: loss:  0.046578046\n",
      "4764 : Training: loss:  0.06602889\n",
      "4765 : Training: loss:  0.043472104\n",
      "4766 : Training: loss:  0.032806326\n",
      "4767 : Training: loss:  0.0413622\n",
      "4768 : Training: loss:  0.0650877\n",
      "4769 : Training: loss:  0.080035\n",
      "4770 : Training: loss:  0.046096507\n",
      "4771 : Training: loss:  0.076768294\n",
      "4772 : Training: loss:  0.048476424\n",
      "4773 : Training: loss:  0.029992545\n",
      "4774 : Training: loss:  0.08339988\n",
      "4775 : Training: loss:  0.09983031\n",
      "4776 : Training: loss:  0.05260005\n",
      "4777 : Training: loss:  0.0938675\n",
      "4778 : Training: loss:  0.09022712\n",
      "4779 : Training: loss:  0.054689\n",
      "4780 : Training: loss:  0.07368098\n",
      "Validation: Loss:  0.07341573  Accuracy:  0.71153843\n",
      "4781 : Training: loss:  0.04093004\n",
      "4782 : Training: loss:  0.06127282\n",
      "4783 : Training: loss:  0.050033804\n",
      "4784 : Training: loss:  0.028423082\n",
      "4785 : Training: loss:  0.052679066\n",
      "4786 : Training: loss:  0.034443796\n",
      "4787 : Training: loss:  0.070811145\n",
      "4788 : Training: loss:  0.09097788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4789 : Training: loss:  0.099109694\n",
      "4790 : Training: loss:  0.05008827\n",
      "4791 : Training: loss:  0.05206162\n",
      "4792 : Training: loss:  0.084974974\n",
      "4793 : Training: loss:  0.033223964\n",
      "4794 : Training: loss:  0.08848067\n",
      "4795 : Training: loss:  0.10910775\n",
      "4796 : Training: loss:  0.11258512\n",
      "4797 : Training: loss:  0.071293674\n",
      "4798 : Training: loss:  0.033040848\n",
      "4799 : Training: loss:  0.031582765\n",
      "4800 : Training: loss:  0.054163713\n",
      "Validation: Loss:  0.07350229  Accuracy:  0.7307692\n",
      "4801 : Training: loss:  0.054239564\n",
      "4802 : Training: loss:  0.03367445\n",
      "4803 : Training: loss:  0.09953674\n",
      "4804 : Training: loss:  0.052793417\n",
      "4805 : Training: loss:  0.067955464\n",
      "4806 : Training: loss:  0.06453724\n",
      "4807 : Training: loss:  0.07948914\n",
      "4808 : Training: loss:  0.05998748\n",
      "4809 : Training: loss:  0.057459984\n",
      "4810 : Training: loss:  0.05076874\n",
      "4811 : Training: loss:  0.076181315\n",
      "4812 : Training: loss:  0.04343125\n",
      "4813 : Training: loss:  0.09649082\n",
      "4814 : Training: loss:  0.04459827\n",
      "4815 : Training: loss:  0.037843063\n",
      "4816 : Training: loss:  0.049576465\n",
      "4817 : Training: loss:  0.052863594\n",
      "4818 : Training: loss:  0.05921808\n",
      "4819 : Training: loss:  0.046792053\n",
      "4820 : Training: loss:  0.11090957\n",
      "Validation: Loss:  0.073325634  Accuracy:  0.75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3327, 0.0392, 0.0864, 0.0181, 0.0078, 0.011...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.073326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.2898, 0.0855, 0.0093, 0.0054, 0.0408, 0.078...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1201, 0.135, 0.0239, 0.0105, 0.0039, 0.0441...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0303, 0.0408, 0.008, 0.0006, 0.0119, 0.0335...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1224, 0.0552, 0.0291, 0.0084, 0.0074, 0.018...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0359, 0.009, 0.2031, 0.2702, 0.0032, 0.0073...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0167, 0.0052, 0.0892, 0.8343, 0.0064, 0.001...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.012, 0.0062, 0.0676, 0.8071, 0.0084, 0.0022...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0602, 0.0061, 0.095, 0.1939, 0.0127, 0.0133...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1062, 0.0624, 0.0006, 0.0007, 0.5694, 0.106...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0258, 0.3058, 0.0004, 0.0013, 0.4255, 0.201...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.017, 0.1998, 0.0008, 0.0009, 0.008, 0.2955,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0011, 0.0429, 0.0011, 0.0005, 0.0136, 0.012...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1e-04, 0.0049, 0.0045, 0.0023, 0.0008, 0.0008...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0263, 0.0397, 0.0029, 0.0012, 0.1097, 0.012...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.2393, 0.0866, 0.0026, 0.0002, 0.007, 0.0253...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0248, 0.0077, 0.0711, 0.048, 0.007, 0.0107,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.006, 0.0055, 0.0093, 0.0071, 0.0074, 0.0082...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0276, 0.1004, 0.0048, 0.0018, 0.0462, 0.183...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0985, 0.1888, 0.0051, 0.0008, 0.0703, 0.286...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.079, 0.0187, 0.0278, 0.0018, 0.0066, 0.0216...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0033, 0.0185, 0.0032, 0.0071, 0.0019, 0.004...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0301, 0.0129, 0.0068, 0.0006, 0.0106, 0.021...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0249, 0.2145, 0.0029, 0.0003, 0.0375, 0.426...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1302, 0.0204, 0.1388, 0.0356, 0.0079, 0.023...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0866, 0.016, 0.0614, 0.0178, 0.0062, 0.003,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0512, 0.0233, 0.0147, 0.0031, 0.0441, 0.007...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0118, 0.0118, 0.0084, 0.0062, 0.0156, 0.009...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0003, 0.0037, 0.0012, 0.0034, 0.003, 0.0014...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0007, 0.0061, 0.0005, 0.001, 0.0056, 0.0021...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0004, 0.0072, 0.0027, 1e-04, 0.0022, 0.0039...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0169, 0.0079, 0.0184, 0.0129, 0.0071, 0.010...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0101, 0.006, 0.0084, 0.0217, 0.0033, 0.0032...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0673, 0.037, 0.0256, 0.018, 0.0152, 0.0108,...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0386, 0.0566, 0.007, 0.0011, 0.0086, 0.0365...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.1043, 0.0125, 0.032, 0.0215, 0.0027, 0.0025...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0088, 0.0277, 0.0249, 0.0763, 0.002, 0.0233...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 0.0037, 0.0008, 0.0046, 1e-04, 0.0045, 0...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0044, 0.0177, 0.0177, 0.0147, 0.0528, 0.002...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0103, 0.0154, 0.0265, 0.0313, 0.0406, 0.005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0118, 0.0216, 0.0136, 0.0023, 0.0043, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0346, 0.0154, 0.0821, 0.1047, 0.0067, 0.005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.003, 0.0157, 0.0051, 0.0501, 0.0005, 0.0045...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0003, 0.0081, 0.0007, 0.0132, 1e-04, 0.0008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0003, 0.0035, 0.0005, 0.0082, 1e-04, 0.0003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0007, 0.0063, 0.0025, 0.0448, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0051, 0.0028, 0.031, 0.0009, 0.0028, 0.0033...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2171, 0.0154, 0.0854, 0.0238, 0.0044, 0.013...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0117, 0.1038, 0.0012, 1e-04, 0.0805, 0.0326...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0421, 0.3872, 0.0004, 0.0002, 0.0111, 0.170...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1247, 0.0211, 0.0077, 0.0011, 0.0034, 0.016...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0831, 0.0582, 0.0078, 0.0029, 0.0064, 0.096...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3327, 0.0392, 0.0864, 0.0181, 0.0078, 0.011...               0   \n",
       "1   [0.2898, 0.0855, 0.0093, 0.0054, 0.0408, 0.078...               0   \n",
       "2   [0.1201, 0.135, 0.0239, 0.0105, 0.0039, 0.0441...               0   \n",
       "3   [0.0303, 0.0408, 0.008, 0.0006, 0.0119, 0.0335...               1   \n",
       "4   [0.1224, 0.0552, 0.0291, 0.0084, 0.0074, 0.018...               1   \n",
       "5   [0.0359, 0.009, 0.2031, 0.2702, 0.0032, 0.0073...               2   \n",
       "6   [0.0167, 0.0052, 0.0892, 0.8343, 0.0064, 0.001...               3   \n",
       "7   [0.012, 0.0062, 0.0676, 0.8071, 0.0084, 0.0022...               3   \n",
       "8   [0.0602, 0.0061, 0.095, 0.1939, 0.0127, 0.0133...               3   \n",
       "9   [0.1062, 0.0624, 0.0006, 0.0007, 0.5694, 0.106...               4   \n",
       "10  [0.0258, 0.3058, 0.0004, 0.0013, 0.4255, 0.201...               4   \n",
       "11  [0.017, 0.1998, 0.0008, 0.0009, 0.008, 0.2955,...               5   \n",
       "12  [0.0011, 0.0429, 0.0011, 0.0005, 0.0136, 0.012...               6   \n",
       "13  [1e-04, 0.0049, 0.0045, 0.0023, 0.0008, 0.0008...               7   \n",
       "14  [0.0263, 0.0397, 0.0029, 0.0012, 0.1097, 0.012...               8   \n",
       "15  [0.2393, 0.0866, 0.0026, 0.0002, 0.007, 0.0253...               8   \n",
       "16  [0.0248, 0.0077, 0.0711, 0.048, 0.007, 0.0107,...               9   \n",
       "17  [0.006, 0.0055, 0.0093, 0.0071, 0.0074, 0.0082...               9   \n",
       "18  [0.0276, 0.1004, 0.0048, 0.0018, 0.0462, 0.183...              10   \n",
       "19  [0.0985, 0.1888, 0.0051, 0.0008, 0.0703, 0.286...              10   \n",
       "20  [0.079, 0.0187, 0.0278, 0.0018, 0.0066, 0.0216...              11   \n",
       "21  [0.0033, 0.0185, 0.0032, 0.0071, 0.0019, 0.004...              11   \n",
       "22  [0.0301, 0.0129, 0.0068, 0.0006, 0.0106, 0.021...              12   \n",
       "23  [0.0249, 0.2145, 0.0029, 0.0003, 0.0375, 0.426...              13   \n",
       "24  [0.1302, 0.0204, 0.1388, 0.0356, 0.0079, 0.023...              13   \n",
       "25  [0.0866, 0.016, 0.0614, 0.0178, 0.0062, 0.003,...              14   \n",
       "26  [0.0512, 0.0233, 0.0147, 0.0031, 0.0441, 0.007...              14   \n",
       "27  [0.0118, 0.0118, 0.0084, 0.0062, 0.0156, 0.009...              15   \n",
       "28  [0.0003, 0.0037, 0.0012, 0.0034, 0.003, 0.0014...              15   \n",
       "29  [0.0007, 0.0061, 0.0005, 0.001, 0.0056, 0.0021...              15   \n",
       "30  [0.0004, 0.0072, 0.0027, 1e-04, 0.0022, 0.0039...              16   \n",
       "31  [0.0169, 0.0079, 0.0184, 0.0129, 0.0071, 0.010...              17   \n",
       "32  [0.0101, 0.006, 0.0084, 0.0217, 0.0033, 0.0032...              17   \n",
       "33  [0.0673, 0.037, 0.0256, 0.018, 0.0152, 0.0108,...              18   \n",
       "34  [0.0386, 0.0566, 0.007, 0.0011, 0.0086, 0.0365...              19   \n",
       "35  [0.1043, 0.0125, 0.032, 0.0215, 0.0027, 0.0025...              20   \n",
       "36  [0.0088, 0.0277, 0.0249, 0.0763, 0.002, 0.0233...              21   \n",
       "37  [0.0, 0.0037, 0.0008, 0.0046, 1e-04, 0.0045, 0...              21   \n",
       "38  [0.0044, 0.0177, 0.0177, 0.0147, 0.0528, 0.002...              22   \n",
       "39  [0.0103, 0.0154, 0.0265, 0.0313, 0.0406, 0.005...              22   \n",
       "40  [0.0118, 0.0216, 0.0136, 0.0023, 0.0043, 0.001...              22   \n",
       "41  [0.0346, 0.0154, 0.0821, 0.1047, 0.0067, 0.005...              22   \n",
       "42  [0.003, 0.0157, 0.0051, 0.0501, 0.0005, 0.0045...              23   \n",
       "43  [0.0003, 0.0081, 0.0007, 0.0132, 1e-04, 0.0008...              23   \n",
       "44  [0.0003, 0.0035, 0.0005, 0.0082, 1e-04, 0.0003...              23   \n",
       "45  [0.0007, 0.0063, 0.0025, 0.0448, 0.0002, 0.000...              23   \n",
       "46  [0.0051, 0.0028, 0.031, 0.0009, 0.0028, 0.0033...              24   \n",
       "47  [0.2171, 0.0154, 0.0854, 0.0238, 0.0044, 0.013...              24   \n",
       "48  [0.0117, 0.1038, 0.0012, 1e-04, 0.0805, 0.0326...              25   \n",
       "49  [0.0421, 0.3872, 0.0004, 0.0002, 0.0111, 0.170...              25   \n",
       "50  [0.1247, 0.0211, 0.0077, 0.0011, 0.0034, 0.016...              26   \n",
       "51  [0.0831, 0.0582, 0.0078, 0.0029, 0.0064, 0.096...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0      0.75  0.073326  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                  1       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  3       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                12       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                 0       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                23       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                14       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4821 : Training: loss:  0.058962777\n",
      "4822 : Training: loss:  0.05564429\n",
      "4823 : Training: loss:  0.0928634\n",
      "4824 : Training: loss:  0.041846585\n",
      "4825 : Training: loss:  0.04365267\n",
      "4826 : Training: loss:  0.05221683\n",
      "4827 : Training: loss:  0.07850295\n",
      "4828 : Training: loss:  0.08901009\n",
      "4829 : Training: loss:  0.09720367\n",
      "4830 : Training: loss:  0.044498112\n",
      "4831 : Training: loss:  0.031063898\n",
      "4832 : Training: loss:  0.052254062\n",
      "4833 : Training: loss:  0.07632909\n",
      "4834 : Training: loss:  0.07166876\n",
      "4835 : Training: loss:  0.08052567\n",
      "4836 : Training: loss:  0.06419294\n",
      "4837 : Training: loss:  0.09727903\n",
      "4838 : Training: loss:  0.01672745\n",
      "4839 : Training: loss:  0.052945495\n",
      "4840 : Training: loss:  0.050697222\n",
      "Validation: Loss:  0.07334624  Accuracy:  0.7307692\n",
      "4841 : Training: loss:  0.051460203\n",
      "4842 : Training: loss:  0.073503524\n",
      "4843 : Training: loss:  0.038129672\n",
      "4844 : Training: loss:  0.058423594\n",
      "4845 : Training: loss:  0.06658715\n",
      "4846 : Training: loss:  0.025007972\n",
      "4847 : Training: loss:  0.085471444\n",
      "4848 : Training: loss:  0.07435449\n",
      "4849 : Training: loss:  0.09040149\n",
      "4850 : Training: loss:  0.062181074\n",
      "4851 : Training: loss:  0.029387759\n",
      "4852 : Training: loss:  0.058452006\n",
      "4853 : Training: loss:  0.06426828\n",
      "4854 : Training: loss:  0.024453454\n",
      "4855 : Training: loss:  0.042264912\n",
      "4856 : Training: loss:  0.05484698\n",
      "4857 : Training: loss:  0.05529922\n",
      "4858 : Training: loss:  0.06819801\n",
      "4859 : Training: loss:  0.04951107\n",
      "4860 : Training: loss:  0.04016074\n",
      "Validation: Loss:  0.07308421  Accuracy:  0.71153843\n",
      "4861 : Training: loss:  0.044544622\n",
      "4862 : Training: loss:  0.08991269\n",
      "4863 : Training: loss:  0.101580136\n",
      "4864 : Training: loss:  0.07495863\n",
      "4865 : Training: loss:  0.052698515\n",
      "4866 : Training: loss:  0.05572188\n",
      "4867 : Training: loss:  0.063081\n",
      "4868 : Training: loss:  0.04205283\n",
      "4869 : Training: loss:  0.09316001\n",
      "4870 : Training: loss:  0.07931973\n",
      "4871 : Training: loss:  0.07543917\n",
      "4872 : Training: loss:  0.08784596\n",
      "4873 : Training: loss:  0.040621147\n",
      "4874 : Training: loss:  0.040399585\n",
      "4875 : Training: loss:  0.059334606\n",
      "4876 : Training: loss:  0.0756708\n",
      "4877 : Training: loss:  0.12233618\n",
      "4878 : Training: loss:  0.035593107\n",
      "4879 : Training: loss:  0.038835008\n",
      "4880 : Training: loss:  0.08278467\n",
      "Validation: Loss:  0.07294874  Accuracy:  0.6923077\n",
      "4881 : Training: loss:  0.09334903\n",
      "4882 : Training: loss:  0.040196672\n",
      "4883 : Training: loss:  0.063474216\n",
      "4884 : Training: loss:  0.0402056\n",
      "4885 : Training: loss:  0.06655722\n",
      "4886 : Training: loss:  0.07350878\n",
      "4887 : Training: loss:  0.03352676\n",
      "4888 : Training: loss:  0.061159305\n",
      "4889 : Training: loss:  0.11336609\n",
      "4890 : Training: loss:  0.052159976\n",
      "4891 : Training: loss:  0.06922579\n",
      "4892 : Training: loss:  0.04415493\n",
      "4893 : Training: loss:  0.070728235\n",
      "4894 : Training: loss:  0.104515046\n",
      "4895 : Training: loss:  0.04641139\n",
      "4896 : Training: loss:  0.084208824\n",
      "4897 : Training: loss:  0.07891892\n",
      "4898 : Training: loss:  0.058698464\n",
      "4899 : Training: loss:  0.036426622\n",
      "4900 : Training: loss:  0.030899953\n",
      "Validation: Loss:  0.07292318  Accuracy:  0.71153843\n",
      "4901 : Training: loss:  0.06687197\n",
      "4902 : Training: loss:  0.06318144\n",
      "4903 : Training: loss:  0.05506737\n",
      "4904 : Training: loss:  0.05132434\n",
      "4905 : Training: loss:  0.042728305\n",
      "4906 : Training: loss:  0.06574664\n",
      "4907 : Training: loss:  0.020279754\n",
      "4908 : Training: loss:  0.050565787\n",
      "4909 : Training: loss:  0.06510469\n",
      "4910 : Training: loss:  0.040331762\n",
      "4911 : Training: loss:  0.08868119\n",
      "4912 : Training: loss:  0.042156883\n",
      "4913 : Training: loss:  0.070123\n",
      "4914 : Training: loss:  0.07689494\n",
      "4915 : Training: loss:  0.07404698\n",
      "4916 : Training: loss:  0.03264547\n",
      "4917 : Training: loss:  0.019457063\n",
      "4918 : Training: loss:  0.094957635\n",
      "4919 : Training: loss:  0.027367277\n",
      "4920 : Training: loss:  0.075466216\n",
      "Validation: Loss:  0.07280612  Accuracy:  0.71153843\n",
      "4921 : Training: loss:  0.060241956\n",
      "4922 : Training: loss:  0.037939206\n",
      "4923 : Training: loss:  0.060478665\n",
      "4924 : Training: loss:  0.080854066\n",
      "4925 : Training: loss:  0.07098962\n",
      "4926 : Training: loss:  0.061411325\n",
      "4927 : Training: loss:  0.058844015\n",
      "4928 : Training: loss:  0.049287472\n",
      "4929 : Training: loss:  0.059170756\n",
      "4930 : Training: loss:  0.0465121\n",
      "4931 : Training: loss:  0.058543403\n",
      "4932 : Training: loss:  0.06611738\n",
      "4933 : Training: loss:  0.039532848\n",
      "4934 : Training: loss:  0.0407752\n",
      "4935 : Training: loss:  0.09289079\n",
      "4936 : Training: loss:  0.068099365\n",
      "4937 : Training: loss:  0.029677598\n",
      "4938 : Training: loss:  0.03973879\n",
      "4939 : Training: loss:  0.05922299\n",
      "4940 : Training: loss:  0.039341543\n",
      "Validation: Loss:  0.07269198  Accuracy:  0.7307692\n",
      "4941 : Training: loss:  0.023056472\n",
      "4942 : Training: loss:  0.064126424\n",
      "4943 : Training: loss:  0.089300014\n",
      "4944 : Training: loss:  0.08931888\n",
      "4945 : Training: loss:  0.04151159\n",
      "4946 : Training: loss:  0.061177686\n",
      "4947 : Training: loss:  0.055746548\n",
      "4948 : Training: loss:  0.07345236\n",
      "4949 : Training: loss:  0.0700288\n",
      "4950 : Training: loss:  0.036878355\n",
      "4951 : Training: loss:  0.06222581\n",
      "4952 : Training: loss:  0.05614621\n",
      "4953 : Training: loss:  0.058384735\n",
      "4954 : Training: loss:  0.08195591\n",
      "4955 : Training: loss:  0.06887575\n",
      "4956 : Training: loss:  0.05791592\n",
      "4957 : Training: loss:  0.08883773\n",
      "4958 : Training: loss:  0.037345357\n",
      "4959 : Training: loss:  0.06589938\n",
      "4960 : Training: loss:  0.052118786\n",
      "Validation: Loss:  0.07257553  Accuracy:  0.71153843\n",
      "4961 : Training: loss:  0.07382901\n",
      "4962 : Training: loss:  0.04822092\n",
      "4963 : Training: loss:  0.082779236\n",
      "4964 : Training: loss:  0.0756334\n",
      "4965 : Training: loss:  0.0290904\n",
      "4966 : Training: loss:  0.06832971\n",
      "4967 : Training: loss:  0.076815434\n",
      "4968 : Training: loss:  0.044855855\n",
      "4969 : Training: loss:  0.021711526\n",
      "4970 : Training: loss:  0.07278368\n",
      "4971 : Training: loss:  0.03228133\n",
      "4972 : Training: loss:  0.02705684\n",
      "4973 : Training: loss:  0.054091357\n",
      "4974 : Training: loss:  0.12134553\n",
      "4975 : Training: loss:  0.03326212\n",
      "4976 : Training: loss:  0.07340349\n",
      "4977 : Training: loss:  0.055567432\n",
      "4978 : Training: loss:  0.035511028\n",
      "4979 : Training: loss:  0.046727046\n",
      "4980 : Training: loss:  0.047835853\n",
      "Validation: Loss:  0.07241395  Accuracy:  0.75\n",
      "4981 : Training: loss:  0.06571069\n",
      "4982 : Training: loss:  0.07584979\n",
      "4983 : Training: loss:  0.044366505\n",
      "4984 : Training: loss:  0.043905232\n",
      "4985 : Training: loss:  0.08180231\n",
      "4986 : Training: loss:  0.06001602\n",
      "4987 : Training: loss:  0.0792468\n",
      "4988 : Training: loss:  0.06578659\n",
      "4989 : Training: loss:  0.06194351\n",
      "4990 : Training: loss:  0.064254895\n",
      "4991 : Training: loss:  0.04204528\n",
      "4992 : Training: loss:  0.08894659\n",
      "4993 : Training: loss:  0.06875387\n",
      "4994 : Training: loss:  0.036971863\n",
      "4995 : Training: loss:  0.05664482\n",
      "4996 : Training: loss:  0.1078518\n",
      "4997 : Training: loss:  0.0837132\n",
      "4998 : Training: loss:  0.026563242\n",
      "4999 : Training: loss:  0.07981283\n",
      "5000 : Training: loss:  0.075102895\n",
      "Validation: Loss:  0.07206735  Accuracy:  0.71153843\n",
      "5001 : Training: loss:  0.0688937\n",
      "5002 : Training: loss:  0.052396614\n",
      "5003 : Training: loss:  0.029244794\n",
      "5004 : Training: loss:  0.06756128\n",
      "5005 : Training: loss:  0.05964298\n",
      "5006 : Training: loss:  0.0761337\n",
      "5007 : Training: loss:  0.03928816\n",
      "5008 : Training: loss:  0.060274724\n",
      "5009 : Training: loss:  0.07304927\n",
      "5010 : Training: loss:  0.060681656\n",
      "5011 : Training: loss:  0.04134264\n",
      "5012 : Training: loss:  0.035691373\n",
      "5013 : Training: loss:  0.06911246\n",
      "5014 : Training: loss:  0.07820145\n",
      "5015 : Training: loss:  0.06933409\n",
      "5016 : Training: loss:  0.08987423\n",
      "5017 : Training: loss:  0.105015956\n",
      "5018 : Training: loss:  0.049471874\n",
      "5019 : Training: loss:  0.07328711\n",
      "5020 : Training: loss:  0.06254868\n",
      "Validation: Loss:  0.07193859  Accuracy:  0.7307692\n",
      "5021 : Training: loss:  0.04796359\n",
      "5022 : Training: loss:  0.067482255\n",
      "5023 : Training: loss:  0.040394466\n",
      "5024 : Training: loss:  0.06243455\n",
      "5025 : Training: loss:  0.0661551\n",
      "5026 : Training: loss:  0.10241256\n",
      "5027 : Training: loss:  0.07616326\n",
      "5028 : Training: loss:  0.013522974\n",
      "5029 : Training: loss:  0.07235902\n",
      "5030 : Training: loss:  0.043352526\n",
      "5031 : Training: loss:  0.036977205\n",
      "5032 : Training: loss:  0.07347062\n",
      "5033 : Training: loss:  0.031098004\n",
      "5034 : Training: loss:  0.047720846\n",
      "5035 : Training: loss:  0.07124806\n",
      "5036 : Training: loss:  0.0684597\n",
      "5037 : Training: loss:  0.047605295\n",
      "5038 : Training: loss:  0.052361507\n",
      "5039 : Training: loss:  0.06696135\n",
      "5040 : Training: loss:  0.08387108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss:  0.07174284  Accuracy:  0.7307692\n",
      "5041 : Training: loss:  0.06464983\n",
      "5042 : Training: loss:  0.06573474\n",
      "5043 : Training: loss:  0.057645068\n",
      "5044 : Training: loss:  0.02400403\n",
      "5045 : Training: loss:  0.031949155\n",
      "5046 : Training: loss:  0.070246354\n",
      "5047 : Training: loss:  0.021708833\n",
      "5048 : Training: loss:  0.05264228\n",
      "5049 : Training: loss:  0.077286\n",
      "5050 : Training: loss:  0.0456402\n",
      "5051 : Training: loss:  0.06146044\n",
      "5052 : Training: loss:  0.051017985\n",
      "5053 : Training: loss:  0.047349643\n",
      "5054 : Training: loss:  0.066212796\n",
      "5055 : Training: loss:  0.042973433\n",
      "5056 : Training: loss:  0.057620052\n",
      "5057 : Training: loss:  0.030565914\n",
      "5058 : Training: loss:  0.023808025\n",
      "5059 : Training: loss:  0.07486311\n",
      "5060 : Training: loss:  0.025224147\n",
      "Validation: Loss:  0.07150115  Accuracy:  0.7307692\n",
      "5061 : Training: loss:  0.02242995\n",
      "5062 : Training: loss:  0.08638675\n",
      "5063 : Training: loss:  0.041724022\n",
      "5064 : Training: loss:  0.05905485\n",
      "5065 : Training: loss:  0.03631776\n",
      "5066 : Training: loss:  0.04139387\n",
      "5067 : Training: loss:  0.102450036\n",
      "5068 : Training: loss:  0.060643673\n",
      "5069 : Training: loss:  0.05774737\n",
      "5070 : Training: loss:  0.09245934\n",
      "5071 : Training: loss:  0.015499925\n",
      "5072 : Training: loss:  0.09065466\n",
      "5073 : Training: loss:  0.051015045\n",
      "5074 : Training: loss:  0.0709\n",
      "5075 : Training: loss:  0.047389857\n",
      "5076 : Training: loss:  0.06703802\n",
      "5077 : Training: loss:  0.068421155\n",
      "5078 : Training: loss:  0.08745\n",
      "5079 : Training: loss:  0.0921265\n",
      "5080 : Training: loss:  0.057994265\n",
      "Validation: Loss:  0.07122342  Accuracy:  0.7307692\n",
      "5081 : Training: loss:  0.07812605\n",
      "5082 : Training: loss:  0.043921735\n",
      "5083 : Training: loss:  0.09345552\n",
      "5084 : Training: loss:  0.055528387\n",
      "5085 : Training: loss:  0.03862317\n",
      "5086 : Training: loss:  0.060756728\n",
      "5087 : Training: loss:  0.034738626\n",
      "5088 : Training: loss:  0.048484422\n",
      "5089 : Training: loss:  0.04502308\n",
      "5090 : Training: loss:  0.021562126\n",
      "5091 : Training: loss:  0.06366588\n",
      "5092 : Training: loss:  0.0925948\n",
      "5093 : Training: loss:  0.034193154\n",
      "5094 : Training: loss:  0.050260924\n",
      "5095 : Training: loss:  0.04455974\n",
      "5096 : Training: loss:  0.01896265\n",
      "5097 : Training: loss:  0.036932547\n",
      "5098 : Training: loss:  0.053889837\n",
      "5099 : Training: loss:  0.0666484\n",
      "5100 : Training: loss:  0.02661963\n",
      "Validation: Loss:  0.07132764  Accuracy:  0.7307692\n",
      "5101 : Training: loss:  0.09101507\n",
      "5102 : Training: loss:  0.060974922\n",
      "5103 : Training: loss:  0.078572035\n",
      "5104 : Training: loss:  0.044005387\n",
      "5105 : Training: loss:  0.065769784\n",
      "5106 : Training: loss:  0.031235585\n",
      "5107 : Training: loss:  0.05253875\n",
      "5108 : Training: loss:  0.059564844\n",
      "5109 : Training: loss:  0.060244553\n",
      "5110 : Training: loss:  0.11006122\n",
      "5111 : Training: loss:  0.08068424\n",
      "5112 : Training: loss:  0.046471562\n",
      "5113 : Training: loss:  0.032182936\n",
      "5114 : Training: loss:  0.087029465\n",
      "5115 : Training: loss:  0.06566655\n",
      "5116 : Training: loss:  0.08296586\n",
      "5117 : Training: loss:  0.042883307\n",
      "5118 : Training: loss:  0.06795591\n",
      "5119 : Training: loss:  0.062453628\n",
      "5120 : Training: loss:  0.07895889\n",
      "Validation: Loss:  0.07141209  Accuracy:  0.7307692\n",
      "5121 : Training: loss:  0.07212189\n",
      "5122 : Training: loss:  0.065209575\n",
      "5123 : Training: loss:  0.02333682\n",
      "5124 : Training: loss:  0.08420567\n",
      "5125 : Training: loss:  0.059654012\n",
      "5126 : Training: loss:  0.034039166\n",
      "5127 : Training: loss:  0.050396238\n",
      "5128 : Training: loss:  0.07769145\n",
      "5129 : Training: loss:  0.06674472\n",
      "5130 : Training: loss:  0.044533856\n",
      "5131 : Training: loss:  0.04055557\n",
      "5132 : Training: loss:  0.01095119\n",
      "5133 : Training: loss:  0.07057787\n",
      "5134 : Training: loss:  0.04571332\n",
      "5135 : Training: loss:  0.036545653\n",
      "5136 : Training: loss:  0.03361925\n",
      "5137 : Training: loss:  0.07332414\n",
      "5138 : Training: loss:  0.039381776\n",
      "5139 : Training: loss:  0.052654713\n",
      "5140 : Training: loss:  0.056642562\n",
      "Validation: Loss:  0.071574636  Accuracy:  0.7307692\n",
      "5141 : Training: loss:  0.06237151\n",
      "5142 : Training: loss:  0.056872964\n",
      "5143 : Training: loss:  0.08506678\n",
      "5144 : Training: loss:  0.029781183\n",
      "5145 : Training: loss:  0.06972342\n",
      "5146 : Training: loss:  0.06977871\n",
      "5147 : Training: loss:  0.030943442\n",
      "5148 : Training: loss:  0.06890463\n",
      "5149 : Training: loss:  0.026079668\n",
      "5150 : Training: loss:  0.041670848\n",
      "5151 : Training: loss:  0.0439965\n",
      "5152 : Training: loss:  0.047147352\n",
      "5153 : Training: loss:  0.037662182\n",
      "5154 : Training: loss:  0.07935129\n",
      "5155 : Training: loss:  0.04284759\n",
      "5156 : Training: loss:  0.06807225\n",
      "5157 : Training: loss:  0.070124514\n",
      "5158 : Training: loss:  0.051475864\n",
      "5159 : Training: loss:  0.03942899\n",
      "5160 : Training: loss:  0.057293627\n",
      "Validation: Loss:  0.07142491  Accuracy:  0.7307692\n",
      "5161 : Training: loss:  0.035506394\n",
      "5162 : Training: loss:  0.06859237\n",
      "5163 : Training: loss:  0.04534102\n",
      "5164 : Training: loss:  0.058993116\n",
      "5165 : Training: loss:  0.07907247\n",
      "5166 : Training: loss:  0.07909271\n",
      "5167 : Training: loss:  0.033458866\n",
      "5168 : Training: loss:  0.06271937\n",
      "5169 : Training: loss:  0.047536656\n",
      "5170 : Training: loss:  0.04873858\n",
      "5171 : Training: loss:  0.046659596\n",
      "5172 : Training: loss:  0.038109865\n",
      "5173 : Training: loss:  0.047857996\n",
      "5174 : Training: loss:  0.07841353\n",
      "5175 : Training: loss:  0.01525587\n",
      "5176 : Training: loss:  0.014965119\n",
      "5177 : Training: loss:  0.08922559\n",
      "5178 : Training: loss:  0.05360139\n",
      "5179 : Training: loss:  0.033776466\n",
      "5180 : Training: loss:  0.081007585\n",
      "Validation: Loss:  0.07124084  Accuracy:  0.7307692\n",
      "5181 : Training: loss:  0.09371836\n",
      "5182 : Training: loss:  0.018657569\n",
      "5183 : Training: loss:  0.044453684\n",
      "5184 : Training: loss:  0.085825816\n",
      "5185 : Training: loss:  0.049465105\n",
      "5186 : Training: loss:  0.10478994\n",
      "5187 : Training: loss:  0.043338086\n",
      "5188 : Training: loss:  0.02574461\n",
      "5189 : Training: loss:  0.06289518\n",
      "5190 : Training: loss:  0.07338789\n",
      "5191 : Training: loss:  0.06272263\n",
      "5192 : Training: loss:  0.021739157\n",
      "5193 : Training: loss:  0.050642803\n",
      "5194 : Training: loss:  0.07967161\n",
      "5195 : Training: loss:  0.0711999\n",
      "5196 : Training: loss:  0.0255862\n",
      "5197 : Training: loss:  0.085785486\n",
      "5198 : Training: loss:  0.023607185\n",
      "5199 : Training: loss:  0.08008253\n",
      "5200 : Training: loss:  0.051655825\n",
      "Validation: Loss:  0.07093611  Accuracy:  0.71153843\n",
      "5201 : Training: loss:  0.0893722\n",
      "5202 : Training: loss:  0.034197476\n",
      "5203 : Training: loss:  0.06587236\n",
      "5204 : Training: loss:  0.073564336\n",
      "5205 : Training: loss:  0.035887387\n",
      "5206 : Training: loss:  0.050643086\n",
      "5207 : Training: loss:  0.044132132\n",
      "5208 : Training: loss:  0.040208343\n",
      "5209 : Training: loss:  0.054925278\n",
      "5210 : Training: loss:  0.050035533\n",
      "5211 : Training: loss:  0.05215092\n",
      "5212 : Training: loss:  0.069673926\n",
      "5213 : Training: loss:  0.053700544\n",
      "5214 : Training: loss:  0.06079701\n",
      "5215 : Training: loss:  0.07408057\n",
      "5216 : Training: loss:  0.031764735\n",
      "5217 : Training: loss:  0.08962153\n",
      "5218 : Training: loss:  0.069962494\n",
      "5219 : Training: loss:  0.033734936\n",
      "5220 : Training: loss:  0.042110827\n",
      "Validation: Loss:  0.07095763  Accuracy:  0.71153843\n",
      "5221 : Training: loss:  0.039207358\n",
      "5222 : Training: loss:  0.013070531\n",
      "5223 : Training: loss:  0.058937773\n",
      "5224 : Training: loss:  0.08680102\n",
      "5225 : Training: loss:  0.049803663\n",
      "5226 : Training: loss:  0.070520446\n",
      "5227 : Training: loss:  0.045292813\n",
      "5228 : Training: loss:  0.08233901\n",
      "5229 : Training: loss:  0.080055684\n",
      "5230 : Training: loss:  0.039183952\n",
      "5231 : Training: loss:  0.040951565\n",
      "5232 : Training: loss:  0.030853728\n",
      "5233 : Training: loss:  0.022348782\n",
      "5234 : Training: loss:  0.07773236\n",
      "5235 : Training: loss:  0.05327601\n",
      "5236 : Training: loss:  0.042329784\n",
      "5237 : Training: loss:  0.060748056\n",
      "5238 : Training: loss:  0.045590606\n",
      "5239 : Training: loss:  0.09575964\n",
      "5240 : Training: loss:  0.061183088\n",
      "Validation: Loss:  0.070719555  Accuracy:  0.71153843\n",
      "5241 : Training: loss:  0.086311884\n",
      "5242 : Training: loss:  0.071191214\n",
      "5243 : Training: loss:  0.08783722\n",
      "5244 : Training: loss:  0.08488811\n",
      "5245 : Training: loss:  0.063052095\n",
      "5246 : Training: loss:  0.0969408\n",
      "5247 : Training: loss:  0.06342534\n",
      "5248 : Training: loss:  0.068897255\n",
      "5249 : Training: loss:  0.07491687\n",
      "5250 : Training: loss:  0.05509736\n",
      "5251 : Training: loss:  0.06850382\n",
      "5252 : Training: loss:  0.038988788\n",
      "5253 : Training: loss:  0.019350205\n",
      "5254 : Training: loss:  0.058454737\n",
      "5255 : Training: loss:  0.049352434\n",
      "5256 : Training: loss:  0.046053357\n",
      "5257 : Training: loss:  0.048327796\n",
      "5258 : Training: loss:  0.05775172\n",
      "5259 : Training: loss:  0.044555243\n",
      "5260 : Training: loss:  0.061734077\n",
      "Validation: Loss:  0.07063562  Accuracy:  0.71153843\n",
      "5261 : Training: loss:  0.049822833\n",
      "5262 : Training: loss:  0.0745675\n",
      "5263 : Training: loss:  0.08723443\n",
      "5264 : Training: loss:  0.04982681\n",
      "5265 : Training: loss:  0.037335295\n",
      "5266 : Training: loss:  0.075726934\n",
      "5267 : Training: loss:  0.024264563\n",
      "5268 : Training: loss:  0.060262397\n",
      "5269 : Training: loss:  0.069969445\n",
      "5270 : Training: loss:  0.052807383\n",
      "5271 : Training: loss:  0.04487253\n",
      "5272 : Training: loss:  0.04211084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5273 : Training: loss:  0.029047262\n",
      "5274 : Training: loss:  0.056939226\n",
      "5275 : Training: loss:  0.039984662\n",
      "5276 : Training: loss:  0.075495005\n",
      "5277 : Training: loss:  0.048624087\n",
      "5278 : Training: loss:  0.024989704\n",
      "5279 : Training: loss:  0.04390119\n",
      "5280 : Training: loss:  0.04809158\n",
      "Validation: Loss:  0.070677996  Accuracy:  0.71153843\n",
      "5281 : Training: loss:  0.04471967\n",
      "5282 : Training: loss:  0.0360665\n",
      "5283 : Training: loss:  0.07143167\n",
      "5284 : Training: loss:  0.031715054\n",
      "5285 : Training: loss:  0.017578812\n",
      "5286 : Training: loss:  0.048462197\n",
      "5287 : Training: loss:  0.0682814\n",
      "5288 : Training: loss:  0.033739977\n",
      "5289 : Training: loss:  0.06523114\n",
      "5290 : Training: loss:  0.036948767\n",
      "5291 : Training: loss:  0.09055382\n",
      "5292 : Training: loss:  0.061975002\n",
      "5293 : Training: loss:  0.04613257\n",
      "5294 : Training: loss:  0.032575015\n",
      "5295 : Training: loss:  0.022885494\n",
      "5296 : Training: loss:  0.060749974\n",
      "5297 : Training: loss:  0.054354113\n",
      "5298 : Training: loss:  0.080736406\n",
      "5299 : Training: loss:  0.051479053\n",
      "5300 : Training: loss:  0.04423518\n",
      "Validation: Loss:  0.07068099  Accuracy:  0.71153843\n",
      "5301 : Training: loss:  0.0424086\n",
      "5302 : Training: loss:  0.06111643\n",
      "5303 : Training: loss:  0.039229468\n",
      "5304 : Training: loss:  0.051095657\n",
      "5305 : Training: loss:  0.008016134\n",
      "5306 : Training: loss:  0.081902355\n",
      "5307 : Training: loss:  0.058901433\n",
      "5308 : Training: loss:  0.03830606\n",
      "5309 : Training: loss:  0.014802502\n",
      "5310 : Training: loss:  0.028062705\n",
      "5311 : Training: loss:  0.05473271\n",
      "5312 : Training: loss:  0.04139633\n",
      "5313 : Training: loss:  0.05285749\n",
      "5314 : Training: loss:  0.041157857\n",
      "5315 : Training: loss:  0.05771763\n",
      "5316 : Training: loss:  0.045327466\n",
      "5317 : Training: loss:  0.063126355\n",
      "5318 : Training: loss:  0.060475856\n",
      "5319 : Training: loss:  0.022468464\n",
      "5320 : Training: loss:  0.08071733\n",
      "Validation: Loss:  0.07052206  Accuracy:  0.71153843\n",
      "5321 : Training: loss:  0.06806529\n",
      "5322 : Training: loss:  0.055277847\n",
      "5323 : Training: loss:  0.039397765\n",
      "5324 : Training: loss:  0.040471967\n",
      "5325 : Training: loss:  0.044638045\n",
      "5326 : Training: loss:  0.034841876\n",
      "5327 : Training: loss:  0.04525736\n",
      "5328 : Training: loss:  0.06044697\n",
      "5329 : Training: loss:  0.028360777\n",
      "5330 : Training: loss:  0.07182924\n",
      "5331 : Training: loss:  0.021982882\n",
      "5332 : Training: loss:  0.070867024\n",
      "5333 : Training: loss:  0.038319163\n",
      "5334 : Training: loss:  0.06698824\n",
      "5335 : Training: loss:  0.05616527\n",
      "5336 : Training: loss:  0.05540564\n",
      "5337 : Training: loss:  0.05289051\n",
      "5338 : Training: loss:  0.07251422\n",
      "5339 : Training: loss:  0.060496457\n",
      "5340 : Training: loss:  0.07703525\n",
      "Validation: Loss:  0.0700555  Accuracy:  0.6923077\n",
      "5341 : Training: loss:  0.014813558\n",
      "5342 : Training: loss:  0.058354616\n",
      "5343 : Training: loss:  0.09985666\n",
      "5344 : Training: loss:  0.05264879\n",
      "5345 : Training: loss:  0.082390256\n",
      "5346 : Training: loss:  0.027295794\n",
      "5347 : Training: loss:  0.07082062\n",
      "5348 : Training: loss:  0.023029761\n",
      "5349 : Training: loss:  0.05297129\n",
      "5350 : Training: loss:  0.04544018\n",
      "5351 : Training: loss:  0.052946396\n",
      "5352 : Training: loss:  0.039409645\n",
      "5353 : Training: loss:  0.055291086\n",
      "5354 : Training: loss:  0.08882156\n",
      "5355 : Training: loss:  0.06311884\n",
      "5356 : Training: loss:  0.041257214\n",
      "5357 : Training: loss:  0.043624394\n",
      "5358 : Training: loss:  0.013153808\n",
      "5359 : Training: loss:  0.06584567\n",
      "5360 : Training: loss:  0.036719292\n",
      "Validation: Loss:  0.06992841  Accuracy:  0.71153843\n",
      "5361 : Training: loss:  0.049275342\n",
      "5362 : Training: loss:  0.022901032\n",
      "5363 : Training: loss:  0.038764026\n",
      "5364 : Training: loss:  0.094063334\n",
      "5365 : Training: loss:  0.065748535\n",
      "5366 : Training: loss:  0.023097882\n",
      "5367 : Training: loss:  0.030010378\n",
      "5368 : Training: loss:  0.028745012\n",
      "5369 : Training: loss:  0.068785675\n",
      "5370 : Training: loss:  0.071966246\n",
      "5371 : Training: loss:  0.050412893\n",
      "5372 : Training: loss:  0.0758616\n",
      "5373 : Training: loss:  0.03178541\n",
      "5374 : Training: loss:  0.074258834\n",
      "5375 : Training: loss:  0.10146121\n",
      "5376 : Training: loss:  0.01451908\n",
      "5377 : Training: loss:  0.08302905\n",
      "5378 : Training: loss:  0.09554934\n",
      "5379 : Training: loss:  0.034995478\n",
      "5380 : Training: loss:  0.028532041\n",
      "Validation: Loss:  0.069723696  Accuracy:  0.71153843\n",
      "5381 : Training: loss:  0.05138526\n",
      "5382 : Training: loss:  0.06316478\n",
      "5383 : Training: loss:  0.08316001\n",
      "5384 : Training: loss:  0.051037\n",
      "5385 : Training: loss:  0.033663556\n",
      "5386 : Training: loss:  0.12067429\n",
      "5387 : Training: loss:  0.06801103\n",
      "5388 : Training: loss:  0.03154989\n",
      "5389 : Training: loss:  0.060646955\n",
      "5390 : Training: loss:  0.057457227\n",
      "5391 : Training: loss:  0.08380396\n",
      "5392 : Training: loss:  0.07777387\n",
      "5393 : Training: loss:  0.05017433\n",
      "5394 : Training: loss:  0.05760915\n",
      "5395 : Training: loss:  0.08024188\n",
      "5396 : Training: loss:  0.037677974\n",
      "5397 : Training: loss:  0.029356886\n",
      "5398 : Training: loss:  0.057200518\n",
      "5399 : Training: loss:  0.06736669\n",
      "5400 : Training: loss:  0.039780345\n",
      "Validation: Loss:  0.06986237  Accuracy:  0.7307692\n",
      "5401 : Training: loss:  0.060300305\n",
      "5402 : Training: loss:  0.054607768\n",
      "5403 : Training: loss:  0.06182885\n",
      "5404 : Training: loss:  0.049721487\n",
      "5405 : Training: loss:  0.07899643\n",
      "5406 : Training: loss:  0.05511013\n",
      "5407 : Training: loss:  0.04069466\n",
      "5408 : Training: loss:  0.022646068\n",
      "5409 : Training: loss:  0.07401667\n",
      "5410 : Training: loss:  0.07025301\n",
      "5411 : Training: loss:  0.06589819\n",
      "5412 : Training: loss:  0.055785816\n",
      "5413 : Training: loss:  0.07179359\n",
      "5414 : Training: loss:  0.030128421\n",
      "5415 : Training: loss:  0.038296007\n",
      "5416 : Training: loss:  0.034904253\n",
      "5417 : Training: loss:  0.078081496\n",
      "5418 : Training: loss:  0.06325347\n",
      "5419 : Training: loss:  0.013153425\n",
      "5420 : Training: loss:  0.07025679\n",
      "Validation: Loss:  0.07014806  Accuracy:  0.7307692\n",
      "5421 : Training: loss:  0.04615834\n",
      "5422 : Training: loss:  0.013647113\n",
      "5423 : Training: loss:  0.028277447\n",
      "5424 : Training: loss:  0.062234163\n",
      "5425 : Training: loss:  0.056177035\n",
      "5426 : Training: loss:  0.058956522\n",
      "5427 : Training: loss:  0.050852753\n",
      "5428 : Training: loss:  0.036789697\n",
      "5429 : Training: loss:  0.08620311\n",
      "5430 : Training: loss:  0.060128514\n",
      "5431 : Training: loss:  0.051160455\n",
      "5432 : Training: loss:  0.06293596\n",
      "5433 : Training: loss:  0.06831426\n",
      "5434 : Training: loss:  0.0683248\n",
      "5435 : Training: loss:  0.057516888\n",
      "5436 : Training: loss:  0.03650864\n",
      "5437 : Training: loss:  0.04950626\n",
      "5438 : Training: loss:  0.04003187\n",
      "5439 : Training: loss:  0.06489591\n",
      "5440 : Training: loss:  0.05666135\n",
      "Validation: Loss:  0.070021346  Accuracy:  0.7307692\n",
      "5441 : Training: loss:  0.046503656\n",
      "5442 : Training: loss:  0.069131784\n",
      "5443 : Training: loss:  0.04916269\n",
      "5444 : Training: loss:  0.039050464\n",
      "5445 : Training: loss:  0.09184352\n",
      "5446 : Training: loss:  0.043928776\n",
      "5447 : Training: loss:  0.08023849\n",
      "5448 : Training: loss:  0.06737936\n",
      "5449 : Training: loss:  0.03525698\n",
      "5450 : Training: loss:  0.06465517\n",
      "5451 : Training: loss:  0.108456515\n",
      "5452 : Training: loss:  0.070614\n",
      "5453 : Training: loss:  0.004390661\n",
      "5454 : Training: loss:  0.04556164\n",
      "5455 : Training: loss:  0.06194876\n",
      "5456 : Training: loss:  0.07140606\n",
      "5457 : Training: loss:  0.03325719\n",
      "5458 : Training: loss:  0.078796804\n",
      "5459 : Training: loss:  0.033374004\n",
      "5460 : Training: loss:  0.05911861\n",
      "Validation: Loss:  0.06947761  Accuracy:  0.7307692\n",
      "5461 : Training: loss:  0.044627532\n",
      "5462 : Training: loss:  0.061372444\n",
      "5463 : Training: loss:  0.057730462\n",
      "5464 : Training: loss:  0.032427344\n",
      "5465 : Training: loss:  0.056845877\n",
      "5466 : Training: loss:  0.035351366\n",
      "5467 : Training: loss:  0.03831885\n",
      "5468 : Training: loss:  0.057138275\n",
      "5469 : Training: loss:  0.05794545\n",
      "5470 : Training: loss:  0.05130671\n",
      "5471 : Training: loss:  0.049073234\n",
      "5472 : Training: loss:  0.07329072\n",
      "5473 : Training: loss:  0.042167135\n",
      "5474 : Training: loss:  0.09152169\n",
      "5475 : Training: loss:  0.056944344\n",
      "5476 : Training: loss:  0.05734498\n",
      "5477 : Training: loss:  0.07200906\n",
      "5478 : Training: loss:  0.033151202\n",
      "5479 : Training: loss:  0.05182734\n",
      "5480 : Training: loss:  0.030501476\n",
      "Validation: Loss:  0.06903701  Accuracy:  0.71153843\n",
      "5481 : Training: loss:  0.0762709\n",
      "5482 : Training: loss:  0.022724377\n",
      "5483 : Training: loss:  0.057644077\n",
      "5484 : Training: loss:  0.062904865\n",
      "5485 : Training: loss:  0.0498172\n",
      "5486 : Training: loss:  0.09847268\n",
      "5487 : Training: loss:  0.08947457\n",
      "5488 : Training: loss:  0.024905903\n",
      "5489 : Training: loss:  0.0724285\n",
      "5490 : Training: loss:  0.047627166\n",
      "5491 : Training: loss:  0.05893132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5492 : Training: loss:  0.04879419\n",
      "5493 : Training: loss:  0.023732625\n",
      "5494 : Training: loss:  0.055537004\n",
      "5495 : Training: loss:  0.062154833\n",
      "5496 : Training: loss:  0.010830189\n",
      "5497 : Training: loss:  0.07700879\n",
      "5498 : Training: loss:  0.034116883\n",
      "5499 : Training: loss:  0.04126508\n",
      "5500 : Training: loss:  0.039266452\n",
      "Validation: Loss:  0.06901934  Accuracy:  0.7307692\n",
      "5501 : Training: loss:  0.03964319\n",
      "5502 : Training: loss:  0.08053016\n",
      "5503 : Training: loss:  0.024927935\n",
      "5504 : Training: loss:  0.0687954\n",
      "5505 : Training: loss:  0.01691152\n",
      "5506 : Training: loss:  0.042444315\n",
      "5507 : Training: loss:  0.033285227\n",
      "5508 : Training: loss:  0.09564014\n",
      "5509 : Training: loss:  0.034959674\n",
      "5510 : Training: loss:  0.051610198\n",
      "5511 : Training: loss:  0.083786935\n",
      "5512 : Training: loss:  0.039726656\n",
      "5513 : Training: loss:  0.06419967\n",
      "5514 : Training: loss:  0.10023628\n",
      "5515 : Training: loss:  0.04676139\n",
      "5516 : Training: loss:  0.06807705\n",
      "5517 : Training: loss:  0.09369927\n",
      "5518 : Training: loss:  0.020686802\n",
      "5519 : Training: loss:  0.08279816\n",
      "5520 : Training: loss:  0.036937173\n",
      "Validation: Loss:  0.068972364  Accuracy:  0.7307692\n",
      "5521 : Training: loss:  0.06565531\n",
      "5522 : Training: loss:  0.091191776\n",
      "5523 : Training: loss:  0.082408465\n",
      "5524 : Training: loss:  0.035372\n",
      "5525 : Training: loss:  0.038105875\n",
      "5526 : Training: loss:  0.028265135\n",
      "5527 : Training: loss:  0.04584318\n",
      "5528 : Training: loss:  0.09005688\n",
      "5529 : Training: loss:  0.025716864\n",
      "5530 : Training: loss:  0.04442566\n",
      "5531 : Training: loss:  0.047095012\n",
      "5532 : Training: loss:  0.04297863\n",
      "5533 : Training: loss:  0.044883423\n",
      "5534 : Training: loss:  0.05084345\n",
      "5535 : Training: loss:  0.039912622\n",
      "5536 : Training: loss:  0.025626697\n",
      "5537 : Training: loss:  0.041595384\n",
      "5538 : Training: loss:  0.04628214\n",
      "5539 : Training: loss:  0.06386641\n",
      "5540 : Training: loss:  0.02959871\n",
      "Validation: Loss:  0.06854956  Accuracy:  0.75\n",
      "5541 : Training: loss:  0.08985353\n",
      "5542 : Training: loss:  0.042298175\n",
      "5543 : Training: loss:  0.061715398\n",
      "5544 : Training: loss:  0.026097143\n",
      "5545 : Training: loss:  0.08090183\n",
      "5546 : Training: loss:  0.038865834\n",
      "5547 : Training: loss:  0.060770176\n",
      "5548 : Training: loss:  0.035963506\n",
      "5549 : Training: loss:  0.048513167\n",
      "5550 : Training: loss:  0.04740667\n",
      "5551 : Training: loss:  0.07522125\n",
      "5552 : Training: loss:  0.026060415\n",
      "5553 : Training: loss:  0.045565143\n",
      "5554 : Training: loss:  0.02573173\n",
      "5555 : Training: loss:  0.03557344\n",
      "5556 : Training: loss:  0.08418031\n",
      "5557 : Training: loss:  0.042747863\n",
      "5558 : Training: loss:  0.017639717\n",
      "5559 : Training: loss:  0.03588778\n",
      "5560 : Training: loss:  0.07404915\n",
      "Validation: Loss:  0.06832128  Accuracy:  0.75\n",
      "5561 : Training: loss:  0.047690626\n",
      "5562 : Training: loss:  0.08424636\n",
      "5563 : Training: loss:  0.0109309815\n",
      "5564 : Training: loss:  0.03050417\n",
      "5565 : Training: loss:  0.057337314\n",
      "5566 : Training: loss:  0.027636483\n",
      "5567 : Training: loss:  0.043285534\n",
      "5568 : Training: loss:  0.055887073\n",
      "5569 : Training: loss:  0.046764918\n",
      "5570 : Training: loss:  0.04333922\n",
      "5571 : Training: loss:  0.02923248\n",
      "5572 : Training: loss:  0.042844057\n",
      "5573 : Training: loss:  0.075199306\n",
      "5574 : Training: loss:  0.0344944\n",
      "5575 : Training: loss:  0.029966565\n",
      "5576 : Training: loss:  0.03311784\n",
      "5577 : Training: loss:  0.02159686\n",
      "5578 : Training: loss:  0.055351514\n",
      "5579 : Training: loss:  0.063673384\n",
      "5580 : Training: loss:  0.028266314\n",
      "Validation: Loss:  0.06848094  Accuracy:  0.75\n",
      "5581 : Training: loss:  0.04386716\n",
      "5582 : Training: loss:  0.04876986\n",
      "5583 : Training: loss:  0.06002584\n",
      "5584 : Training: loss:  0.06454896\n",
      "5585 : Training: loss:  0.07282747\n",
      "5586 : Training: loss:  0.058209315\n",
      "5587 : Training: loss:  0.10989634\n",
      "5588 : Training: loss:  0.078276604\n",
      "5589 : Training: loss:  0.032542378\n",
      "5590 : Training: loss:  0.043672338\n",
      "5591 : Training: loss:  0.020466208\n",
      "5592 : Training: loss:  0.037444092\n",
      "5593 : Training: loss:  0.03271696\n",
      "5594 : Training: loss:  0.07318956\n",
      "5595 : Training: loss:  0.040624578\n",
      "5596 : Training: loss:  0.048618436\n",
      "5597 : Training: loss:  0.03211312\n",
      "5598 : Training: loss:  0.037169084\n",
      "5599 : Training: loss:  0.049512956\n",
      "5600 : Training: loss:  0.051814742\n",
      "Validation: Loss:  0.06840094  Accuracy:  0.75\n",
      "5601 : Training: loss:  0.05715993\n",
      "5602 : Training: loss:  0.07497211\n",
      "5603 : Training: loss:  0.035643913\n",
      "5604 : Training: loss:  0.03224109\n",
      "5605 : Training: loss:  0.059089214\n",
      "5606 : Training: loss:  0.06230012\n",
      "5607 : Training: loss:  0.07471929\n",
      "5608 : Training: loss:  0.024480894\n",
      "5609 : Training: loss:  0.03290669\n",
      "5610 : Training: loss:  0.042528782\n",
      "5611 : Training: loss:  0.035001323\n",
      "5612 : Training: loss:  0.0327885\n",
      "5613 : Training: loss:  0.044545874\n",
      "5614 : Training: loss:  0.049114794\n",
      "5615 : Training: loss:  0.06703371\n",
      "5616 : Training: loss:  0.06274301\n",
      "5617 : Training: loss:  0.08384523\n",
      "5618 : Training: loss:  0.05786988\n",
      "5619 : Training: loss:  0.044058483\n",
      "5620 : Training: loss:  0.03252161\n",
      "Validation: Loss:  0.068434685  Accuracy:  0.75\n",
      "5621 : Training: loss:  0.0515613\n",
      "5622 : Training: loss:  0.035543147\n",
      "5623 : Training: loss:  0.06333059\n",
      "5624 : Training: loss:  0.045875575\n",
      "5625 : Training: loss:  0.025922466\n",
      "5626 : Training: loss:  0.059186667\n",
      "5627 : Training: loss:  0.016308842\n",
      "5628 : Training: loss:  0.039269462\n",
      "5629 : Training: loss:  0.06344456\n",
      "5630 : Training: loss:  0.049916286\n",
      "5631 : Training: loss:  0.014460866\n",
      "5632 : Training: loss:  0.07747596\n",
      "5633 : Training: loss:  0.039868955\n",
      "5634 : Training: loss:  0.057761878\n",
      "5635 : Training: loss:  0.082583055\n",
      "5636 : Training: loss:  0.050724976\n",
      "5637 : Training: loss:  0.0648523\n",
      "5638 : Training: loss:  0.044632886\n",
      "5639 : Training: loss:  0.042847987\n",
      "5640 : Training: loss:  0.009564705\n",
      "Validation: Loss:  0.06809997  Accuracy:  0.71153843\n",
      "5641 : Training: loss:  0.07191508\n",
      "5642 : Training: loss:  0.06485907\n",
      "5643 : Training: loss:  0.05089959\n",
      "5644 : Training: loss:  0.037582796\n",
      "5645 : Training: loss:  0.040331814\n",
      "5646 : Training: loss:  0.042688243\n",
      "5647 : Training: loss:  0.021098541\n",
      "5648 : Training: loss:  0.058071796\n",
      "5649 : Training: loss:  0.037724804\n",
      "5650 : Training: loss:  0.057821695\n",
      "5651 : Training: loss:  0.044198677\n",
      "5652 : Training: loss:  0.048317604\n",
      "5653 : Training: loss:  0.036710795\n",
      "5654 : Training: loss:  0.073350765\n",
      "5655 : Training: loss:  0.05987743\n",
      "5656 : Training: loss:  0.10876447\n",
      "5657 : Training: loss:  0.052726503\n",
      "5658 : Training: loss:  0.047702245\n",
      "5659 : Training: loss:  0.055397965\n",
      "5660 : Training: loss:  0.038760778\n",
      "Validation: Loss:  0.067929424  Accuracy:  0.7692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3442, 0.0329, 0.0692, 0.011, 0.004, 0.0049,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.067929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.3849, 0.077, 0.0068, 0.0038, 0.0308, 0.0514...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0981, 0.1328, 0.0144, 0.0064, 0.0016, 0.024...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0193, 0.0328, 0.0064, 0.0003, 0.0081, 0.025...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1016, 0.0521, 0.021, 0.0049, 0.004, 0.009, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0231, 0.0049, 0.2418, 0.2403, 0.0016, 0.004...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0131, 0.0029, 0.0986, 0.8835, 0.0038, 0.000...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0094, 0.0032, 0.0724, 0.8565, 0.0049, 0.001...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0485, 0.0031, 0.1079, 0.2373, 0.0091, 0.008...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1375, 0.0431, 0.0003, 0.0006, 0.6447, 0.084...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0271, 0.2767, 0.0002, 0.0011, 0.4732, 0.185...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0128, 0.1723, 0.0004, 0.0006, 0.0053, 0.321...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0005, 0.0381, 0.0006, 0.0002, 0.0147, 0.010...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0, 0.0026, 0.0039, 0.0014, 0.0004, 0.0003, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0234, 0.0273, 0.0025, 0.0009, 0.1152, 0.007...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.2875, 0.0603, 0.0016, 1e-04, 0.0041, 0.0094...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0154, 0.0047, 0.0707, 0.0262, 0.0045, 0.007...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0032, 0.0036, 0.0063, 0.0033, 0.0056, 0.005...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0209, 0.0996, 0.0029, 0.0009, 0.0433, 0.196...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.1052, 0.1946, 0.0035, 0.0005, 0.0778, 0.298...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0583, 0.0123, 0.021, 0.0008, 0.0037, 0.0141...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0018, 0.0142, 0.0017, 0.004, 0.001, 0.0024,...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0187, 0.0073, 0.0038, 0.0003, 0.0063, 0.015...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0183, 0.2071, 0.0017, 1e-04, 0.0315, 0.4935...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1102, 0.0147, 0.1491, 0.0231, 0.0047, 0.015...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0645, 0.0127, 0.0499, 0.0098, 0.0033, 0.001...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0341, 0.0197, 0.0097, 0.0015, 0.0422, 0.003...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.008, 0.0093, 0.007, 0.0031, 0.0119, 0.0061,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[1e-04, 0.0022, 0.0007, 0.002, 0.0019, 0.0006,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0004, 0.0043, 0.0003, 0.0005, 0.0041, 0.000...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[1e-04, 0.003, 0.0017, 0.0, 0.001, 0.0014, 1e-...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0089, 0.0043, 0.0124, 0.0066, 0.0048, 0.007...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.005, 0.0033, 0.0046, 0.0127, 0.002, 0.0016,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0542, 0.0301, 0.0188, 0.0105, 0.0096, 0.005...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.027, 0.0476, 0.0046, 0.0006, 0.0052, 0.0262...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0968, 0.0086, 0.0288, 0.0181, 0.0015, 0.001...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0043, 0.0212, 0.0168, 0.047, 0.0011, 0.0216...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 0.0013, 0.0005, 0.0024, 0.0, 0.0035, 0.0...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0017, 0.0112, 0.0139, 0.0078, 0.0364, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0053, 0.0097, 0.0241, 0.021, 0.0304, 0.0027...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0065, 0.0181, 0.0094, 0.0011, 0.0021, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0218, 0.0098, 0.0772, 0.0777, 0.0035, 0.002...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0017, 0.0115, 0.0033, 0.0362, 0.0002, 0.003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[1e-04, 0.006, 0.0004, 0.009, 0.0, 0.0004, 0.0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0002, 0.0023, 0.0003, 0.0055, 0.0, 0.0002, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0003, 0.0042, 0.0015, 0.0286, 1e-04, 0.0004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0017, 0.0011, 0.0245, 0.0003, 0.0017, 0.001...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2299, 0.0096, 0.08, 0.019, 0.0024, 0.0069, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0061, 0.0892, 0.0007, 1e-04, 0.0742, 0.0174...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0347, 0.3856, 1e-04, 1e-04, 0.0064, 0.1039,...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1166, 0.0107, 0.0055, 0.0006, 0.0014, 0.006...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0659, 0.0365, 0.0047, 0.0017, 0.0032, 0.067...</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3442, 0.0329, 0.0692, 0.011, 0.004, 0.0049,...               0   \n",
       "1   [0.3849, 0.077, 0.0068, 0.0038, 0.0308, 0.0514...               0   \n",
       "2   [0.0981, 0.1328, 0.0144, 0.0064, 0.0016, 0.024...               0   \n",
       "3   [0.0193, 0.0328, 0.0064, 0.0003, 0.0081, 0.025...               1   \n",
       "4   [0.1016, 0.0521, 0.021, 0.0049, 0.004, 0.009, ...               1   \n",
       "5   [0.0231, 0.0049, 0.2418, 0.2403, 0.0016, 0.004...               2   \n",
       "6   [0.0131, 0.0029, 0.0986, 0.8835, 0.0038, 0.000...               3   \n",
       "7   [0.0094, 0.0032, 0.0724, 0.8565, 0.0049, 0.001...               3   \n",
       "8   [0.0485, 0.0031, 0.1079, 0.2373, 0.0091, 0.008...               3   \n",
       "9   [0.1375, 0.0431, 0.0003, 0.0006, 0.6447, 0.084...               4   \n",
       "10  [0.0271, 0.2767, 0.0002, 0.0011, 0.4732, 0.185...               4   \n",
       "11  [0.0128, 0.1723, 0.0004, 0.0006, 0.0053, 0.321...               5   \n",
       "12  [0.0005, 0.0381, 0.0006, 0.0002, 0.0147, 0.010...               6   \n",
       "13  [0.0, 0.0026, 0.0039, 0.0014, 0.0004, 0.0003, ...               7   \n",
       "14  [0.0234, 0.0273, 0.0025, 0.0009, 0.1152, 0.007...               8   \n",
       "15  [0.2875, 0.0603, 0.0016, 1e-04, 0.0041, 0.0094...               8   \n",
       "16  [0.0154, 0.0047, 0.0707, 0.0262, 0.0045, 0.007...               9   \n",
       "17  [0.0032, 0.0036, 0.0063, 0.0033, 0.0056, 0.005...               9   \n",
       "18  [0.0209, 0.0996, 0.0029, 0.0009, 0.0433, 0.196...              10   \n",
       "19  [0.1052, 0.1946, 0.0035, 0.0005, 0.0778, 0.298...              10   \n",
       "20  [0.0583, 0.0123, 0.021, 0.0008, 0.0037, 0.0141...              11   \n",
       "21  [0.0018, 0.0142, 0.0017, 0.004, 0.001, 0.0024,...              11   \n",
       "22  [0.0187, 0.0073, 0.0038, 0.0003, 0.0063, 0.015...              12   \n",
       "23  [0.0183, 0.2071, 0.0017, 1e-04, 0.0315, 0.4935...              13   \n",
       "24  [0.1102, 0.0147, 0.1491, 0.0231, 0.0047, 0.015...              13   \n",
       "25  [0.0645, 0.0127, 0.0499, 0.0098, 0.0033, 0.001...              14   \n",
       "26  [0.0341, 0.0197, 0.0097, 0.0015, 0.0422, 0.003...              14   \n",
       "27  [0.008, 0.0093, 0.007, 0.0031, 0.0119, 0.0061,...              15   \n",
       "28  [1e-04, 0.0022, 0.0007, 0.002, 0.0019, 0.0006,...              15   \n",
       "29  [0.0004, 0.0043, 0.0003, 0.0005, 0.0041, 0.000...              15   \n",
       "30  [1e-04, 0.003, 0.0017, 0.0, 0.001, 0.0014, 1e-...              16   \n",
       "31  [0.0089, 0.0043, 0.0124, 0.0066, 0.0048, 0.007...              17   \n",
       "32  [0.005, 0.0033, 0.0046, 0.0127, 0.002, 0.0016,...              17   \n",
       "33  [0.0542, 0.0301, 0.0188, 0.0105, 0.0096, 0.005...              18   \n",
       "34  [0.027, 0.0476, 0.0046, 0.0006, 0.0052, 0.0262...              19   \n",
       "35  [0.0968, 0.0086, 0.0288, 0.0181, 0.0015, 0.001...              20   \n",
       "36  [0.0043, 0.0212, 0.0168, 0.047, 0.0011, 0.0216...              21   \n",
       "37  [0.0, 0.0013, 0.0005, 0.0024, 0.0, 0.0035, 0.0...              21   \n",
       "38  [0.0017, 0.0112, 0.0139, 0.0078, 0.0364, 0.001...              22   \n",
       "39  [0.0053, 0.0097, 0.0241, 0.021, 0.0304, 0.0027...              22   \n",
       "40  [0.0065, 0.0181, 0.0094, 0.0011, 0.0021, 0.000...              22   \n",
       "41  [0.0218, 0.0098, 0.0772, 0.0777, 0.0035, 0.002...              22   \n",
       "42  [0.0017, 0.0115, 0.0033, 0.0362, 0.0002, 0.003...              23   \n",
       "43  [1e-04, 0.006, 0.0004, 0.009, 0.0, 0.0004, 0.0...              23   \n",
       "44  [0.0002, 0.0023, 0.0003, 0.0055, 0.0, 0.0002, ...              23   \n",
       "45  [0.0003, 0.0042, 0.0015, 0.0286, 1e-04, 0.0004...              23   \n",
       "46  [0.0017, 0.0011, 0.0245, 0.0003, 0.0017, 0.001...              24   \n",
       "47  [0.2299, 0.0096, 0.08, 0.019, 0.0024, 0.0069, ...              24   \n",
       "48  [0.0061, 0.0892, 0.0007, 1e-04, 0.0742, 0.0174...              25   \n",
       "49  [0.0347, 0.3856, 1e-04, 1e-04, 0.0064, 0.1039,...              25   \n",
       "50  [0.1166, 0.0107, 0.0055, 0.0006, 0.0014, 0.006...              26   \n",
       "51  [0.0659, 0.0365, 0.0047, 0.0017, 0.0032, 0.067...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.769231  0.067929  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                 10       NaN       NaN  \n",
       "4                  0       NaN       NaN  \n",
       "5                  2       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                11       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                15       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                11       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                21       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                11       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                19       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5661 : Training: loss:  0.04091918\n",
      "5662 : Training: loss:  0.096640676\n",
      "5663 : Training: loss:  0.05973032\n",
      "5664 : Training: loss:  0.075257234\n",
      "5665 : Training: loss:  0.08085729\n",
      "5666 : Training: loss:  0.021777304\n",
      "5667 : Training: loss:  0.046460096\n",
      "5668 : Training: loss:  0.043101557\n",
      "5669 : Training: loss:  0.059656613\n",
      "5670 : Training: loss:  0.056321554\n",
      "5671 : Training: loss:  0.038564343\n",
      "5672 : Training: loss:  0.07387529\n",
      "5673 : Training: loss:  0.086130925\n",
      "5674 : Training: loss:  0.013522822\n",
      "5675 : Training: loss:  0.07110754\n",
      "5676 : Training: loss:  0.042725023\n",
      "5677 : Training: loss:  0.067632236\n",
      "5678 : Training: loss:  0.048351757\n",
      "5679 : Training: loss:  0.04636808\n",
      "5680 : Training: loss:  0.07411965\n",
      "Validation: Loss:  0.06771836  Accuracy:  0.75\n",
      "5681 : Training: loss:  0.061597005\n",
      "5682 : Training: loss:  0.07768778\n",
      "5683 : Training: loss:  0.09191791\n",
      "5684 : Training: loss:  0.09530419\n",
      "5685 : Training: loss:  0.05325041\n",
      "5686 : Training: loss:  0.033446666\n",
      "5687 : Training: loss:  0.060516946\n",
      "5688 : Training: loss:  0.06763723\n",
      "5689 : Training: loss:  0.08320514\n",
      "5690 : Training: loss:  0.09534959\n",
      "5691 : Training: loss:  0.07294475\n",
      "5692 : Training: loss:  0.055468794\n",
      "5693 : Training: loss:  0.017606081\n",
      "5694 : Training: loss:  0.03127508\n",
      "5695 : Training: loss:  0.029112445\n",
      "5696 : Training: loss:  0.082931995\n",
      "5697 : Training: loss:  0.057180073\n",
      "5698 : Training: loss:  0.070486054\n",
      "5699 : Training: loss:  0.01842984\n",
      "5700 : Training: loss:  0.0851098\n",
      "Validation: Loss:  0.06777684  Accuracy:  0.7692308\n",
      "5701 : Training: loss:  0.057186835\n",
      "5702 : Training: loss:  0.035534747\n",
      "5703 : Training: loss:  0.034335822\n",
      "5704 : Training: loss:  0.067783035\n",
      "5705 : Training: loss:  0.05840176\n",
      "5706 : Training: loss:  0.03400851\n",
      "5707 : Training: loss:  0.06977863\n",
      "5708 : Training: loss:  0.08040694\n",
      "5709 : Training: loss:  0.05226788\n",
      "5710 : Training: loss:  0.014955174\n",
      "5711 : Training: loss:  0.041707635\n",
      "5712 : Training: loss:  0.04758109\n",
      "5713 : Training: loss:  0.02667836\n",
      "5714 : Training: loss:  0.041935377\n",
      "5715 : Training: loss:  0.030745141\n",
      "5716 : Training: loss:  0.02334085\n",
      "5717 : Training: loss:  0.05934927\n",
      "5718 : Training: loss:  0.062326495\n",
      "5719 : Training: loss:  0.046879705\n",
      "5720 : Training: loss:  0.052000795\n",
      "Validation: Loss:  0.068113215  Accuracy:  0.75\n",
      "5721 : Training: loss:  0.068030104\n",
      "5722 : Training: loss:  0.045210402\n",
      "5723 : Training: loss:  0.06269221\n",
      "5724 : Training: loss:  0.07454987\n",
      "5725 : Training: loss:  0.025342437\n",
      "5726 : Training: loss:  0.03539702\n",
      "5727 : Training: loss:  0.06543602\n",
      "5728 : Training: loss:  0.049362198\n",
      "5729 : Training: loss:  0.055170488\n",
      "5730 : Training: loss:  0.033345073\n",
      "5731 : Training: loss:  0.041044947\n",
      "5732 : Training: loss:  0.039035637\n",
      "5733 : Training: loss:  0.09846063\n",
      "5734 : Training: loss:  0.025393298\n",
      "5735 : Training: loss:  0.046763472\n",
      "5736 : Training: loss:  0.102262914\n",
      "5737 : Training: loss:  0.015812483\n",
      "5738 : Training: loss:  0.0691782\n",
      "5739 : Training: loss:  0.024642877\n",
      "5740 : Training: loss:  0.065295205\n",
      "Validation: Loss:  0.06764453  Accuracy:  0.7307692\n",
      "5741 : Training: loss:  0.05165577\n",
      "5742 : Training: loss:  0.020473171\n",
      "5743 : Training: loss:  0.03469404\n",
      "5744 : Training: loss:  0.058245532\n",
      "5745 : Training: loss:  0.09888757\n",
      "5746 : Training: loss:  0.028400853\n",
      "5747 : Training: loss:  0.0363538\n",
      "5748 : Training: loss:  0.05398555\n",
      "5749 : Training: loss:  0.06410169\n",
      "5750 : Training: loss:  0.027370937\n",
      "5751 : Training: loss:  0.03990197\n",
      "5752 : Training: loss:  0.05295276\n",
      "5753 : Training: loss:  0.051313546\n",
      "5754 : Training: loss:  0.0392442\n",
      "5755 : Training: loss:  0.0314923\n",
      "5756 : Training: loss:  0.067291215\n",
      "5757 : Training: loss:  0.040449277\n",
      "5758 : Training: loss:  0.04875228\n",
      "5759 : Training: loss:  0.040621635\n",
      "5760 : Training: loss:  0.06357802\n",
      "Validation: Loss:  0.06761146  Accuracy:  0.7307692\n",
      "5761 : Training: loss:  0.062886305\n",
      "5762 : Training: loss:  0.036704887\n",
      "5763 : Training: loss:  0.054505117\n",
      "5764 : Training: loss:  0.08553631\n",
      "5765 : Training: loss:  0.051766332\n",
      "5766 : Training: loss:  0.07550569\n",
      "5767 : Training: loss:  0.063045986\n",
      "5768 : Training: loss:  0.034838896\n",
      "5769 : Training: loss:  0.08274517\n",
      "5770 : Training: loss:  0.015125494\n",
      "5771 : Training: loss:  0.045413222\n",
      "5772 : Training: loss:  0.049455732\n",
      "5773 : Training: loss:  0.08870567\n",
      "5774 : Training: loss:  0.06821869\n",
      "5775 : Training: loss:  0.08225905\n",
      "5776 : Training: loss:  0.044079464\n",
      "5777 : Training: loss:  0.07417352\n",
      "5778 : Training: loss:  0.047684386\n",
      "5779 : Training: loss:  0.034620516\n",
      "5780 : Training: loss:  0.07332279\n",
      "Validation: Loss:  0.06758607  Accuracy:  0.7307692\n",
      "5781 : Training: loss:  0.012703241\n",
      "5782 : Training: loss:  0.05246052\n",
      "5783 : Training: loss:  0.058179848\n",
      "5784 : Training: loss:  0.049103852\n",
      "5785 : Training: loss:  0.029529033\n",
      "5786 : Training: loss:  0.047546983\n",
      "5787 : Training: loss:  0.06268687\n",
      "5788 : Training: loss:  0.03434232\n",
      "5789 : Training: loss:  0.080467656\n",
      "5790 : Training: loss:  0.039112616\n",
      "5791 : Training: loss:  0.059745017\n",
      "5792 : Training: loss:  0.050599195\n",
      "5793 : Training: loss:  0.059117787\n",
      "5794 : Training: loss:  0.019750139\n",
      "5795 : Training: loss:  0.055234257\n",
      "5796 : Training: loss:  0.0378919\n",
      "5797 : Training: loss:  0.05795969\n",
      "5798 : Training: loss:  0.07351708\n",
      "5799 : Training: loss:  0.06343414\n",
      "5800 : Training: loss:  0.032524843\n",
      "Validation: Loss:  0.06755559  Accuracy:  0.7307692\n",
      "5801 : Training: loss:  0.04340382\n",
      "5802 : Training: loss:  0.035219453\n",
      "5803 : Training: loss:  0.08109599\n",
      "5804 : Training: loss:  0.026911436\n",
      "5805 : Training: loss:  0.044081617\n",
      "5806 : Training: loss:  0.05573962\n",
      "5807 : Training: loss:  0.04195828\n",
      "5808 : Training: loss:  0.09567796\n",
      "5809 : Training: loss:  0.059040204\n",
      "5810 : Training: loss:  0.05368655\n",
      "5811 : Training: loss:  0.04956091\n",
      "5812 : Training: loss:  0.04467537\n",
      "5813 : Training: loss:  0.088794805\n",
      "5814 : Training: loss:  0.06854738\n",
      "5815 : Training: loss:  0.027276771\n",
      "5816 : Training: loss:  0.037754413\n",
      "5817 : Training: loss:  0.068972655\n",
      "5818 : Training: loss:  0.036490418\n",
      "5819 : Training: loss:  0.09430486\n",
      "5820 : Training: loss:  0.020990022\n",
      "Validation: Loss:  0.06744198  Accuracy:  0.7307692\n",
      "5821 : Training: loss:  0.04412699\n",
      "5822 : Training: loss:  0.036217943\n",
      "5823 : Training: loss:  0.01883728\n",
      "5824 : Training: loss:  0.101022385\n",
      "5825 : Training: loss:  0.041978464\n",
      "5826 : Training: loss:  0.047182817\n",
      "5827 : Training: loss:  0.038121745\n",
      "5828 : Training: loss:  0.05032167\n",
      "5829 : Training: loss:  0.05273921\n",
      "5830 : Training: loss:  0.04954081\n",
      "5831 : Training: loss:  0.02942724\n",
      "5832 : Training: loss:  0.06742773\n",
      "5833 : Training: loss:  0.055179555\n",
      "5834 : Training: loss:  0.031144384\n",
      "5835 : Training: loss:  0.026547607\n",
      "5836 : Training: loss:  0.028658668\n",
      "5837 : Training: loss:  0.039959136\n",
      "5838 : Training: loss:  0.028632198\n",
      "5839 : Training: loss:  0.064499155\n",
      "5840 : Training: loss:  0.02262921\n",
      "Validation: Loss:  0.06715722  Accuracy:  0.7307692\n",
      "5841 : Training: loss:  0.023264315\n",
      "5842 : Training: loss:  0.059732907\n",
      "5843 : Training: loss:  0.050374072\n",
      "5844 : Training: loss:  0.06718894\n",
      "5845 : Training: loss:  0.046523422\n",
      "5846 : Training: loss:  0.06107531\n",
      "5847 : Training: loss:  0.068672076\n",
      "5848 : Training: loss:  0.077894695\n",
      "5849 : Training: loss:  0.041617293\n",
      "5850 : Training: loss:  0.027772844\n",
      "5851 : Training: loss:  0.04836534\n",
      "5852 : Training: loss:  0.04671264\n",
      "5853 : Training: loss:  0.042283993\n",
      "5854 : Training: loss:  0.05172515\n",
      "5855 : Training: loss:  0.04669338\n",
      "5856 : Training: loss:  0.025033234\n",
      "5857 : Training: loss:  0.060138185\n",
      "5858 : Training: loss:  0.04443572\n",
      "5859 : Training: loss:  0.054535937\n",
      "5860 : Training: loss:  0.056503374\n",
      "Validation: Loss:  0.06722596  Accuracy:  0.7307692\n",
      "5861 : Training: loss:  0.03406984\n",
      "5862 : Training: loss:  0.035196345\n",
      "5863 : Training: loss:  0.059212383\n",
      "5864 : Training: loss:  0.058650214\n",
      "5865 : Training: loss:  0.041032836\n",
      "5866 : Training: loss:  0.05291898\n",
      "5867 : Training: loss:  0.05933206\n",
      "5868 : Training: loss:  0.03809234\n",
      "5869 : Training: loss:  0.07866288\n",
      "5870 : Training: loss:  0.063440144\n",
      "5871 : Training: loss:  0.01755223\n",
      "5872 : Training: loss:  0.026901385\n",
      "5873 : Training: loss:  0.044994816\n",
      "5874 : Training: loss:  0.074613914\n",
      "5875 : Training: loss:  0.06537086\n",
      "5876 : Training: loss:  0.046651743\n",
      "5877 : Training: loss:  0.0549215\n",
      "5878 : Training: loss:  0.08466891\n",
      "5879 : Training: loss:  0.081134364\n",
      "5880 : Training: loss:  0.07021908\n",
      "Validation: Loss:  0.067290165  Accuracy:  0.7692308\n",
      "5881 : Training: loss:  0.030084595\n",
      "5882 : Training: loss:  0.07365935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5883 : Training: loss:  0.05729641\n",
      "5884 : Training: loss:  0.050686665\n",
      "5885 : Training: loss:  0.018744817\n",
      "5886 : Training: loss:  0.021481948\n",
      "5887 : Training: loss:  0.029983677\n",
      "5888 : Training: loss:  0.074284494\n",
      "5889 : Training: loss:  0.08983206\n",
      "5890 : Training: loss:  0.064896174\n",
      "5891 : Training: loss:  0.05423941\n",
      "5892 : Training: loss:  0.10254725\n",
      "5893 : Training: loss:  0.01101235\n",
      "5894 : Training: loss:  0.07687491\n",
      "5895 : Training: loss:  0.059188217\n",
      "5896 : Training: loss:  0.08558273\n",
      "5897 : Training: loss:  0.037395023\n",
      "5898 : Training: loss:  0.061658975\n",
      "5899 : Training: loss:  0.008374035\n",
      "5900 : Training: loss:  0.041075546\n",
      "Validation: Loss:  0.067206666  Accuracy:  0.7692308\n",
      "5901 : Training: loss:  0.031272516\n",
      "5902 : Training: loss:  0.039146665\n",
      "5903 : Training: loss:  0.04385752\n",
      "5904 : Training: loss:  0.032288425\n",
      "5905 : Training: loss:  0.052279368\n",
      "5906 : Training: loss:  0.05468981\n",
      "5907 : Training: loss:  0.04962541\n",
      "5908 : Training: loss:  0.055867657\n",
      "5909 : Training: loss:  0.07043909\n",
      "5910 : Training: loss:  0.04149546\n",
      "5911 : Training: loss:  0.03208402\n",
      "5912 : Training: loss:  0.06523747\n",
      "5913 : Training: loss:  0.02323902\n",
      "5914 : Training: loss:  0.035881646\n",
      "5915 : Training: loss:  0.03461401\n",
      "5916 : Training: loss:  0.049982093\n",
      "5917 : Training: loss:  0.03699293\n",
      "5918 : Training: loss:  0.036650877\n",
      "5919 : Training: loss:  0.056512754\n",
      "5920 : Training: loss:  0.042912442\n",
      "Validation: Loss:  0.06695685  Accuracy:  0.7692308\n",
      "5921 : Training: loss:  0.016201876\n",
      "5922 : Training: loss:  0.073209986\n",
      "5923 : Training: loss:  0.022718452\n",
      "5924 : Training: loss:  0.04424982\n",
      "5925 : Training: loss:  0.093428425\n",
      "5926 : Training: loss:  0.05714551\n",
      "5927 : Training: loss:  0.026796069\n",
      "5928 : Training: loss:  0.044308398\n",
      "5929 : Training: loss:  0.037640613\n",
      "5930 : Training: loss:  0.059103236\n",
      "5931 : Training: loss:  0.026844053\n",
      "5932 : Training: loss:  0.055563323\n",
      "5933 : Training: loss:  0.06872322\n",
      "5934 : Training: loss:  0.027057027\n",
      "5935 : Training: loss:  0.09077866\n",
      "5936 : Training: loss:  0.020882575\n",
      "5937 : Training: loss:  0.042895988\n",
      "5938 : Training: loss:  0.054131858\n",
      "5939 : Training: loss:  0.045386035\n",
      "5940 : Training: loss:  0.06524405\n",
      "Validation: Loss:  0.066792116  Accuracy:  0.7692308\n",
      "5941 : Training: loss:  0.04659352\n",
      "5942 : Training: loss:  0.07422306\n",
      "5943 : Training: loss:  0.048470877\n",
      "5944 : Training: loss:  0.045383964\n",
      "5945 : Training: loss:  0.045186114\n",
      "5946 : Training: loss:  0.03921107\n",
      "5947 : Training: loss:  0.06868317\n",
      "5948 : Training: loss:  0.01916873\n",
      "5949 : Training: loss:  0.014674461\n",
      "5950 : Training: loss:  0.050720606\n",
      "5951 : Training: loss:  0.012294471\n",
      "5952 : Training: loss:  0.052703682\n",
      "5953 : Training: loss:  0.06173509\n",
      "5954 : Training: loss:  0.05247702\n",
      "5955 : Training: loss:  0.04618782\n",
      "5956 : Training: loss:  0.057096142\n",
      "5957 : Training: loss:  0.046583366\n",
      "5958 : Training: loss:  0.031261664\n",
      "5959 : Training: loss:  0.06085689\n",
      "5960 : Training: loss:  0.059711687\n",
      "Validation: Loss:  0.06649584  Accuracy:  0.7692308\n",
      "5961 : Training: loss:  0.09323758\n",
      "5962 : Training: loss:  0.06819842\n",
      "5963 : Training: loss:  0.076398805\n",
      "5964 : Training: loss:  0.056467596\n",
      "5965 : Training: loss:  0.076510094\n",
      "5966 : Training: loss:  0.05642109\n",
      "5967 : Training: loss:  0.069894835\n",
      "5968 : Training: loss:  0.061728314\n",
      "5969 : Training: loss:  0.034889273\n",
      "5970 : Training: loss:  0.07260321\n",
      "5971 : Training: loss:  0.09563902\n",
      "5972 : Training: loss:  0.049713034\n",
      "5973 : Training: loss:  0.049639065\n",
      "5974 : Training: loss:  0.05500677\n",
      "5975 : Training: loss:  0.049714796\n",
      "5976 : Training: loss:  0.026264152\n",
      "5977 : Training: loss:  0.057728186\n",
      "5978 : Training: loss:  0.045639057\n",
      "5979 : Training: loss:  0.065784365\n",
      "5980 : Training: loss:  0.019476775\n",
      "Validation: Loss:  0.066182606  Accuracy:  0.75\n",
      "5981 : Training: loss:  0.04659194\n",
      "5982 : Training: loss:  0.05897208\n",
      "5983 : Training: loss:  0.08441309\n",
      "5984 : Training: loss:  0.0601058\n",
      "5985 : Training: loss:  0.08003068\n",
      "5986 : Training: loss:  0.055000763\n",
      "5987 : Training: loss:  0.053205773\n",
      "5988 : Training: loss:  0.047096554\n",
      "5989 : Training: loss:  0.06930587\n",
      "5990 : Training: loss:  0.042768903\n",
      "5991 : Training: loss:  0.0758665\n",
      "5992 : Training: loss:  0.05576472\n",
      "5993 : Training: loss:  0.044910315\n",
      "5994 : Training: loss:  0.048393037\n",
      "5995 : Training: loss:  0.037769776\n",
      "5996 : Training: loss:  0.064014286\n",
      "5997 : Training: loss:  0.017928766\n",
      "5998 : Training: loss:  0.036242217\n",
      "5999 : Training: loss:  0.058601115\n",
      "6000 : Training: loss:  0.06814225\n",
      "Validation: Loss:  0.06602357  Accuracy:  0.7692308\n",
      "6001 : Training: loss:  0.056214277\n",
      "6002 : Training: loss:  0.052007794\n",
      "6003 : Training: loss:  0.08773059\n",
      "6004 : Training: loss:  0.06436599\n",
      "6005 : Training: loss:  0.05984848\n",
      "6006 : Training: loss:  0.03352799\n",
      "6007 : Training: loss:  0.016710207\n",
      "6008 : Training: loss:  0.06607648\n",
      "6009 : Training: loss:  0.05993186\n",
      "6010 : Training: loss:  0.015427234\n",
      "6011 : Training: loss:  0.04481137\n",
      "6012 : Training: loss:  0.06499818\n",
      "6013 : Training: loss:  0.03832904\n",
      "6014 : Training: loss:  0.034714133\n",
      "6015 : Training: loss:  0.042379636\n",
      "6016 : Training: loss:  0.072337754\n",
      "6017 : Training: loss:  0.039020915\n",
      "6018 : Training: loss:  0.021561345\n",
      "6019 : Training: loss:  0.017534435\n",
      "6020 : Training: loss:  0.05139698\n",
      "Validation: Loss:  0.06622801  Accuracy:  0.75\n",
      "6021 : Training: loss:  0.06471919\n",
      "6022 : Training: loss:  0.01317653\n",
      "6023 : Training: loss:  0.04686739\n",
      "6024 : Training: loss:  0.01933955\n",
      "6025 : Training: loss:  0.057797246\n",
      "6026 : Training: loss:  0.06271914\n",
      "6027 : Training: loss:  0.03948595\n",
      "6028 : Training: loss:  0.08026076\n",
      "6029 : Training: loss:  0.073348634\n",
      "6030 : Training: loss:  0.050744034\n",
      "6031 : Training: loss:  0.059367996\n",
      "6032 : Training: loss:  0.04622016\n",
      "6033 : Training: loss:  0.065349475\n",
      "6034 : Training: loss:  0.06560086\n",
      "6035 : Training: loss:  0.044202846\n",
      "6036 : Training: loss:  0.013048934\n",
      "6037 : Training: loss:  0.018708397\n",
      "6038 : Training: loss:  0.035740726\n",
      "6039 : Training: loss:  0.054998573\n",
      "6040 : Training: loss:  0.014862497\n",
      "Validation: Loss:  0.06597753  Accuracy:  0.7307692\n",
      "6041 : Training: loss:  0.039075304\n",
      "6042 : Training: loss:  0.049780127\n",
      "6043 : Training: loss:  0.06739772\n",
      "6044 : Training: loss:  0.03887574\n",
      "6045 : Training: loss:  0.059447393\n",
      "6046 : Training: loss:  0.052313548\n",
      "6047 : Training: loss:  0.029215613\n",
      "6048 : Training: loss:  0.03698006\n",
      "6049 : Training: loss:  0.043922875\n",
      "6050 : Training: loss:  0.016835121\n",
      "6051 : Training: loss:  0.049805447\n",
      "6052 : Training: loss:  0.0477029\n",
      "6053 : Training: loss:  0.042656966\n",
      "6054 : Training: loss:  0.016353596\n",
      "6055 : Training: loss:  0.042720623\n",
      "6056 : Training: loss:  0.032329366\n",
      "6057 : Training: loss:  0.06358004\n",
      "6058 : Training: loss:  0.027087854\n",
      "6059 : Training: loss:  0.019922294\n",
      "6060 : Training: loss:  0.062289562\n",
      "Validation: Loss:  0.066225685  Accuracy:  0.7307692\n",
      "6061 : Training: loss:  0.01650157\n",
      "6062 : Training: loss:  0.07388493\n",
      "6063 : Training: loss:  0.03778257\n",
      "6064 : Training: loss:  0.031512808\n",
      "6065 : Training: loss:  0.041805822\n",
      "6066 : Training: loss:  0.035650987\n",
      "6067 : Training: loss:  0.06432189\n",
      "6068 : Training: loss:  0.04745079\n",
      "6069 : Training: loss:  0.07030755\n",
      "6070 : Training: loss:  0.030601883\n",
      "6071 : Training: loss:  0.09846279\n",
      "6072 : Training: loss:  0.06544675\n",
      "6073 : Training: loss:  0.028542073\n",
      "6074 : Training: loss:  0.023936255\n",
      "6075 : Training: loss:  0.059185598\n",
      "6076 : Training: loss:  0.04389882\n",
      "6077 : Training: loss:  0.07689477\n",
      "6078 : Training: loss:  0.02507062\n",
      "6079 : Training: loss:  0.04876801\n",
      "6080 : Training: loss:  0.043542054\n",
      "Validation: Loss:  0.06602415  Accuracy:  0.75\n",
      "6081 : Training: loss:  0.016724192\n",
      "6082 : Training: loss:  0.062344313\n",
      "6083 : Training: loss:  0.041856173\n",
      "6084 : Training: loss:  0.0151369795\n",
      "6085 : Training: loss:  0.057486705\n",
      "6086 : Training: loss:  0.047193613\n",
      "6087 : Training: loss:  0.069253616\n",
      "6088 : Training: loss:  0.036996346\n",
      "6089 : Training: loss:  0.034199882\n",
      "6090 : Training: loss:  0.07940964\n",
      "6091 : Training: loss:  0.051109888\n",
      "6092 : Training: loss:  0.082602404\n",
      "6093 : Training: loss:  0.0384958\n",
      "6094 : Training: loss:  0.052880123\n",
      "6095 : Training: loss:  0.09185025\n",
      "6096 : Training: loss:  0.06346105\n",
      "6097 : Training: loss:  0.035652634\n",
      "6098 : Training: loss:  0.017362831\n",
      "6099 : Training: loss:  0.017954024\n",
      "6100 : Training: loss:  0.0229148\n",
      "Validation: Loss:  0.06593694  Accuracy:  0.7307692\n",
      "6101 : Training: loss:  0.05455854\n",
      "6102 : Training: loss:  0.05415852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6103 : Training: loss:  0.0734009\n",
      "6104 : Training: loss:  0.017654518\n",
      "6105 : Training: loss:  0.07469838\n",
      "6106 : Training: loss:  0.06001646\n",
      "6107 : Training: loss:  0.05684831\n",
      "6108 : Training: loss:  0.05850115\n",
      "6109 : Training: loss:  0.041527763\n",
      "6110 : Training: loss:  0.044093672\n",
      "6111 : Training: loss:  0.056746773\n",
      "6112 : Training: loss:  0.045400824\n",
      "6113 : Training: loss:  0.052111115\n",
      "6114 : Training: loss:  0.0496672\n",
      "6115 : Training: loss:  0.029932406\n",
      "6116 : Training: loss:  0.030872699\n",
      "6117 : Training: loss:  0.044851474\n",
      "6118 : Training: loss:  0.06706747\n",
      "6119 : Training: loss:  0.01545485\n",
      "6120 : Training: loss:  0.034400288\n",
      "Validation: Loss:  0.065655895  Accuracy:  0.7307692\n",
      "6121 : Training: loss:  0.05732426\n",
      "6122 : Training: loss:  0.04729918\n",
      "6123 : Training: loss:  0.04153995\n",
      "6124 : Training: loss:  0.049888376\n",
      "6125 : Training: loss:  0.07254483\n",
      "6126 : Training: loss:  0.085859105\n",
      "6127 : Training: loss:  0.061029725\n",
      "6128 : Training: loss:  0.032462664\n",
      "6129 : Training: loss:  0.021395281\n",
      "6130 : Training: loss:  0.09187431\n",
      "6131 : Training: loss:  0.045768052\n",
      "6132 : Training: loss:  0.034616526\n",
      "6133 : Training: loss:  0.044003874\n",
      "6134 : Training: loss:  0.033044584\n",
      "6135 : Training: loss:  0.028430657\n",
      "6136 : Training: loss:  0.055774864\n",
      "6137 : Training: loss:  0.043106794\n",
      "6138 : Training: loss:  0.028019056\n",
      "6139 : Training: loss:  0.079607\n",
      "6140 : Training: loss:  0.04584326\n",
      "Validation: Loss:  0.065383375  Accuracy:  0.7307692\n",
      "6141 : Training: loss:  0.038890608\n",
      "6142 : Training: loss:  0.096809916\n",
      "6143 : Training: loss:  0.0772633\n",
      "6144 : Training: loss:  0.059878904\n",
      "6145 : Training: loss:  0.04931379\n",
      "6146 : Training: loss:  0.0374887\n",
      "6147 : Training: loss:  0.051299877\n",
      "6148 : Training: loss:  0.047659878\n",
      "6149 : Training: loss:  0.010758133\n",
      "6150 : Training: loss:  0.06158798\n",
      "6151 : Training: loss:  0.03569177\n",
      "6152 : Training: loss:  0.05011217\n",
      "6153 : Training: loss:  0.06338641\n",
      "6154 : Training: loss:  0.06598757\n",
      "6155 : Training: loss:  0.035599817\n",
      "6156 : Training: loss:  0.05890231\n",
      "6157 : Training: loss:  0.064602956\n",
      "6158 : Training: loss:  0.04024927\n",
      "6159 : Training: loss:  0.047143385\n",
      "6160 : Training: loss:  0.05539934\n",
      "Validation: Loss:  0.065251954  Accuracy:  0.7307692\n",
      "6161 : Training: loss:  0.061216626\n",
      "6162 : Training: loss:  0.030673826\n",
      "6163 : Training: loss:  0.047527414\n",
      "6164 : Training: loss:  0.036437914\n",
      "6165 : Training: loss:  0.063510984\n",
      "6166 : Training: loss:  0.036626603\n",
      "6167 : Training: loss:  0.0318688\n",
      "6168 : Training: loss:  0.058671564\n",
      "6169 : Training: loss:  0.046703715\n",
      "6170 : Training: loss:  0.0332336\n",
      "6171 : Training: loss:  0.03844127\n",
      "6172 : Training: loss:  0.04788593\n",
      "6173 : Training: loss:  0.0700503\n",
      "6174 : Training: loss:  0.03671511\n",
      "6175 : Training: loss:  0.055544004\n",
      "6176 : Training: loss:  0.049798418\n",
      "6177 : Training: loss:  0.09354865\n",
      "6178 : Training: loss:  0.01580631\n",
      "6179 : Training: loss:  0.024024501\n",
      "6180 : Training: loss:  0.090367034\n",
      "Validation: Loss:  0.06496862  Accuracy:  0.7307692\n",
      "6181 : Training: loss:  0.05132739\n",
      "6182 : Training: loss:  0.008956206\n",
      "6183 : Training: loss:  0.053739782\n",
      "6184 : Training: loss:  0.033218864\n",
      "6185 : Training: loss:  0.046149872\n",
      "6186 : Training: loss:  0.049383666\n",
      "6187 : Training: loss:  0.041920304\n",
      "6188 : Training: loss:  0.06016243\n",
      "6189 : Training: loss:  0.057801344\n",
      "6190 : Training: loss:  0.050410174\n",
      "6191 : Training: loss:  0.06911575\n",
      "6192 : Training: loss:  0.05928242\n",
      "6193 : Training: loss:  0.07441932\n",
      "6194 : Training: loss:  0.03951502\n",
      "6195 : Training: loss:  0.049072362\n",
      "6196 : Training: loss:  0.049147453\n",
      "6197 : Training: loss:  0.031796895\n",
      "6198 : Training: loss:  0.055481955\n",
      "6199 : Training: loss:  0.033581335\n",
      "6200 : Training: loss:  0.034639183\n",
      "Validation: Loss:  0.064884976  Accuracy:  0.7307692\n",
      "6201 : Training: loss:  0.032252464\n",
      "6202 : Training: loss:  0.0122065125\n",
      "6203 : Training: loss:  0.06082534\n",
      "6204 : Training: loss:  0.051940378\n",
      "6205 : Training: loss:  0.023152456\n",
      "6206 : Training: loss:  0.031066563\n",
      "6207 : Training: loss:  0.03302808\n",
      "6208 : Training: loss:  0.08334063\n",
      "6209 : Training: loss:  0.030857252\n",
      "6210 : Training: loss:  0.05578692\n",
      "6211 : Training: loss:  0.032366507\n",
      "6212 : Training: loss:  0.038366765\n",
      "6213 : Training: loss:  0.08218101\n",
      "6214 : Training: loss:  0.041096132\n",
      "6215 : Training: loss:  0.054426614\n",
      "6216 : Training: loss:  0.03674554\n",
      "6217 : Training: loss:  0.047523078\n",
      "6218 : Training: loss:  0.026662802\n",
      "6219 : Training: loss:  0.067113556\n",
      "6220 : Training: loss:  0.04843929\n",
      "Validation: Loss:  0.06506979  Accuracy:  0.75\n",
      "6221 : Training: loss:  0.06741796\n",
      "6222 : Training: loss:  0.038878445\n",
      "6223 : Training: loss:  0.035916004\n",
      "6224 : Training: loss:  0.06801932\n",
      "6225 : Training: loss:  0.04841481\n",
      "6226 : Training: loss:  0.049991257\n",
      "6227 : Training: loss:  0.038738243\n",
      "6228 : Training: loss:  0.055548377\n",
      "6229 : Training: loss:  0.022433374\n",
      "6230 : Training: loss:  0.0608871\n",
      "6231 : Training: loss:  0.027627485\n",
      "6232 : Training: loss:  0.04743746\n",
      "6233 : Training: loss:  0.026220439\n",
      "6234 : Training: loss:  0.034983594\n",
      "6235 : Training: loss:  0.038749043\n",
      "6236 : Training: loss:  0.06280717\n",
      "6237 : Training: loss:  0.024391392\n",
      "6238 : Training: loss:  0.068112314\n",
      "6239 : Training: loss:  0.0572363\n",
      "6240 : Training: loss:  0.035120424\n",
      "Validation: Loss:  0.06529162  Accuracy:  0.71153843\n",
      "6241 : Training: loss:  0.06270928\n",
      "6242 : Training: loss:  0.08702911\n",
      "6243 : Training: loss:  0.07063746\n",
      "6244 : Training: loss:  0.05755208\n",
      "6245 : Training: loss:  0.054597933\n",
      "6246 : Training: loss:  0.047269482\n",
      "6247 : Training: loss:  0.05874518\n",
      "6248 : Training: loss:  0.029747082\n",
      "6249 : Training: loss:  0.014136143\n",
      "6250 : Training: loss:  0.047498032\n",
      "6251 : Training: loss:  0.025116364\n",
      "6252 : Training: loss:  0.070196934\n",
      "6253 : Training: loss:  0.026775196\n",
      "6254 : Training: loss:  0.045422312\n",
      "6255 : Training: loss:  0.033369064\n",
      "6256 : Training: loss:  0.023100646\n",
      "6257 : Training: loss:  0.027535694\n",
      "6258 : Training: loss:  0.020363994\n",
      "6259 : Training: loss:  0.04380811\n",
      "6260 : Training: loss:  0.053362902\n",
      "Validation: Loss:  0.06522035  Accuracy:  0.7307692\n",
      "6261 : Training: loss:  0.041382153\n",
      "6262 : Training: loss:  0.030047704\n",
      "6263 : Training: loss:  0.045545842\n",
      "6264 : Training: loss:  0.010417383\n",
      "6265 : Training: loss:  0.044753652\n",
      "6266 : Training: loss:  0.032280117\n",
      "6267 : Training: loss:  0.05198491\n",
      "6268 : Training: loss:  0.037587006\n",
      "6269 : Training: loss:  0.07163499\n",
      "6270 : Training: loss:  0.044156723\n",
      "6271 : Training: loss:  0.025206758\n",
      "6272 : Training: loss:  0.019299982\n",
      "6273 : Training: loss:  0.031108057\n",
      "6274 : Training: loss:  0.02002794\n",
      "6275 : Training: loss:  0.05384998\n",
      "6276 : Training: loss:  0.036011316\n",
      "6277 : Training: loss:  0.055290133\n",
      "6278 : Training: loss:  0.033581745\n",
      "6279 : Training: loss:  0.061456654\n",
      "6280 : Training: loss:  0.034445737\n",
      "Validation: Loss:  0.06511063  Accuracy:  0.7307692\n",
      "6281 : Training: loss:  0.05310977\n",
      "6282 : Training: loss:  0.048099406\n",
      "6283 : Training: loss:  0.07782207\n",
      "6284 : Training: loss:  0.035050504\n",
      "6285 : Training: loss:  0.043157715\n",
      "6286 : Training: loss:  0.08027376\n",
      "6287 : Training: loss:  0.093850516\n",
      "6288 : Training: loss:  0.011565382\n",
      "6289 : Training: loss:  0.022773828\n",
      "6290 : Training: loss:  0.03232366\n",
      "6291 : Training: loss:  0.02908207\n",
      "6292 : Training: loss:  0.03930977\n",
      "6293 : Training: loss:  0.037830546\n",
      "6294 : Training: loss:  0.054945596\n",
      "6295 : Training: loss:  0.052860085\n",
      "6296 : Training: loss:  0.028468587\n",
      "6297 : Training: loss:  0.046452794\n",
      "6298 : Training: loss:  0.035611145\n",
      "6299 : Training: loss:  0.053372208\n",
      "6300 : Training: loss:  0.08934618\n",
      "Validation: Loss:  0.06468283  Accuracy:  0.7307692\n",
      "6301 : Training: loss:  0.056021925\n",
      "6302 : Training: loss:  0.054146845\n",
      "6303 : Training: loss:  0.05298254\n",
      "6304 : Training: loss:  0.06441627\n",
      "6305 : Training: loss:  0.036679253\n",
      "6306 : Training: loss:  0.03504322\n",
      "6307 : Training: loss:  0.0397763\n",
      "6308 : Training: loss:  0.012117152\n",
      "6309 : Training: loss:  0.06088583\n",
      "6310 : Training: loss:  0.047264434\n",
      "6311 : Training: loss:  0.043621246\n",
      "6312 : Training: loss:  0.066478625\n",
      "6313 : Training: loss:  0.04324586\n",
      "6314 : Training: loss:  0.08363833\n",
      "6315 : Training: loss:  0.034418553\n",
      "6316 : Training: loss:  0.063258186\n",
      "6317 : Training: loss:  0.05874996\n",
      "6318 : Training: loss:  0.059047613\n",
      "6319 : Training: loss:  0.049221136\n",
      "6320 : Training: loss:  0.06873915\n",
      "Validation: Loss:  0.06502134  Accuracy:  0.7307692\n",
      "6321 : Training: loss:  0.045203045\n",
      "6322 : Training: loss:  0.052816577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6323 : Training: loss:  0.033201907\n",
      "6324 : Training: loss:  0.024509907\n",
      "6325 : Training: loss:  0.067175396\n",
      "6326 : Training: loss:  0.0278203\n",
      "6327 : Training: loss:  0.077275135\n",
      "6328 : Training: loss:  0.06415202\n",
      "6329 : Training: loss:  0.041937497\n",
      "6330 : Training: loss:  0.06018312\n",
      "6331 : Training: loss:  0.047454238\n",
      "6332 : Training: loss:  0.033120364\n",
      "6333 : Training: loss:  0.030398691\n",
      "6334 : Training: loss:  0.058473594\n",
      "6335 : Training: loss:  0.050780002\n",
      "6336 : Training: loss:  0.06546926\n",
      "6337 : Training: loss:  0.049647603\n",
      "6338 : Training: loss:  0.051939066\n",
      "6339 : Training: loss:  0.03569663\n",
      "6340 : Training: loss:  0.06763433\n",
      "Validation: Loss:  0.064929985  Accuracy:  0.7307692\n",
      "6341 : Training: loss:  0.08289909\n",
      "6342 : Training: loss:  0.050448664\n",
      "6343 : Training: loss:  0.027228236\n",
      "6344 : Training: loss:  0.037257336\n",
      "6345 : Training: loss:  0.06702918\n",
      "6346 : Training: loss:  0.04565919\n",
      "6347 : Training: loss:  0.052046254\n",
      "6348 : Training: loss:  0.04385014\n",
      "6349 : Training: loss:  0.05305035\n",
      "6350 : Training: loss:  0.03154794\n",
      "6351 : Training: loss:  0.07754182\n",
      "6352 : Training: loss:  0.05428307\n",
      "6353 : Training: loss:  0.028607884\n",
      "6354 : Training: loss:  0.06613253\n",
      "6355 : Training: loss:  0.061869435\n",
      "6356 : Training: loss:  0.03293638\n",
      "6357 : Training: loss:  0.057087366\n",
      "6358 : Training: loss:  0.042031106\n",
      "6359 : Training: loss:  0.035371598\n",
      "6360 : Training: loss:  0.052855086\n",
      "Validation: Loss:  0.064729944  Accuracy:  0.7307692\n",
      "6361 : Training: loss:  0.051071383\n",
      "6362 : Training: loss:  0.0359893\n",
      "6363 : Training: loss:  0.019990757\n",
      "6364 : Training: loss:  0.050315354\n",
      "6365 : Training: loss:  0.08823416\n",
      "6366 : Training: loss:  0.025643658\n",
      "6367 : Training: loss:  0.048060108\n",
      "6368 : Training: loss:  0.0414766\n",
      "6369 : Training: loss:  0.038299672\n",
      "6370 : Training: loss:  0.019411528\n",
      "6371 : Training: loss:  0.031113677\n",
      "6372 : Training: loss:  0.013655985\n",
      "6373 : Training: loss:  0.030456817\n",
      "6374 : Training: loss:  0.028192058\n",
      "6375 : Training: loss:  0.03105064\n",
      "6376 : Training: loss:  0.051879\n",
      "6377 : Training: loss:  0.036988776\n",
      "6378 : Training: loss:  0.027072107\n",
      "6379 : Training: loss:  0.058843326\n",
      "6380 : Training: loss:  0.058145717\n",
      "Validation: Loss:  0.06476951  Accuracy:  0.7307692\n",
      "6381 : Training: loss:  0.07089838\n",
      "6382 : Training: loss:  0.030812964\n",
      "6383 : Training: loss:  0.040696073\n",
      "6384 : Training: loss:  0.07075307\n",
      "6385 : Training: loss:  0.040401477\n",
      "6386 : Training: loss:  0.029522311\n",
      "6387 : Training: loss:  0.064874835\n",
      "6388 : Training: loss:  0.037831403\n",
      "6389 : Training: loss:  0.057769228\n",
      "6390 : Training: loss:  0.020394765\n",
      "6391 : Training: loss:  0.042068694\n",
      "6392 : Training: loss:  0.059029978\n",
      "6393 : Training: loss:  0.052778173\n",
      "6394 : Training: loss:  0.058501434\n",
      "6395 : Training: loss:  0.025500085\n",
      "6396 : Training: loss:  0.045156326\n",
      "6397 : Training: loss:  0.01207171\n",
      "6398 : Training: loss:  0.030176807\n",
      "6399 : Training: loss:  0.07323851\n",
      "6400 : Training: loss:  0.061269388\n",
      "Validation: Loss:  0.06444833  Accuracy:  0.7307692\n",
      "6401 : Training: loss:  0.03666173\n",
      "6402 : Training: loss:  0.019537333\n",
      "6403 : Training: loss:  0.025408123\n",
      "6404 : Training: loss:  0.029767709\n",
      "6405 : Training: loss:  0.039322298\n",
      "6406 : Training: loss:  0.03177844\n",
      "6407 : Training: loss:  0.061590783\n",
      "6408 : Training: loss:  0.026106903\n",
      "6409 : Training: loss:  0.042400986\n",
      "6410 : Training: loss:  0.060035497\n",
      "6411 : Training: loss:  0.014288922\n",
      "6412 : Training: loss:  0.05983701\n",
      "6413 : Training: loss:  0.036325913\n",
      "6414 : Training: loss:  0.037719116\n",
      "6415 : Training: loss:  0.030570326\n",
      "6416 : Training: loss:  0.064890005\n",
      "6417 : Training: loss:  0.027886674\n",
      "6418 : Training: loss:  0.058784574\n",
      "6419 : Training: loss:  0.0592731\n",
      "6420 : Training: loss:  0.02678676\n",
      "Validation: Loss:  0.064661615  Accuracy:  0.7307692\n",
      "6421 : Training: loss:  0.030258782\n",
      "6422 : Training: loss:  0.03425392\n",
      "6423 : Training: loss:  0.07939515\n",
      "6424 : Training: loss:  0.044832803\n",
      "6425 : Training: loss:  0.081377916\n",
      "6426 : Training: loss:  0.048431735\n",
      "6427 : Training: loss:  0.066169806\n",
      "6428 : Training: loss:  0.019025365\n",
      "6429 : Training: loss:  0.04092274\n",
      "6430 : Training: loss:  0.049376216\n",
      "6431 : Training: loss:  0.05486908\n",
      "6432 : Training: loss:  0.032347698\n",
      "6433 : Training: loss:  0.059469633\n",
      "6434 : Training: loss:  0.04366012\n",
      "6435 : Training: loss:  0.031607334\n",
      "6436 : Training: loss:  0.024362179\n",
      "6437 : Training: loss:  0.05512554\n",
      "6438 : Training: loss:  0.034254402\n",
      "6439 : Training: loss:  0.06582027\n",
      "6440 : Training: loss:  0.0037077516\n",
      "Validation: Loss:  0.06472209  Accuracy:  0.7307692\n",
      "6441 : Training: loss:  0.06999169\n",
      "6442 : Training: loss:  0.06838815\n",
      "6443 : Training: loss:  0.040608235\n",
      "6444 : Training: loss:  0.036425147\n",
      "6445 : Training: loss:  0.072205536\n",
      "6446 : Training: loss:  0.061511617\n",
      "6447 : Training: loss:  0.016103113\n",
      "6448 : Training: loss:  0.048019018\n",
      "6449 : Training: loss:  0.06713613\n",
      "6450 : Training: loss:  0.06924751\n",
      "6451 : Training: loss:  0.05965596\n",
      "6452 : Training: loss:  0.08657156\n",
      "6453 : Training: loss:  0.034722656\n",
      "6454 : Training: loss:  0.024584793\n",
      "6455 : Training: loss:  0.040243387\n",
      "6456 : Training: loss:  0.043542966\n",
      "6457 : Training: loss:  0.049602803\n",
      "6458 : Training: loss:  0.034953255\n",
      "6459 : Training: loss:  0.058443554\n",
      "6460 : Training: loss:  0.022466326\n",
      "Validation: Loss:  0.06503498  Accuracy:  0.7307692\n",
      "6461 : Training: loss:  0.047933526\n",
      "6462 : Training: loss:  0.08155366\n",
      "6463 : Training: loss:  0.07032476\n",
      "6464 : Training: loss:  0.034098048\n",
      "6465 : Training: loss:  0.045234278\n",
      "6466 : Training: loss:  0.028683249\n",
      "6467 : Training: loss:  0.04066119\n",
      "6468 : Training: loss:  0.03758642\n",
      "6469 : Training: loss:  0.046721116\n",
      "6470 : Training: loss:  0.015502509\n",
      "6471 : Training: loss:  0.042882584\n",
      "6472 : Training: loss:  0.042755194\n",
      "6473 : Training: loss:  0.013792707\n",
      "6474 : Training: loss:  0.025961589\n",
      "6475 : Training: loss:  0.0581011\n",
      "6476 : Training: loss:  0.02441399\n",
      "6477 : Training: loss:  0.040304285\n",
      "6478 : Training: loss:  0.05022537\n",
      "6479 : Training: loss:  0.06801537\n",
      "6480 : Training: loss:  0.04366651\n",
      "Validation: Loss:  0.06513839  Accuracy:  0.75\n",
      "6481 : Training: loss:  0.029841125\n",
      "6482 : Training: loss:  0.049031682\n",
      "6483 : Training: loss:  0.046973396\n",
      "6484 : Training: loss:  0.053198013\n",
      "6485 : Training: loss:  0.030092588\n",
      "6486 : Training: loss:  0.06615484\n",
      "6487 : Training: loss:  0.039648373\n",
      "6488 : Training: loss:  0.049199812\n",
      "6489 : Training: loss:  0.03809832\n",
      "6490 : Training: loss:  0.024525067\n",
      "6491 : Training: loss:  0.027661208\n",
      "6492 : Training: loss:  0.039081022\n",
      "6493 : Training: loss:  0.066566594\n",
      "6494 : Training: loss:  0.03807513\n",
      "6495 : Training: loss:  0.05985179\n",
      "6496 : Training: loss:  0.08208608\n",
      "6497 : Training: loss:  0.045011785\n",
      "6498 : Training: loss:  0.075943075\n",
      "6499 : Training: loss:  0.07225951\n",
      "6500 : Training: loss:  0.027995888\n",
      "Validation: Loss:  0.064815044  Accuracy:  0.7307692\n",
      "6501 : Training: loss:  0.048290603\n",
      "6502 : Training: loss:  0.08621332\n",
      "6503 : Training: loss:  0.03854927\n",
      "6504 : Training: loss:  0.031930562\n",
      "6505 : Training: loss:  0.07829561\n",
      "6506 : Training: loss:  0.04063949\n",
      "6507 : Training: loss:  0.0596991\n",
      "6508 : Training: loss:  0.038453203\n",
      "6509 : Training: loss:  0.09023513\n",
      "6510 : Training: loss:  0.039865\n",
      "6511 : Training: loss:  0.035222657\n",
      "6512 : Training: loss:  0.021845074\n",
      "6513 : Training: loss:  0.007322471\n",
      "6514 : Training: loss:  0.062469196\n",
      "6515 : Training: loss:  0.05472358\n",
      "6516 : Training: loss:  0.025691\n",
      "6517 : Training: loss:  0.04112485\n",
      "6518 : Training: loss:  0.04511313\n",
      "6519 : Training: loss:  0.052779652\n",
      "6520 : Training: loss:  0.024530124\n",
      "Validation: Loss:  0.064296655  Accuracy:  0.7307692\n",
      "6521 : Training: loss:  0.019452738\n",
      "6522 : Training: loss:  0.010827701\n",
      "6523 : Training: loss:  0.058648366\n",
      "6524 : Training: loss:  0.053158887\n",
      "6525 : Training: loss:  0.04783543\n",
      "6526 : Training: loss:  0.041171264\n",
      "6527 : Training: loss:  0.016656628\n",
      "6528 : Training: loss:  0.019131683\n",
      "6529 : Training: loss:  0.07182572\n",
      "6530 : Training: loss:  0.04646201\n",
      "6531 : Training: loss:  0.047706984\n",
      "6532 : Training: loss:  0.079526715\n",
      "6533 : Training: loss:  0.031463083\n",
      "6534 : Training: loss:  0.03164251\n",
      "6535 : Training: loss:  0.036490843\n",
      "6536 : Training: loss:  0.0406812\n",
      "6537 : Training: loss:  0.035331894\n",
      "6538 : Training: loss:  0.022431834\n",
      "6539 : Training: loss:  0.07092065\n",
      "6540 : Training: loss:  0.042917296\n",
      "Validation: Loss:  0.06454492  Accuracy:  0.7307692\n",
      "6541 : Training: loss:  0.02360104\n",
      "6542 : Training: loss:  0.08534803\n",
      "6543 : Training: loss:  0.040183846\n",
      "6544 : Training: loss:  0.05374379\n",
      "6545 : Training: loss:  0.040424444\n",
      "6546 : Training: loss:  0.043374296\n",
      "6547 : Training: loss:  0.031679418\n",
      "6548 : Training: loss:  0.046499275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6549 : Training: loss:  0.026498111\n",
      "6550 : Training: loss:  0.05227571\n",
      "6551 : Training: loss:  0.059457827\n",
      "6552 : Training: loss:  0.03697714\n",
      "6553 : Training: loss:  0.03466727\n",
      "6554 : Training: loss:  0.032045595\n",
      "6555 : Training: loss:  0.048412666\n",
      "6556 : Training: loss:  0.028048337\n",
      "6557 : Training: loss:  0.049293235\n",
      "6558 : Training: loss:  0.014159958\n",
      "6559 : Training: loss:  0.06110595\n",
      "6560 : Training: loss:  0.048247147\n",
      "Validation: Loss:  0.06445506  Accuracy:  0.7307692\n",
      "6561 : Training: loss:  0.058830407\n",
      "6562 : Training: loss:  0.034641437\n",
      "6563 : Training: loss:  0.047503848\n",
      "6564 : Training: loss:  0.024019277\n",
      "6565 : Training: loss:  0.05360435\n",
      "6566 : Training: loss:  0.028315723\n",
      "6567 : Training: loss:  0.035543624\n",
      "6568 : Training: loss:  0.055562165\n",
      "6569 : Training: loss:  0.01773411\n",
      "6570 : Training: loss:  0.04190691\n",
      "6571 : Training: loss:  0.049246557\n",
      "6572 : Training: loss:  0.028338892\n",
      "6573 : Training: loss:  0.03765912\n",
      "6574 : Training: loss:  0.07595223\n",
      "6575 : Training: loss:  0.016742967\n",
      "6576 : Training: loss:  0.011097729\n",
      "6577 : Training: loss:  0.064107716\n",
      "6578 : Training: loss:  0.0681746\n",
      "6579 : Training: loss:  0.018921874\n",
      "6580 : Training: loss:  0.025684819\n",
      "Validation: Loss:  0.06430258  Accuracy:  0.71153843\n",
      "6581 : Training: loss:  0.028202971\n",
      "6582 : Training: loss:  0.03195687\n",
      "6583 : Training: loss:  0.043300755\n",
      "6584 : Training: loss:  0.051909383\n",
      "6585 : Training: loss:  0.05287513\n",
      "6586 : Training: loss:  0.051559553\n",
      "6587 : Training: loss:  0.06785239\n",
      "6588 : Training: loss:  0.05478474\n",
      "6589 : Training: loss:  0.061906297\n",
      "6590 : Training: loss:  0.060657\n",
      "6591 : Training: loss:  0.04321526\n",
      "6592 : Training: loss:  0.04016265\n",
      "6593 : Training: loss:  0.034157183\n",
      "6594 : Training: loss:  0.017989958\n",
      "6595 : Training: loss:  0.030378606\n",
      "6596 : Training: loss:  0.042832486\n",
      "6597 : Training: loss:  0.031852994\n",
      "6598 : Training: loss:  0.016213031\n",
      "6599 : Training: loss:  0.050017692\n",
      "6600 : Training: loss:  0.018636843\n",
      "Validation: Loss:  0.06423192  Accuracy:  0.71153843\n",
      "6601 : Training: loss:  0.08129054\n",
      "6602 : Training: loss:  0.03256494\n",
      "6603 : Training: loss:  0.04258794\n",
      "6604 : Training: loss:  0.061431073\n",
      "6605 : Training: loss:  0.05687507\n",
      "6606 : Training: loss:  0.06446816\n",
      "6607 : Training: loss:  0.03218538\n",
      "6608 : Training: loss:  0.0058711623\n",
      "6609 : Training: loss:  0.034572877\n",
      "6610 : Training: loss:  0.030301908\n",
      "6611 : Training: loss:  0.03439383\n",
      "6612 : Training: loss:  0.09695548\n",
      "6613 : Training: loss:  0.056130145\n",
      "6614 : Training: loss:  0.010108194\n",
      "6615 : Training: loss:  0.057652406\n",
      "6616 : Training: loss:  0.048717596\n",
      "6617 : Training: loss:  0.06510852\n",
      "6618 : Training: loss:  0.06122682\n",
      "6619 : Training: loss:  0.023170754\n",
      "6620 : Training: loss:  0.032160997\n",
      "Validation: Loss:  0.06392631  Accuracy:  0.6923077\n",
      "6621 : Training: loss:  0.035386715\n",
      "6622 : Training: loss:  0.04446079\n",
      "6623 : Training: loss:  0.060309984\n",
      "6624 : Training: loss:  0.039797284\n",
      "6625 : Training: loss:  0.029518807\n",
      "6626 : Training: loss:  0.03083973\n",
      "6627 : Training: loss:  0.02597902\n",
      "6628 : Training: loss:  0.056873854\n",
      "6629 : Training: loss:  0.07079083\n",
      "6630 : Training: loss:  0.055930495\n",
      "6631 : Training: loss:  0.044641152\n",
      "6632 : Training: loss:  0.009596503\n",
      "6633 : Training: loss:  0.072038524\n",
      "6634 : Training: loss:  0.05695362\n",
      "6635 : Training: loss:  0.053624853\n",
      "6636 : Training: loss:  0.03231228\n",
      "6637 : Training: loss:  0.028762642\n",
      "6638 : Training: loss:  0.063418694\n",
      "6639 : Training: loss:  0.052656338\n",
      "6640 : Training: loss:  0.051647443\n",
      "Validation: Loss:  0.06365126  Accuracy:  0.6923077\n",
      "6641 : Training: loss:  0.056950197\n",
      "6642 : Training: loss:  0.03438917\n",
      "6643 : Training: loss:  0.11121561\n",
      "6644 : Training: loss:  0.04625446\n",
      "6645 : Training: loss:  0.0506025\n",
      "6646 : Training: loss:  0.06725552\n",
      "6647 : Training: loss:  0.032903317\n",
      "6648 : Training: loss:  0.06957137\n",
      "6649 : Training: loss:  0.076090336\n",
      "6650 : Training: loss:  0.011540513\n",
      "6651 : Training: loss:  0.051930938\n",
      "6652 : Training: loss:  0.0660471\n",
      "6653 : Training: loss:  0.038535666\n",
      "6654 : Training: loss:  0.041449167\n",
      "6655 : Training: loss:  0.022207089\n",
      "6656 : Training: loss:  0.030313874\n",
      "6657 : Training: loss:  0.016193222\n",
      "6658 : Training: loss:  0.08674503\n",
      "6659 : Training: loss:  0.08268395\n",
      "6660 : Training: loss:  0.065292746\n",
      "Validation: Loss:  0.06352181  Accuracy:  0.6923077\n",
      "6661 : Training: loss:  0.057567257\n",
      "6662 : Training: loss:  0.038578536\n",
      "6663 : Training: loss:  0.024127759\n",
      "6664 : Training: loss:  0.010199203\n",
      "6665 : Training: loss:  0.03939806\n",
      "6666 : Training: loss:  0.02818708\n",
      "6667 : Training: loss:  0.021703722\n",
      "6668 : Training: loss:  0.056147773\n",
      "6669 : Training: loss:  0.06713008\n",
      "6670 : Training: loss:  0.05228787\n",
      "6671 : Training: loss:  0.056157596\n",
      "6672 : Training: loss:  0.052917294\n",
      "6673 : Training: loss:  0.05569539\n",
      "6674 : Training: loss:  0.022130381\n",
      "6675 : Training: loss:  0.030241799\n",
      "6676 : Training: loss:  0.026639976\n",
      "6677 : Training: loss:  0.055004187\n",
      "6678 : Training: loss:  0.04080372\n",
      "6679 : Training: loss:  0.012819967\n",
      "6680 : Training: loss:  0.05801928\n",
      "Validation: Loss:  0.06344605  Accuracy:  0.6923077\n",
      "6681 : Training: loss:  0.031426504\n",
      "6682 : Training: loss:  0.023760302\n",
      "6683 : Training: loss:  0.028267637\n",
      "6684 : Training: loss:  0.044992454\n",
      "6685 : Training: loss:  0.046035048\n",
      "6686 : Training: loss:  0.026297033\n",
      "6687 : Training: loss:  0.050183907\n",
      "6688 : Training: loss:  0.076405235\n",
      "6689 : Training: loss:  0.03129112\n",
      "6690 : Training: loss:  0.032990776\n",
      "6691 : Training: loss:  0.05085201\n",
      "6692 : Training: loss:  0.061968874\n",
      "6693 : Training: loss:  0.059015267\n",
      "6694 : Training: loss:  0.036871415\n",
      "6695 : Training: loss:  0.032884866\n",
      "6696 : Training: loss:  0.057918515\n",
      "6697 : Training: loss:  0.037397504\n",
      "6698 : Training: loss:  0.059382826\n",
      "6699 : Training: loss:  0.04031415\n",
      "6700 : Training: loss:  0.058699384\n",
      "Validation: Loss:  0.063160814  Accuracy:  0.6923077\n",
      "6701 : Training: loss:  0.05160667\n",
      "6702 : Training: loss:  0.06790198\n",
      "6703 : Training: loss:  0.01921115\n",
      "6704 : Training: loss:  0.07299623\n",
      "6705 : Training: loss:  0.035146892\n",
      "6706 : Training: loss:  0.055294983\n",
      "6707 : Training: loss:  0.03246611\n",
      "6708 : Training: loss:  0.04672139\n",
      "6709 : Training: loss:  0.05311718\n",
      "6710 : Training: loss:  0.03737212\n",
      "6711 : Training: loss:  0.006973066\n",
      "6712 : Training: loss:  0.055563305\n",
      "6713 : Training: loss:  0.05370371\n",
      "6714 : Training: loss:  0.03203621\n",
      "6715 : Training: loss:  0.017642956\n",
      "6716 : Training: loss:  0.069294676\n",
      "6717 : Training: loss:  0.029877512\n",
      "6718 : Training: loss:  0.045596458\n",
      "6719 : Training: loss:  0.03581137\n",
      "6720 : Training: loss:  0.053274006\n",
      "Validation: Loss:  0.06281151  Accuracy:  0.6923077\n",
      "6721 : Training: loss:  0.07284636\n",
      "6722 : Training: loss:  0.060575835\n",
      "6723 : Training: loss:  0.08584494\n",
      "6724 : Training: loss:  0.03530476\n",
      "6725 : Training: loss:  0.041650012\n",
      "6726 : Training: loss:  0.08177295\n",
      "6727 : Training: loss:  0.050819\n",
      "6728 : Training: loss:  0.031279143\n",
      "6729 : Training: loss:  0.05302121\n",
      "6730 : Training: loss:  0.052836634\n",
      "6731 : Training: loss:  0.029100284\n",
      "6732 : Training: loss:  0.021512909\n",
      "6733 : Training: loss:  0.036112584\n",
      "6734 : Training: loss:  0.05934902\n",
      "6735 : Training: loss:  0.07525131\n",
      "6736 : Training: loss:  0.062011365\n",
      "6737 : Training: loss:  0.03750534\n",
      "6738 : Training: loss:  0.03863899\n",
      "6739 : Training: loss:  0.0056124274\n",
      "6740 : Training: loss:  0.05114243\n",
      "Validation: Loss:  0.06255273  Accuracy:  0.71153843\n",
      "6741 : Training: loss:  0.03743184\n",
      "6742 : Training: loss:  0.011062636\n",
      "6743 : Training: loss:  0.02043705\n",
      "6744 : Training: loss:  0.018175596\n",
      "6745 : Training: loss:  0.0782919\n",
      "6746 : Training: loss:  0.045324832\n",
      "6747 : Training: loss:  0.081289895\n",
      "6748 : Training: loss:  0.080225214\n",
      "6749 : Training: loss:  0.062378574\n",
      "6750 : Training: loss:  0.043586593\n",
      "6751 : Training: loss:  0.034632728\n",
      "6752 : Training: loss:  0.015244347\n",
      "6753 : Training: loss:  0.05138454\n",
      "6754 : Training: loss:  0.07491053\n",
      "6755 : Training: loss:  0.059912607\n",
      "6756 : Training: loss:  0.07473985\n",
      "6757 : Training: loss:  0.03320824\n",
      "6758 : Training: loss:  0.056195863\n",
      "6759 : Training: loss:  0.020054376\n",
      "6760 : Training: loss:  0.031541143\n",
      "Validation: Loss:  0.06234561  Accuracy:  0.71153843\n",
      "6761 : Training: loss:  0.044790655\n",
      "6762 : Training: loss:  0.047896847\n",
      "6763 : Training: loss:  0.015345224\n",
      "6764 : Training: loss:  0.025902348\n",
      "6765 : Training: loss:  0.047066\n",
      "6766 : Training: loss:  0.021321584\n",
      "6767 : Training: loss:  0.07367183\n",
      "6768 : Training: loss:  0.02246675\n",
      "6769 : Training: loss:  0.03338893\n",
      "6770 : Training: loss:  0.06099463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6771 : Training: loss:  0.06681495\n",
      "6772 : Training: loss:  0.057260506\n",
      "6773 : Training: loss:  0.022894738\n",
      "6774 : Training: loss:  0.05152145\n",
      "6775 : Training: loss:  0.037655156\n",
      "6776 : Training: loss:  0.02706507\n",
      "6777 : Training: loss:  0.068890944\n",
      "6778 : Training: loss:  0.017614968\n",
      "6779 : Training: loss:  0.03836047\n",
      "6780 : Training: loss:  0.01742533\n",
      "Validation: Loss:  0.062275358  Accuracy:  0.71153843\n",
      "6781 : Training: loss:  0.040924594\n",
      "6782 : Training: loss:  0.036575086\n",
      "6783 : Training: loss:  0.026905343\n",
      "6784 : Training: loss:  0.053154398\n",
      "6785 : Training: loss:  0.059417244\n",
      "6786 : Training: loss:  0.06572405\n",
      "6787 : Training: loss:  0.10920993\n",
      "6788 : Training: loss:  0.05948139\n",
      "6789 : Training: loss:  0.059044354\n",
      "6790 : Training: loss:  0.030652506\n",
      "6791 : Training: loss:  0.055707272\n",
      "6792 : Training: loss:  0.046508327\n",
      "6793 : Training: loss:  0.040564448\n",
      "6794 : Training: loss:  0.020107074\n",
      "6795 : Training: loss:  0.012142927\n",
      "6796 : Training: loss:  0.067467704\n",
      "6797 : Training: loss:  0.028873023\n",
      "6798 : Training: loss:  0.017155845\n",
      "6799 : Training: loss:  0.064571984\n",
      "6800 : Training: loss:  0.05175008\n",
      "Validation: Loss:  0.062179852  Accuracy:  0.71153843\n",
      "6801 : Training: loss:  0.046603616\n",
      "6802 : Training: loss:  0.035941202\n",
      "6803 : Training: loss:  0.008163589\n",
      "6804 : Training: loss:  0.040885996\n",
      "6805 : Training: loss:  0.07997956\n",
      "6806 : Training: loss:  0.061748933\n",
      "6807 : Training: loss:  0.07485785\n",
      "6808 : Training: loss:  0.04143467\n",
      "6809 : Training: loss:  0.0194774\n",
      "6810 : Training: loss:  0.02488896\n",
      "6811 : Training: loss:  0.055426255\n",
      "6812 : Training: loss:  0.055209927\n",
      "6813 : Training: loss:  0.0894616\n",
      "6814 : Training: loss:  0.026565278\n",
      "6815 : Training: loss:  0.039488252\n",
      "6816 : Training: loss:  0.028983453\n",
      "6817 : Training: loss:  0.065280475\n",
      "6818 : Training: loss:  0.054783415\n",
      "6819 : Training: loss:  0.047305256\n",
      "6820 : Training: loss:  0.03089292\n",
      "Validation: Loss:  0.06208525  Accuracy:  0.75\n",
      "6821 : Training: loss:  0.031914994\n",
      "6822 : Training: loss:  0.052394103\n",
      "6823 : Training: loss:  0.06203336\n",
      "6824 : Training: loss:  0.04507256\n",
      "6825 : Training: loss:  0.02924559\n",
      "6826 : Training: loss:  0.031135323\n",
      "6827 : Training: loss:  0.032365855\n",
      "6828 : Training: loss:  0.03003482\n",
      "6829 : Training: loss:  0.044534974\n",
      "6830 : Training: loss:  0.048103936\n",
      "6831 : Training: loss:  0.047455624\n",
      "6832 : Training: loss:  0.042247538\n",
      "6833 : Training: loss:  0.06917001\n",
      "6834 : Training: loss:  0.07544119\n",
      "6835 : Training: loss:  0.02762253\n",
      "6836 : Training: loss:  0.044213492\n",
      "6837 : Training: loss:  0.041051533\n",
      "6838 : Training: loss:  0.040936034\n",
      "6839 : Training: loss:  0.05550893\n",
      "6840 : Training: loss:  0.04787275\n",
      "Validation: Loss:  0.062062733  Accuracy:  0.75\n",
      "6841 : Training: loss:  0.06357914\n",
      "6842 : Training: loss:  0.0892398\n",
      "6843 : Training: loss:  0.061351147\n",
      "6844 : Training: loss:  0.03664728\n",
      "6845 : Training: loss:  0.029965675\n",
      "6846 : Training: loss:  0.06724334\n",
      "6847 : Training: loss:  0.027305067\n",
      "6848 : Training: loss:  0.027535152\n",
      "6849 : Training: loss:  0.06005881\n",
      "6850 : Training: loss:  0.056279954\n",
      "6851 : Training: loss:  0.009874998\n",
      "6852 : Training: loss:  0.03154854\n",
      "6853 : Training: loss:  0.058346756\n",
      "6854 : Training: loss:  0.05402816\n",
      "6855 : Training: loss:  0.032053884\n",
      "6856 : Training: loss:  0.042136397\n",
      "6857 : Training: loss:  0.03775026\n",
      "6858 : Training: loss:  0.026501467\n",
      "6859 : Training: loss:  0.057520084\n",
      "6860 : Training: loss:  0.025407052\n",
      "Validation: Loss:  0.062040977  Accuracy:  0.7307692\n",
      "6861 : Training: loss:  0.033463392\n",
      "6862 : Training: loss:  0.05085369\n",
      "6863 : Training: loss:  0.0712905\n",
      "6864 : Training: loss:  0.052535128\n",
      "6865 : Training: loss:  0.06782274\n",
      "6866 : Training: loss:  0.08448004\n",
      "6867 : Training: loss:  0.027336117\n",
      "6868 : Training: loss:  0.041434314\n",
      "6869 : Training: loss:  0.041565172\n",
      "6870 : Training: loss:  0.04185039\n",
      "6871 : Training: loss:  0.07573004\n",
      "6872 : Training: loss:  0.043895558\n",
      "6873 : Training: loss:  0.061399795\n",
      "6874 : Training: loss:  0.03065952\n",
      "6875 : Training: loss:  0.035671752\n",
      "6876 : Training: loss:  0.03300437\n",
      "6877 : Training: loss:  0.045449156\n",
      "6878 : Training: loss:  0.058897212\n",
      "6879 : Training: loss:  0.046827808\n",
      "6880 : Training: loss:  0.026518228\n",
      "Validation: Loss:  0.06208125  Accuracy:  0.75\n",
      "6881 : Training: loss:  0.04570742\n",
      "6882 : Training: loss:  0.045443077\n",
      "6883 : Training: loss:  0.022248043\n",
      "6884 : Training: loss:  0.05609757\n",
      "6885 : Training: loss:  0.034572333\n",
      "6886 : Training: loss:  0.02291046\n",
      "6887 : Training: loss:  0.04329767\n",
      "6888 : Training: loss:  0.053560767\n",
      "6889 : Training: loss:  0.048908025\n",
      "6890 : Training: loss:  0.023412192\n",
      "6891 : Training: loss:  0.030828187\n",
      "6892 : Training: loss:  0.049351994\n",
      "6893 : Training: loss:  0.009845344\n",
      "6894 : Training: loss:  0.042357095\n",
      "6895 : Training: loss:  0.045030575\n",
      "6896 : Training: loss:  0.010712843\n",
      "6897 : Training: loss:  0.0117299305\n",
      "6898 : Training: loss:  0.052085493\n",
      "6899 : Training: loss:  0.053162806\n",
      "6900 : Training: loss:  0.058718607\n",
      "Validation: Loss:  0.062038768  Accuracy:  0.75\n",
      "6901 : Training: loss:  0.040131826\n",
      "6902 : Training: loss:  0.06141123\n",
      "6903 : Training: loss:  0.033999134\n",
      "6904 : Training: loss:  0.03781235\n",
      "6905 : Training: loss:  0.03021756\n",
      "6906 : Training: loss:  0.02348498\n",
      "6907 : Training: loss:  0.03738049\n",
      "6908 : Training: loss:  0.014402352\n",
      "6909 : Training: loss:  0.038641114\n",
      "6910 : Training: loss:  0.025990834\n",
      "6911 : Training: loss:  0.031968374\n",
      "6912 : Training: loss:  0.03493726\n",
      "6913 : Training: loss:  0.02118896\n",
      "6914 : Training: loss:  0.056156352\n",
      "6915 : Training: loss:  0.050512943\n",
      "6916 : Training: loss:  0.01568402\n",
      "6917 : Training: loss:  0.0352721\n",
      "6918 : Training: loss:  0.042099103\n",
      "6919 : Training: loss:  0.043467537\n",
      "6920 : Training: loss:  0.060217593\n",
      "Validation: Loss:  0.062129665  Accuracy:  0.75\n",
      "6921 : Training: loss:  0.025564395\n",
      "6922 : Training: loss:  0.039686833\n",
      "6923 : Training: loss:  0.056622166\n",
      "6924 : Training: loss:  0.05719924\n",
      "6925 : Training: loss:  0.04976902\n",
      "6926 : Training: loss:  0.006175221\n",
      "6927 : Training: loss:  0.04574352\n",
      "6928 : Training: loss:  0.0315856\n",
      "6929 : Training: loss:  0.05143887\n",
      "6930 : Training: loss:  0.044486325\n",
      "6931 : Training: loss:  0.039840817\n",
      "6932 : Training: loss:  0.025400842\n",
      "6933 : Training: loss:  0.0411828\n",
      "6934 : Training: loss:  0.0147184\n",
      "6935 : Training: loss:  0.039132085\n",
      "6936 : Training: loss:  0.055907413\n",
      "6937 : Training: loss:  0.06531245\n",
      "6938 : Training: loss:  0.011042553\n",
      "6939 : Training: loss:  0.055045314\n",
      "6940 : Training: loss:  0.07646294\n",
      "Validation: Loss:  0.06180763  Accuracy:  0.7307692\n",
      "6941 : Training: loss:  0.047840104\n",
      "6942 : Training: loss:  0.017007252\n",
      "6943 : Training: loss:  0.04308246\n",
      "6944 : Training: loss:  0.04884621\n",
      "6945 : Training: loss:  0.034540776\n",
      "6946 : Training: loss:  0.06470016\n",
      "6947 : Training: loss:  0.02892431\n",
      "6948 : Training: loss:  0.0659932\n",
      "6949 : Training: loss:  0.0606581\n",
      "6950 : Training: loss:  0.04555731\n",
      "6951 : Training: loss:  0.03433049\n",
      "6952 : Training: loss:  0.05424247\n",
      "6953 : Training: loss:  0.021046953\n",
      "6954 : Training: loss:  0.06227933\n",
      "6955 : Training: loss:  0.030913686\n",
      "6956 : Training: loss:  0.020793235\n",
      "6957 : Training: loss:  0.020666478\n",
      "6958 : Training: loss:  0.045330543\n",
      "6959 : Training: loss:  0.05212975\n",
      "6960 : Training: loss:  0.038958244\n",
      "Validation: Loss:  0.0613062  Accuracy:  0.75\n",
      "6961 : Training: loss:  0.04208785\n",
      "6962 : Training: loss:  0.08397602\n",
      "6963 : Training: loss:  0.04069907\n",
      "6964 : Training: loss:  0.01957192\n",
      "6965 : Training: loss:  0.03598122\n",
      "6966 : Training: loss:  0.015266965\n",
      "6967 : Training: loss:  0.034258638\n",
      "6968 : Training: loss:  0.07365709\n",
      "6969 : Training: loss:  0.042107034\n",
      "6970 : Training: loss:  0.032890853\n",
      "6971 : Training: loss:  0.058389235\n",
      "6972 : Training: loss:  0.057083435\n",
      "6973 : Training: loss:  0.05348197\n",
      "6974 : Training: loss:  0.028814225\n",
      "6975 : Training: loss:  0.0465766\n",
      "6976 : Training: loss:  0.05440463\n",
      "6977 : Training: loss:  0.0306267\n",
      "6978 : Training: loss:  0.064099014\n",
      "6979 : Training: loss:  0.029797414\n",
      "6980 : Training: loss:  0.050159153\n",
      "Validation: Loss:  0.06139271  Accuracy:  0.7307692\n",
      "6981 : Training: loss:  0.051191512\n",
      "6982 : Training: loss:  0.032522142\n",
      "6983 : Training: loss:  0.06888331\n",
      "6984 : Training: loss:  0.027505206\n",
      "6985 : Training: loss:  0.048758507\n",
      "6986 : Training: loss:  0.0661988\n",
      "6987 : Training: loss:  0.009924306\n",
      "6988 : Training: loss:  0.07344631\n",
      "6989 : Training: loss:  0.034287732\n",
      "6990 : Training: loss:  0.06323275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6991 : Training: loss:  0.042599097\n",
      "6992 : Training: loss:  0.04703701\n",
      "6993 : Training: loss:  0.044502337\n",
      "6994 : Training: loss:  0.055761416\n",
      "6995 : Training: loss:  0.03103386\n",
      "6996 : Training: loss:  0.05129616\n",
      "6997 : Training: loss:  0.061107226\n",
      "6998 : Training: loss:  0.05835364\n",
      "6999 : Training: loss:  0.030010184\n",
      "7000 : Training: loss:  0.06625923\n",
      "Validation: Loss:  0.06134088  Accuracy:  0.7307692\n",
      "7001 : Training: loss:  0.03128298\n",
      "7002 : Training: loss:  0.0641334\n",
      "7003 : Training: loss:  0.06925989\n",
      "7004 : Training: loss:  0.054260492\n",
      "7005 : Training: loss:  0.031347413\n",
      "7006 : Training: loss:  0.03047026\n",
      "7007 : Training: loss:  0.05561747\n",
      "7008 : Training: loss:  0.0073886104\n",
      "7009 : Training: loss:  0.04477232\n",
      "7010 : Training: loss:  0.062401578\n",
      "7011 : Training: loss:  0.016531818\n",
      "7012 : Training: loss:  0.019895492\n",
      "7013 : Training: loss:  0.07714063\n",
      "7014 : Training: loss:  0.041429907\n",
      "7015 : Training: loss:  0.04513536\n",
      "7016 : Training: loss:  0.07050069\n",
      "7017 : Training: loss:  0.019075178\n",
      "7018 : Training: loss:  0.061267048\n",
      "7019 : Training: loss:  0.040273264\n",
      "7020 : Training: loss:  0.061791584\n",
      "Validation: Loss:  0.061292082  Accuracy:  0.7307692\n",
      "7021 : Training: loss:  0.03442591\n",
      "7022 : Training: loss:  0.020827685\n",
      "7023 : Training: loss:  0.014679492\n",
      "7024 : Training: loss:  0.011795954\n",
      "7025 : Training: loss:  0.03573508\n",
      "7026 : Training: loss:  0.037604164\n",
      "7027 : Training: loss:  0.030181335\n",
      "7028 : Training: loss:  0.08558355\n",
      "7029 : Training: loss:  0.055482145\n",
      "7030 : Training: loss:  0.02665162\n",
      "7031 : Training: loss:  0.03499737\n",
      "7032 : Training: loss:  0.0064891037\n",
      "7033 : Training: loss:  0.05985093\n",
      "7034 : Training: loss:  0.050735444\n",
      "7035 : Training: loss:  0.030036513\n",
      "7036 : Training: loss:  0.0313284\n",
      "7037 : Training: loss:  0.042536285\n",
      "7038 : Training: loss:  0.04179808\n",
      "7039 : Training: loss:  0.04290186\n",
      "7040 : Training: loss:  0.018270344\n",
      "Validation: Loss:  0.061244622  Accuracy:  0.7307692\n",
      "7041 : Training: loss:  0.048609465\n",
      "7042 : Training: loss:  0.04144151\n",
      "7043 : Training: loss:  0.025759347\n",
      "7044 : Training: loss:  0.04619679\n",
      "7045 : Training: loss:  0.043390736\n",
      "7046 : Training: loss:  0.009927773\n",
      "7047 : Training: loss:  0.012650961\n",
      "7048 : Training: loss:  0.021886257\n",
      "7049 : Training: loss:  0.045595318\n",
      "7050 : Training: loss:  0.04654381\n",
      "7051 : Training: loss:  0.03454943\n",
      "7052 : Training: loss:  0.051736243\n",
      "7053 : Training: loss:  0.092884734\n",
      "7054 : Training: loss:  0.014415772\n",
      "7055 : Training: loss:  0.02819083\n",
      "7056 : Training: loss:  0.05566509\n",
      "7057 : Training: loss:  0.06740857\n",
      "7058 : Training: loss:  0.026961517\n",
      "7059 : Training: loss:  0.061132584\n",
      "7060 : Training: loss:  0.043293834\n",
      "Validation: Loss:  0.061209116  Accuracy:  0.7307692\n",
      "7061 : Training: loss:  0.026896743\n",
      "7062 : Training: loss:  0.03507525\n",
      "7063 : Training: loss:  0.053554468\n",
      "7064 : Training: loss:  0.023359852\n",
      "7065 : Training: loss:  0.05661054\n",
      "7066 : Training: loss:  0.04069001\n",
      "7067 : Training: loss:  0.050485607\n",
      "7068 : Training: loss:  0.03143108\n",
      "7069 : Training: loss:  0.038714465\n",
      "7070 : Training: loss:  0.061804064\n",
      "7071 : Training: loss:  0.04468419\n",
      "7072 : Training: loss:  0.022718992\n",
      "7073 : Training: loss:  0.03337731\n",
      "7074 : Training: loss:  0.010013651\n",
      "7075 : Training: loss:  0.04495465\n",
      "7076 : Training: loss:  0.025802935\n",
      "7077 : Training: loss:  0.074471235\n",
      "7078 : Training: loss:  0.05091419\n",
      "7079 : Training: loss:  0.050493624\n",
      "7080 : Training: loss:  0.067178704\n",
      "Validation: Loss:  0.061372768  Accuracy:  0.71153843\n",
      "7081 : Training: loss:  0.049209338\n",
      "7082 : Training: loss:  0.072934106\n",
      "7083 : Training: loss:  0.018492462\n",
      "7084 : Training: loss:  0.051842026\n",
      "7085 : Training: loss:  0.03969777\n",
      "7086 : Training: loss:  0.055973336\n",
      "7087 : Training: loss:  0.069802724\n",
      "7088 : Training: loss:  0.06771167\n",
      "7089 : Training: loss:  0.008624843\n",
      "7090 : Training: loss:  0.046513468\n",
      "7091 : Training: loss:  0.040269684\n",
      "7092 : Training: loss:  0.03255116\n",
      "7093 : Training: loss:  0.08952424\n",
      "7094 : Training: loss:  0.048242375\n",
      "7095 : Training: loss:  0.05814142\n",
      "7096 : Training: loss:  0.026905088\n",
      "7097 : Training: loss:  0.04935563\n",
      "7098 : Training: loss:  0.040664688\n",
      "7099 : Training: loss:  0.061924204\n",
      "7100 : Training: loss:  0.06718932\n",
      "Validation: Loss:  0.061281208  Accuracy:  0.7307692\n",
      "7101 : Training: loss:  0.014738045\n",
      "7102 : Training: loss:  0.068443574\n",
      "7103 : Training: loss:  0.018838374\n",
      "7104 : Training: loss:  0.057066027\n",
      "7105 : Training: loss:  0.03525022\n",
      "7106 : Training: loss:  0.016601317\n",
      "7107 : Training: loss:  0.067679435\n",
      "7108 : Training: loss:  0.012083001\n",
      "7109 : Training: loss:  0.040302902\n",
      "7110 : Training: loss:  0.05570198\n",
      "7111 : Training: loss:  0.03387965\n",
      "7112 : Training: loss:  0.07174336\n",
      "7113 : Training: loss:  0.055593424\n",
      "7114 : Training: loss:  0.035471793\n",
      "7115 : Training: loss:  0.028233923\n",
      "7116 : Training: loss:  0.028685011\n",
      "7117 : Training: loss:  0.042663973\n",
      "7118 : Training: loss:  0.03894496\n",
      "7119 : Training: loss:  0.043177225\n",
      "7120 : Training: loss:  0.053505037\n",
      "Validation: Loss:  0.06124057  Accuracy:  0.71153843\n",
      "7121 : Training: loss:  0.049930353\n",
      "7122 : Training: loss:  0.03819059\n",
      "7123 : Training: loss:  0.03526059\n",
      "7124 : Training: loss:  0.02704606\n",
      "7125 : Training: loss:  0.037145693\n",
      "7126 : Training: loss:  0.038820736\n",
      "7127 : Training: loss:  0.057507657\n",
      "7128 : Training: loss:  0.07595315\n",
      "7129 : Training: loss:  0.093214735\n",
      "7130 : Training: loss:  0.060806297\n",
      "7131 : Training: loss:  0.028744627\n",
      "7132 : Training: loss:  0.007521378\n",
      "7133 : Training: loss:  0.034174602\n",
      "7134 : Training: loss:  0.048913702\n",
      "7135 : Training: loss:  0.033574834\n",
      "7136 : Training: loss:  0.027487652\n",
      "7137 : Training: loss:  0.057636507\n",
      "7138 : Training: loss:  0.028032577\n",
      "7139 : Training: loss:  0.05640037\n",
      "7140 : Training: loss:  0.03563942\n",
      "Validation: Loss:  0.061442178  Accuracy:  0.71153843\n",
      "7141 : Training: loss:  0.06841492\n",
      "7142 : Training: loss:  0.04320162\n",
      "7143 : Training: loss:  0.030907618\n",
      "7144 : Training: loss:  0.037100222\n",
      "7145 : Training: loss:  0.055152677\n",
      "7146 : Training: loss:  0.026598306\n",
      "7147 : Training: loss:  0.02992412\n",
      "7148 : Training: loss:  0.06143\n",
      "7149 : Training: loss:  0.056834165\n",
      "7150 : Training: loss:  0.011952062\n",
      "7151 : Training: loss:  0.041413423\n",
      "7152 : Training: loss:  0.06026511\n",
      "7153 : Training: loss:  0.0069489004\n",
      "7154 : Training: loss:  0.01834211\n",
      "7155 : Training: loss:  0.05025534\n",
      "7156 : Training: loss:  0.039507788\n",
      "7157 : Training: loss:  0.06252434\n",
      "7158 : Training: loss:  0.036116194\n",
      "7159 : Training: loss:  0.0500659\n",
      "7160 : Training: loss:  0.05640397\n",
      "Validation: Loss:  0.061664667  Accuracy:  0.71153843\n",
      "7161 : Training: loss:  0.010444574\n",
      "7162 : Training: loss:  0.061186746\n",
      "7163 : Training: loss:  0.035620123\n",
      "7164 : Training: loss:  0.059225827\n",
      "7165 : Training: loss:  0.016867446\n",
      "7166 : Training: loss:  0.023330078\n",
      "7167 : Training: loss:  0.046733756\n",
      "7168 : Training: loss:  0.042005114\n",
      "7169 : Training: loss:  0.10396009\n",
      "7170 : Training: loss:  0.059020832\n",
      "7171 : Training: loss:  0.058528416\n",
      "7172 : Training: loss:  0.022467216\n",
      "7173 : Training: loss:  0.047464576\n",
      "7174 : Training: loss:  0.020979702\n",
      "7175 : Training: loss:  0.06462578\n",
      "7176 : Training: loss:  0.06360132\n",
      "7177 : Training: loss:  0.07095502\n",
      "7178 : Training: loss:  0.02328238\n",
      "7179 : Training: loss:  0.011007117\n",
      "7180 : Training: loss:  0.03627053\n",
      "Validation: Loss:  0.061547443  Accuracy:  0.71153843\n",
      "7181 : Training: loss:  0.024785101\n",
      "7182 : Training: loss:  0.0076535195\n",
      "7183 : Training: loss:  0.03640622\n",
      "7184 : Training: loss:  0.035769615\n",
      "7185 : Training: loss:  0.03300046\n",
      "7186 : Training: loss:  0.03804633\n",
      "7187 : Training: loss:  0.067541905\n",
      "7188 : Training: loss:  0.05708137\n",
      "7189 : Training: loss:  0.043279245\n",
      "7190 : Training: loss:  0.0072110603\n",
      "7191 : Training: loss:  0.06230805\n",
      "7192 : Training: loss:  0.06534532\n",
      "7193 : Training: loss:  0.04362015\n",
      "7194 : Training: loss:  0.019520732\n",
      "7195 : Training: loss:  0.04284466\n",
      "7196 : Training: loss:  0.031101529\n",
      "7197 : Training: loss:  0.04130718\n",
      "7198 : Training: loss:  0.049265478\n",
      "7199 : Training: loss:  0.042088583\n",
      "7200 : Training: loss:  0.07243341\n",
      "Validation: Loss:  0.060931295  Accuracy:  0.7307692\n",
      "7201 : Training: loss:  0.0259583\n",
      "7202 : Training: loss:  0.06903411\n",
      "7203 : Training: loss:  0.016400231\n",
      "7204 : Training: loss:  0.056315225\n",
      "7205 : Training: loss:  0.05537247\n",
      "7206 : Training: loss:  0.043505836\n",
      "7207 : Training: loss:  0.018640067\n",
      "7208 : Training: loss:  0.073569454\n",
      "7209 : Training: loss:  0.047979347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7210 : Training: loss:  0.034175467\n",
      "7211 : Training: loss:  0.015531331\n",
      "7212 : Training: loss:  0.050455824\n",
      "7213 : Training: loss:  0.024779746\n",
      "7214 : Training: loss:  0.06379863\n",
      "7215 : Training: loss:  0.05221328\n",
      "7216 : Training: loss:  0.047911264\n",
      "7217 : Training: loss:  0.015064407\n",
      "7218 : Training: loss:  0.064256944\n",
      "7219 : Training: loss:  0.042025916\n",
      "7220 : Training: loss:  0.014339398\n",
      "Validation: Loss:  0.0605886  Accuracy:  0.7307692\n",
      "7221 : Training: loss:  0.030351827\n",
      "7222 : Training: loss:  0.06318045\n",
      "7223 : Training: loss:  0.053275347\n",
      "7224 : Training: loss:  0.063853994\n",
      "7225 : Training: loss:  0.040875528\n",
      "7226 : Training: loss:  0.035611544\n",
      "7227 : Training: loss:  0.05964969\n",
      "7228 : Training: loss:  0.015030796\n",
      "7229 : Training: loss:  0.022727761\n",
      "7230 : Training: loss:  0.03847882\n",
      "7231 : Training: loss:  0.020464757\n",
      "7232 : Training: loss:  0.0430007\n",
      "7233 : Training: loss:  0.03371736\n",
      "7234 : Training: loss:  0.034321852\n",
      "7235 : Training: loss:  0.037529875\n",
      "7236 : Training: loss:  0.026644038\n",
      "7237 : Training: loss:  0.022532437\n",
      "7238 : Training: loss:  0.055958897\n",
      "7239 : Training: loss:  0.0363287\n",
      "7240 : Training: loss:  0.04158625\n",
      "Validation: Loss:  0.06023871  Accuracy:  0.71153843\n",
      "7241 : Training: loss:  0.051671077\n",
      "7242 : Training: loss:  0.021203356\n",
      "7243 : Training: loss:  0.050474983\n",
      "7244 : Training: loss:  0.05289955\n",
      "7245 : Training: loss:  0.070401736\n",
      "7246 : Training: loss:  0.020294476\n",
      "7247 : Training: loss:  0.018550614\n",
      "7248 : Training: loss:  0.020745322\n",
      "7249 : Training: loss:  0.02376239\n",
      "7250 : Training: loss:  0.03201033\n",
      "7251 : Training: loss:  0.030581785\n",
      "7252 : Training: loss:  0.027400978\n",
      "7253 : Training: loss:  0.049828295\n",
      "7254 : Training: loss:  0.03456337\n",
      "7255 : Training: loss:  0.09482123\n",
      "7256 : Training: loss:  0.051582236\n",
      "7257 : Training: loss:  0.025864363\n",
      "7258 : Training: loss:  0.053768177\n",
      "7259 : Training: loss:  0.020379337\n",
      "7260 : Training: loss:  0.05851225\n",
      "Validation: Loss:  0.060062874  Accuracy:  0.71153843\n",
      "7261 : Training: loss:  0.014466938\n",
      "7262 : Training: loss:  0.008622752\n",
      "7263 : Training: loss:  0.059089478\n",
      "7264 : Training: loss:  0.047438428\n",
      "7265 : Training: loss:  0.08919235\n",
      "7266 : Training: loss:  0.03561327\n",
      "7267 : Training: loss:  0.028161263\n",
      "7268 : Training: loss:  0.03584675\n",
      "7269 : Training: loss:  0.01805233\n",
      "7270 : Training: loss:  0.025344225\n",
      "7271 : Training: loss:  0.029776983\n",
      "7272 : Training: loss:  0.049366467\n",
      "7273 : Training: loss:  0.045871474\n",
      "7274 : Training: loss:  0.029038217\n",
      "7275 : Training: loss:  0.029749237\n",
      "7276 : Training: loss:  0.057971112\n",
      "7277 : Training: loss:  0.054040596\n",
      "7278 : Training: loss:  0.031845003\n",
      "7279 : Training: loss:  0.066807\n",
      "7280 : Training: loss:  0.018152848\n",
      "Validation: Loss:  0.06019404  Accuracy:  0.71153843\n",
      "7281 : Training: loss:  0.08799957\n",
      "7282 : Training: loss:  0.038073715\n",
      "7283 : Training: loss:  0.051811717\n",
      "7284 : Training: loss:  0.051031116\n",
      "7285 : Training: loss:  0.018489793\n",
      "7286 : Training: loss:  0.053258203\n",
      "7287 : Training: loss:  0.044263676\n",
      "7288 : Training: loss:  0.017871091\n",
      "7289 : Training: loss:  0.022748634\n",
      "7290 : Training: loss:  0.061503597\n",
      "7291 : Training: loss:  0.05684877\n",
      "7292 : Training: loss:  0.0752261\n",
      "7293 : Training: loss:  0.01837027\n",
      "7294 : Training: loss:  0.01868693\n",
      "7295 : Training: loss:  0.0072883926\n",
      "7296 : Training: loss:  0.011799488\n",
      "7297 : Training: loss:  0.0285719\n",
      "7298 : Training: loss:  0.05039472\n",
      "7299 : Training: loss:  0.021743502\n",
      "7300 : Training: loss:  0.03916401\n",
      "Validation: Loss:  0.06006935  Accuracy:  0.71153843\n",
      "7301 : Training: loss:  0.031569388\n",
      "7302 : Training: loss:  0.04465635\n",
      "7303 : Training: loss:  0.038132176\n",
      "7304 : Training: loss:  0.01624795\n",
      "7305 : Training: loss:  0.041884087\n",
      "7306 : Training: loss:  0.0151175\n",
      "7307 : Training: loss:  0.05899958\n",
      "7308 : Training: loss:  0.034378037\n",
      "7309 : Training: loss:  0.035943154\n",
      "7310 : Training: loss:  0.011305996\n",
      "7311 : Training: loss:  0.025287928\n",
      "7312 : Training: loss:  0.05927515\n",
      "7313 : Training: loss:  0.015912455\n",
      "7314 : Training: loss:  0.043074206\n",
      "7315 : Training: loss:  0.064644046\n",
      "7316 : Training: loss:  0.05452241\n",
      "7317 : Training: loss:  0.030119797\n",
      "7318 : Training: loss:  0.027079176\n",
      "7319 : Training: loss:  0.04418873\n",
      "7320 : Training: loss:  0.023424735\n",
      "Validation: Loss:  0.060434658  Accuracy:  0.71153843\n",
      "7321 : Training: loss:  0.026281392\n",
      "7322 : Training: loss:  0.039220255\n",
      "7323 : Training: loss:  0.034066793\n",
      "7324 : Training: loss:  0.07061337\n",
      "7325 : Training: loss:  0.03969893\n",
      "7326 : Training: loss:  0.06784027\n",
      "7327 : Training: loss:  0.045738928\n",
      "7328 : Training: loss:  0.037996702\n",
      "7329 : Training: loss:  0.044500835\n",
      "7330 : Training: loss:  0.036799017\n",
      "7331 : Training: loss:  0.055961523\n",
      "7332 : Training: loss:  0.03972407\n",
      "7333 : Training: loss:  0.076522\n",
      "7334 : Training: loss:  0.052169688\n",
      "7335 : Training: loss:  0.038519766\n",
      "7336 : Training: loss:  0.04343919\n",
      "7337 : Training: loss:  0.01978511\n",
      "7338 : Training: loss:  0.01150092\n",
      "7339 : Training: loss:  0.03833588\n",
      "7340 : Training: loss:  0.019192634\n",
      "Validation: Loss:  0.060327567  Accuracy:  0.71153843\n",
      "7341 : Training: loss:  0.015611329\n",
      "7342 : Training: loss:  0.031139113\n",
      "7343 : Training: loss:  0.054195635\n",
      "7344 : Training: loss:  0.023935225\n",
      "7345 : Training: loss:  0.061956517\n",
      "7346 : Training: loss:  0.03719526\n",
      "7347 : Training: loss:  0.041027237\n",
      "7348 : Training: loss:  0.038702115\n",
      "7349 : Training: loss:  0.015186739\n",
      "7350 : Training: loss:  0.055058032\n",
      "7351 : Training: loss:  0.046746913\n",
      "7352 : Training: loss:  0.0284763\n",
      "7353 : Training: loss:  0.011078352\n",
      "7354 : Training: loss:  0.028746866\n",
      "7355 : Training: loss:  0.044272576\n",
      "7356 : Training: loss:  0.0054342393\n",
      "7357 : Training: loss:  0.03111056\n",
      "7358 : Training: loss:  0.034803595\n",
      "7359 : Training: loss:  0.052753147\n",
      "7360 : Training: loss:  0.045565125\n",
      "Validation: Loss:  0.060204845  Accuracy:  0.71153843\n",
      "7361 : Training: loss:  0.052285824\n",
      "7362 : Training: loss:  0.050822064\n",
      "7363 : Training: loss:  0.059948605\n",
      "7364 : Training: loss:  0.07015861\n",
      "7365 : Training: loss:  0.039640862\n",
      "7366 : Training: loss:  0.052210554\n",
      "7367 : Training: loss:  0.03754876\n",
      "7368 : Training: loss:  0.04229641\n",
      "7369 : Training: loss:  0.039755896\n",
      "7370 : Training: loss:  0.05326772\n",
      "7371 : Training: loss:  0.048575208\n",
      "7372 : Training: loss:  0.065456115\n",
      "7373 : Training: loss:  0.036131337\n",
      "7374 : Training: loss:  0.033323947\n",
      "7375 : Training: loss:  0.020363294\n",
      "7376 : Training: loss:  0.046862986\n",
      "7377 : Training: loss:  0.027204594\n",
      "7378 : Training: loss:  0.011466585\n",
      "7379 : Training: loss:  0.056639794\n",
      "7380 : Training: loss:  0.03737492\n",
      "Validation: Loss:  0.059920695  Accuracy:  0.75\n",
      "7381 : Training: loss:  0.07498217\n",
      "7382 : Training: loss:  0.03267357\n",
      "7383 : Training: loss:  0.034738664\n",
      "7384 : Training: loss:  0.029359363\n",
      "7385 : Training: loss:  0.020730257\n",
      "7386 : Training: loss:  0.013875558\n",
      "7387 : Training: loss:  0.04156092\n",
      "7388 : Training: loss:  0.03735374\n",
      "7389 : Training: loss:  0.049899545\n",
      "7390 : Training: loss:  0.039895963\n",
      "7391 : Training: loss:  0.05947838\n",
      "7392 : Training: loss:  0.014541792\n",
      "7393 : Training: loss:  0.046122674\n",
      "7394 : Training: loss:  0.01882046\n",
      "7395 : Training: loss:  0.03260068\n",
      "7396 : Training: loss:  0.04840839\n",
      "7397 : Training: loss:  0.06435893\n",
      "7398 : Training: loss:  0.0699439\n",
      "7399 : Training: loss:  0.058557943\n",
      "7400 : Training: loss:  0.03706796\n",
      "Validation: Loss:  0.05990166  Accuracy:  0.7692308\n",
      "7401 : Training: loss:  0.058794595\n",
      "7402 : Training: loss:  0.040890746\n",
      "7403 : Training: loss:  0.052881725\n",
      "7404 : Training: loss:  0.019148616\n",
      "7405 : Training: loss:  0.037355945\n",
      "7406 : Training: loss:  0.019216765\n",
      "7407 : Training: loss:  0.006306794\n",
      "7408 : Training: loss:  0.046580058\n",
      "7409 : Training: loss:  0.029593622\n",
      "7410 : Training: loss:  0.028588498\n",
      "7411 : Training: loss:  0.05506811\n",
      "7412 : Training: loss:  0.080373354\n",
      "7413 : Training: loss:  0.035341118\n",
      "7414 : Training: loss:  0.039809603\n",
      "7415 : Training: loss:  0.041933056\n",
      "7416 : Training: loss:  0.050993275\n",
      "7417 : Training: loss:  0.015586741\n",
      "7418 : Training: loss:  0.043459054\n",
      "7419 : Training: loss:  0.055398673\n",
      "7420 : Training: loss:  0.017963594\n",
      "Validation: Loss:  0.06013874  Accuracy:  0.75\n",
      "7421 : Training: loss:  0.02144076\n",
      "7422 : Training: loss:  0.035129637\n",
      "7423 : Training: loss:  0.037914198\n",
      "7424 : Training: loss:  0.04215247\n",
      "7425 : Training: loss:  0.045632157\n",
      "7426 : Training: loss:  0.019714952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7427 : Training: loss:  0.04495473\n",
      "7428 : Training: loss:  0.020000016\n",
      "7429 : Training: loss:  0.06280974\n",
      "7430 : Training: loss:  0.034912188\n",
      "7431 : Training: loss:  0.036290117\n",
      "7432 : Training: loss:  0.038493354\n",
      "7433 : Training: loss:  0.029239753\n",
      "7434 : Training: loss:  0.045741532\n",
      "7435 : Training: loss:  0.03157487\n",
      "7436 : Training: loss:  0.04990259\n",
      "7437 : Training: loss:  0.028820524\n",
      "7438 : Training: loss:  0.008181844\n",
      "7439 : Training: loss:  0.020912316\n",
      "7440 : Training: loss:  0.050318602\n",
      "Validation: Loss:  0.06001605  Accuracy:  0.75\n",
      "7441 : Training: loss:  0.018750964\n",
      "7442 : Training: loss:  0.05895484\n",
      "7443 : Training: loss:  0.05313604\n",
      "7444 : Training: loss:  0.0753257\n",
      "7445 : Training: loss:  0.027938465\n",
      "7446 : Training: loss:  0.04936075\n",
      "7447 : Training: loss:  0.074952446\n",
      "7448 : Training: loss:  0.059296336\n",
      "7449 : Training: loss:  0.033551864\n",
      "7450 : Training: loss:  0.023168929\n",
      "7451 : Training: loss:  0.043698408\n",
      "7452 : Training: loss:  0.03501759\n",
      "7453 : Training: loss:  0.016392209\n",
      "7454 : Training: loss:  0.041192573\n",
      "7455 : Training: loss:  0.035789583\n",
      "7456 : Training: loss:  0.037139278\n",
      "7457 : Training: loss:  0.008377363\n",
      "7458 : Training: loss:  0.02773644\n",
      "7459 : Training: loss:  0.017581766\n",
      "7460 : Training: loss:  0.03153693\n",
      "Validation: Loss:  0.060019128  Accuracy:  0.75\n",
      "7461 : Training: loss:  0.039787475\n",
      "7462 : Training: loss:  0.053804763\n",
      "7463 : Training: loss:  0.032785382\n",
      "7464 : Training: loss:  0.039589055\n",
      "7465 : Training: loss:  0.058585215\n",
      "7466 : Training: loss:  0.033436365\n",
      "7467 : Training: loss:  0.010586405\n",
      "7468 : Training: loss:  0.018468086\n",
      "7469 : Training: loss:  0.01760505\n",
      "7470 : Training: loss:  0.03706229\n",
      "7471 : Training: loss:  0.038013767\n",
      "7472 : Training: loss:  0.036638148\n",
      "7473 : Training: loss:  0.028895628\n",
      "7474 : Training: loss:  0.013443047\n",
      "7475 : Training: loss:  0.055193417\n",
      "7476 : Training: loss:  0.052981038\n",
      "7477 : Training: loss:  0.043053064\n",
      "7478 : Training: loss:  0.03981105\n",
      "7479 : Training: loss:  0.03150633\n",
      "7480 : Training: loss:  0.043923147\n",
      "Validation: Loss:  0.060268924  Accuracy:  0.7307692\n",
      "7481 : Training: loss:  0.062577166\n",
      "7482 : Training: loss:  0.04119249\n",
      "7483 : Training: loss:  0.03472654\n",
      "7484 : Training: loss:  0.04737875\n",
      "7485 : Training: loss:  0.0649309\n",
      "7486 : Training: loss:  0.03272616\n",
      "7487 : Training: loss:  0.04008034\n",
      "7488 : Training: loss:  0.036304694\n",
      "7489 : Training: loss:  0.024536375\n",
      "7490 : Training: loss:  0.052970525\n",
      "7491 : Training: loss:  0.008810313\n",
      "7492 : Training: loss:  0.027337285\n",
      "7493 : Training: loss:  0.06309704\n",
      "7494 : Training: loss:  0.06278927\n",
      "7495 : Training: loss:  0.039117552\n",
      "7496 : Training: loss:  0.055710986\n",
      "7497 : Training: loss:  0.05080046\n",
      "7498 : Training: loss:  0.061742812\n",
      "7499 : Training: loss:  0.042224362\n",
      "7500 : Training: loss:  0.05823223\n",
      "Validation: Loss:  0.06027795  Accuracy:  0.71153843\n",
      "7501 : Training: loss:  0.045885053\n",
      "7502 : Training: loss:  0.07641111\n",
      "7503 : Training: loss:  0.015953489\n",
      "7504 : Training: loss:  0.06409258\n",
      "7505 : Training: loss:  0.05695558\n",
      "7506 : Training: loss:  0.033071067\n",
      "7507 : Training: loss:  0.0634149\n",
      "7508 : Training: loss:  0.026376689\n",
      "7509 : Training: loss:  0.020037008\n",
      "7510 : Training: loss:  0.03056842\n",
      "7511 : Training: loss:  0.019914575\n",
      "7512 : Training: loss:  0.039034527\n",
      "7513 : Training: loss:  0.011343448\n",
      "7514 : Training: loss:  0.03858528\n",
      "7515 : Training: loss:  0.015336383\n",
      "7516 : Training: loss:  0.059835766\n",
      "7517 : Training: loss:  0.062486652\n",
      "7518 : Training: loss:  0.03952604\n",
      "7519 : Training: loss:  0.04744632\n",
      "7520 : Training: loss:  0.052933358\n",
      "Validation: Loss:  0.059832793  Accuracy:  0.71153843\n",
      "7521 : Training: loss:  0.05284795\n",
      "7522 : Training: loss:  0.020911722\n",
      "7523 : Training: loss:  0.06129572\n",
      "7524 : Training: loss:  0.03888967\n",
      "7525 : Training: loss:  0.045642097\n",
      "7526 : Training: loss:  0.057327207\n",
      "7527 : Training: loss:  0.04335683\n",
      "7528 : Training: loss:  0.040734246\n",
      "7529 : Training: loss:  0.039518386\n",
      "7530 : Training: loss:  0.023796044\n",
      "7531 : Training: loss:  0.018277131\n",
      "7532 : Training: loss:  0.028754786\n",
      "7533 : Training: loss:  0.05890244\n",
      "7534 : Training: loss:  0.04269391\n",
      "7535 : Training: loss:  0.075514965\n",
      "7536 : Training: loss:  0.028689401\n",
      "7537 : Training: loss:  0.03840088\n",
      "7538 : Training: loss:  0.043443967\n",
      "7539 : Training: loss:  0.050463963\n",
      "7540 : Training: loss:  0.047894653\n",
      "Validation: Loss:  0.05972764  Accuracy:  0.7307692\n",
      "7541 : Training: loss:  0.03515921\n",
      "7542 : Training: loss:  0.051844247\n",
      "7543 : Training: loss:  0.03021391\n",
      "7544 : Training: loss:  0.02582495\n",
      "7545 : Training: loss:  0.05905911\n",
      "7546 : Training: loss:  0.09192061\n",
      "7547 : Training: loss:  0.043753643\n",
      "7548 : Training: loss:  0.025448339\n",
      "7549 : Training: loss:  0.038514547\n",
      "7550 : Training: loss:  0.025732156\n",
      "7551 : Training: loss:  0.03636742\n",
      "7552 : Training: loss:  0.03697351\n",
      "7553 : Training: loss:  0.025569113\n",
      "7554 : Training: loss:  0.027973745\n",
      "7555 : Training: loss:  0.051834293\n",
      "7556 : Training: loss:  0.028099094\n",
      "7557 : Training: loss:  0.041102957\n",
      "7558 : Training: loss:  0.0654479\n",
      "7559 : Training: loss:  0.010016127\n",
      "7560 : Training: loss:  0.030904552\n",
      "Validation: Loss:  0.059447184  Accuracy:  0.7307692\n",
      "7561 : Training: loss:  0.025660409\n",
      "7562 : Training: loss:  0.038136285\n",
      "7563 : Training: loss:  0.043214418\n",
      "7564 : Training: loss:  0.03328388\n",
      "7565 : Training: loss:  0.030291773\n",
      "7566 : Training: loss:  0.032273766\n",
      "7567 : Training: loss:  0.010746535\n",
      "7568 : Training: loss:  0.029111512\n",
      "7569 : Training: loss:  0.03625044\n",
      "7570 : Training: loss:  0.07354597\n",
      "7571 : Training: loss:  0.018202651\n",
      "7572 : Training: loss:  0.02311394\n",
      "7573 : Training: loss:  0.03724174\n",
      "7574 : Training: loss:  0.053946868\n",
      "7575 : Training: loss:  0.05278377\n",
      "7576 : Training: loss:  0.051015545\n",
      "7577 : Training: loss:  0.024903106\n",
      "7578 : Training: loss:  0.0148428315\n",
      "7579 : Training: loss:  0.06101489\n",
      "7580 : Training: loss:  0.012581323\n",
      "Validation: Loss:  0.05974468  Accuracy:  0.75\n",
      "7581 : Training: loss:  0.01846119\n",
      "7582 : Training: loss:  0.044463627\n",
      "7583 : Training: loss:  0.01694717\n",
      "7584 : Training: loss:  0.063511044\n",
      "7585 : Training: loss:  0.03868167\n",
      "7586 : Training: loss:  0.052225947\n",
      "7587 : Training: loss:  0.02904756\n",
      "7588 : Training: loss:  0.082126476\n",
      "7589 : Training: loss:  0.04733812\n",
      "7590 : Training: loss:  0.057027243\n",
      "7591 : Training: loss:  0.016647914\n",
      "7592 : Training: loss:  0.02143819\n",
      "7593 : Training: loss:  0.07479959\n",
      "7594 : Training: loss:  0.01129991\n",
      "7595 : Training: loss:  0.040099915\n",
      "7596 : Training: loss:  0.06083676\n",
      "7597 : Training: loss:  0.022394096\n",
      "7598 : Training: loss:  0.05923091\n",
      "7599 : Training: loss:  0.0105468165\n",
      "7600 : Training: loss:  0.041449934\n",
      "Validation: Loss:  0.059401378  Accuracy:  0.75\n",
      "7601 : Training: loss:  0.030993523\n",
      "7602 : Training: loss:  0.03984948\n",
      "7603 : Training: loss:  0.016764196\n",
      "7604 : Training: loss:  0.031914514\n",
      "7605 : Training: loss:  0.046619315\n",
      "7606 : Training: loss:  0.025906013\n",
      "7607 : Training: loss:  0.040011447\n",
      "7608 : Training: loss:  0.030004585\n",
      "7609 : Training: loss:  0.033304445\n",
      "7610 : Training: loss:  0.029616522\n",
      "7611 : Training: loss:  0.043870974\n",
      "7612 : Training: loss:  0.041531492\n",
      "7613 : Training: loss:  0.03883234\n",
      "7614 : Training: loss:  0.02185332\n",
      "7615 : Training: loss:  0.037793566\n",
      "7616 : Training: loss:  0.0626957\n",
      "7617 : Training: loss:  0.030238155\n",
      "7618 : Training: loss:  0.035620812\n",
      "7619 : Training: loss:  0.02024257\n",
      "7620 : Training: loss:  0.013792025\n",
      "Validation: Loss:  0.059140023  Accuracy:  0.78846157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4273, 0.0215, 0.034, 0.0064, 0.0019, 0.0011...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.05914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5677, 0.0463, 0.0027, 0.0018, 0.0198, 0.021...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.054, 0.116, 0.0033, 0.0032, 0.0005, 0.0091,...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0077, 0.0285, 0.0034, 1e-04, 0.0067, 0.0221...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0791, 0.061, 0.0068, 0.0024, 0.0025, 0.0027...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0135, 0.0016, 0.4261, 0.4105, 0.0006, 0.001...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0065, 0.0009, 0.0822, 0.9617, 0.0013, 1e-04...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0048, 0.0009, 0.0584, 0.9404, 0.0017, 1e-04...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0257, 0.0007, 0.1236, 0.3587, 0.005, 0.0026...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.2938, 0.0198, 1e-04, 0.0002, 0.7476, 0.0439...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0265, 0.1961, 1e-04, 0.0005, 0.4891, 0.1133...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0044, 0.0862, 1e-04, 1e-04, 0.003, 0.4528, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-04, 0.0594, 0.0002, 1e-04, 0.017, 0.0103, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0, 0.0011, 0.0083, 0.0032, 1e-04, 0.0, 0.00...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0158, 0.0148, 0.0016, 0.0003, 0.2158, 0.002...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.3114, 0.0209, 0.0003, 0.0, 0.0026, 0.0014, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0073, 0.0017, 0.0983, 0.0161, 0.0021, 0.004...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0011, 0.0014, 0.0042, 0.0023, 0.0025, 0.002...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0101, 0.0797, 0.0006, 0.0002, 0.0395, 0.172...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0761, 0.1361, 0.0008, 1e-04, 0.0703, 0.1732...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0396, 0.0049, 0.0091, 1e-04, 0.0013, 0.01, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0007, 0.0157, 0.0004, 0.0025, 0.0004, 0.000...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0093, 0.003, 0.0008, 0.0, 0.0031, 0.0154, 0...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0076, 0.1521, 0.0003, 0.0, 0.0185, 0.5609, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0819, 0.0075, 0.1509, 0.0128, 0.0026, 0.008...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0474, 0.0093, 0.0364, 0.0095, 0.001, 1e-04,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0237, 0.0183, 0.0036, 0.0004, 0.0666, 0.000...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0035, 0.0052, 0.0052, 0.0013, 0.0046, 0.002...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0, 0.0006, 0.0005, 0.0037, 0.0002, 0.0, 0.0...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0002, 0.0022, 0.0002, 0.0005, 0.0009, 1e-04...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0, 0.0009, 0.0004, 0.0, 0.0002, 0.0002, 0.0...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0034, 0.0014, 0.009, 0.0031, 0.0028, 0.0046...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0023, 0.0012, 0.0023, 0.0106, 0.0008, 0.000...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.04, 0.0215, 0.0059, 0.0031, 0.0054, 0.0023,...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0115, 0.0385, 0.0014, 1e-04, 0.0037, 0.0262...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0779, 0.005, 0.0139, 0.0153, 0.0006, 0.0002...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0009, 0.0098, 0.0067, 0.0264, 0.0004, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 0.0002, 0.0002, 0.0008, 0.0, 0.0015, 0.0...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0002, 0.0066, 0.009, 0.0033, 0.0205, 1e-04,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0016, 0.0057, 0.0216, 0.0167, 0.033, 0.0006...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0027, 0.021, 0.0056, 0.001, 0.0008, 1e-04, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0086, 0.0045, 0.056, 0.0688, 0.0014, 0.0006...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0007, 0.0079, 0.0017, 0.0402, 1e-04, 0.0022...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0, 0.0055, 0.0002, 0.0104, 0.0, 1e-04, 1e-0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0, 0.0021, 1e-04, 0.0076, 0.0, 0.0, 0.0, 1e...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[1e-04, 0.0029, 0.0006, 0.0333, 0.0, 1e-04, 0....</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0002, 0.0002, 0.0143, 1e-04, 0.0008, 0.0003...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2653, 0.0032, 0.0493, 0.0131, 0.0012, 0.002...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0015, 0.0802, 0.0002, 0.0, 0.0631, 0.0035, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0179, 0.3781, 0.0, 0.0, 0.0033, 0.0364, 0.0...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0913, 0.0022, 0.0029, 0.0003, 0.0003, 0.001...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0359, 0.0119, 0.0015, 0.0005, 0.0013, 0.046...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4273, 0.0215, 0.034, 0.0064, 0.0019, 0.0011...               0   \n",
       "1   [0.5677, 0.0463, 0.0027, 0.0018, 0.0198, 0.021...               0   \n",
       "2   [0.054, 0.116, 0.0033, 0.0032, 0.0005, 0.0091,...               0   \n",
       "3   [0.0077, 0.0285, 0.0034, 1e-04, 0.0067, 0.0221...               1   \n",
       "4   [0.0791, 0.061, 0.0068, 0.0024, 0.0025, 0.0027...               1   \n",
       "5   [0.0135, 0.0016, 0.4261, 0.4105, 0.0006, 0.001...               2   \n",
       "6   [0.0065, 0.0009, 0.0822, 0.9617, 0.0013, 1e-04...               3   \n",
       "7   [0.0048, 0.0009, 0.0584, 0.9404, 0.0017, 1e-04...               3   \n",
       "8   [0.0257, 0.0007, 0.1236, 0.3587, 0.005, 0.0026...               3   \n",
       "9   [0.2938, 0.0198, 1e-04, 0.0002, 0.7476, 0.0439...               4   \n",
       "10  [0.0265, 0.1961, 1e-04, 0.0005, 0.4891, 0.1133...               4   \n",
       "11  [0.0044, 0.0862, 1e-04, 1e-04, 0.003, 0.4528, ...               5   \n",
       "12  [1e-04, 0.0594, 0.0002, 1e-04, 0.017, 0.0103, ...               6   \n",
       "13  [0.0, 0.0011, 0.0083, 0.0032, 1e-04, 0.0, 0.00...               7   \n",
       "14  [0.0158, 0.0148, 0.0016, 0.0003, 0.2158, 0.002...               8   \n",
       "15  [0.3114, 0.0209, 0.0003, 0.0, 0.0026, 0.0014, ...               8   \n",
       "16  [0.0073, 0.0017, 0.0983, 0.0161, 0.0021, 0.004...               9   \n",
       "17  [0.0011, 0.0014, 0.0042, 0.0023, 0.0025, 0.002...               9   \n",
       "18  [0.0101, 0.0797, 0.0006, 0.0002, 0.0395, 0.172...              10   \n",
       "19  [0.0761, 0.1361, 0.0008, 1e-04, 0.0703, 0.1732...              10   \n",
       "20  [0.0396, 0.0049, 0.0091, 1e-04, 0.0013, 0.01, ...              11   \n",
       "21  [0.0007, 0.0157, 0.0004, 0.0025, 0.0004, 0.000...              11   \n",
       "22  [0.0093, 0.003, 0.0008, 0.0, 0.0031, 0.0154, 0...              12   \n",
       "23  [0.0076, 0.1521, 0.0003, 0.0, 0.0185, 0.5609, ...              13   \n",
       "24  [0.0819, 0.0075, 0.1509, 0.0128, 0.0026, 0.008...              13   \n",
       "25  [0.0474, 0.0093, 0.0364, 0.0095, 0.001, 1e-04,...              14   \n",
       "26  [0.0237, 0.0183, 0.0036, 0.0004, 0.0666, 0.000...              14   \n",
       "27  [0.0035, 0.0052, 0.0052, 0.0013, 0.0046, 0.002...              15   \n",
       "28  [0.0, 0.0006, 0.0005, 0.0037, 0.0002, 0.0, 0.0...              15   \n",
       "29  [0.0002, 0.0022, 0.0002, 0.0005, 0.0009, 1e-04...              15   \n",
       "30  [0.0, 0.0009, 0.0004, 0.0, 0.0002, 0.0002, 0.0...              16   \n",
       "31  [0.0034, 0.0014, 0.009, 0.0031, 0.0028, 0.0046...              17   \n",
       "32  [0.0023, 0.0012, 0.0023, 0.0106, 0.0008, 0.000...              17   \n",
       "33  [0.04, 0.0215, 0.0059, 0.0031, 0.0054, 0.0023,...              18   \n",
       "34  [0.0115, 0.0385, 0.0014, 1e-04, 0.0037, 0.0262...              19   \n",
       "35  [0.0779, 0.005, 0.0139, 0.0153, 0.0006, 0.0002...              20   \n",
       "36  [0.0009, 0.0098, 0.0067, 0.0264, 0.0004, 0.021...              21   \n",
       "37  [0.0, 0.0002, 0.0002, 0.0008, 0.0, 0.0015, 0.0...              21   \n",
       "38  [0.0002, 0.0066, 0.009, 0.0033, 0.0205, 1e-04,...              22   \n",
       "39  [0.0016, 0.0057, 0.0216, 0.0167, 0.033, 0.0006...              22   \n",
       "40  [0.0027, 0.021, 0.0056, 0.001, 0.0008, 1e-04, ...              22   \n",
       "41  [0.0086, 0.0045, 0.056, 0.0688, 0.0014, 0.0006...              22   \n",
       "42  [0.0007, 0.0079, 0.0017, 0.0402, 1e-04, 0.0022...              23   \n",
       "43  [0.0, 0.0055, 0.0002, 0.0104, 0.0, 1e-04, 1e-0...              23   \n",
       "44  [0.0, 0.0021, 1e-04, 0.0076, 0.0, 0.0, 0.0, 1e...              23   \n",
       "45  [1e-04, 0.0029, 0.0006, 0.0333, 0.0, 1e-04, 0....              23   \n",
       "46  [0.0002, 0.0002, 0.0143, 1e-04, 0.0008, 0.0003...              24   \n",
       "47  [0.2653, 0.0032, 0.0493, 0.0131, 0.0012, 0.002...              24   \n",
       "48  [0.0015, 0.0802, 0.0002, 0.0, 0.0631, 0.0035, ...              25   \n",
       "49  [0.0179, 0.3781, 0.0, 0.0, 0.0033, 0.0364, 0.0...              25   \n",
       "50  [0.0913, 0.0022, 0.0029, 0.0003, 0.0003, 0.001...              26   \n",
       "51  [0.0359, 0.0119, 0.0015, 0.0005, 0.0013, 0.046...              26   \n",
       "\n",
       "    Predicted labels  Accuracy     Loss  \n",
       "0                  0  0.788462  0.05914  \n",
       "1                  0       NaN      NaN  \n",
       "2                 19       NaN      NaN  \n",
       "3                 10       NaN      NaN  \n",
       "4                  0       NaN      NaN  \n",
       "5                  2       NaN      NaN  \n",
       "6                  3       NaN      NaN  \n",
       "7                  3       NaN      NaN  \n",
       "8                  3       NaN      NaN  \n",
       "9                  4       NaN      NaN  \n",
       "10                 4       NaN      NaN  \n",
       "11                 5       NaN      NaN  \n",
       "12                17       NaN      NaN  \n",
       "13                 7       NaN      NaN  \n",
       "14                 8       NaN      NaN  \n",
       "15                 8       NaN      NaN  \n",
       "16                 9       NaN      NaN  \n",
       "17                 9       NaN      NaN  \n",
       "18                10       NaN      NaN  \n",
       "19                10       NaN      NaN  \n",
       "20                11       NaN      NaN  \n",
       "21                11       NaN      NaN  \n",
       "22                12       NaN      NaN  \n",
       "23                 5       NaN      NaN  \n",
       "24                13       NaN      NaN  \n",
       "25                14       NaN      NaN  \n",
       "26                13       NaN      NaN  \n",
       "27                10       NaN      NaN  \n",
       "28                15       NaN      NaN  \n",
       "29                15       NaN      NaN  \n",
       "30                16       NaN      NaN  \n",
       "31                17       NaN      NaN  \n",
       "32                17       NaN      NaN  \n",
       "33                11       NaN      NaN  \n",
       "34                19       NaN      NaN  \n",
       "35                20       NaN      NaN  \n",
       "36                21       NaN      NaN  \n",
       "37                21       NaN      NaN  \n",
       "38                22       NaN      NaN  \n",
       "39                22       NaN      NaN  \n",
       "40                11       NaN      NaN  \n",
       "41                22       NaN      NaN  \n",
       "42                23       NaN      NaN  \n",
       "43                23       NaN      NaN  \n",
       "44                23       NaN      NaN  \n",
       "45                23       NaN      NaN  \n",
       "46                24       NaN      NaN  \n",
       "47                 0       NaN      NaN  \n",
       "48                25       NaN      NaN  \n",
       "49                 1       NaN      NaN  \n",
       "50                26       NaN      NaN  \n",
       "51                26       NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7621 : Training: loss:  0.0425324\n",
      "7622 : Training: loss:  0.04043937\n",
      "7623 : Training: loss:  0.027403358\n",
      "7624 : Training: loss:  0.03092313\n",
      "7625 : Training: loss:  0.03715763\n",
      "7626 : Training: loss:  0.02728479\n",
      "7627 : Training: loss:  0.023274995\n",
      "7628 : Training: loss:  0.04372248\n",
      "7629 : Training: loss:  0.060264807\n",
      "7630 : Training: loss:  0.04160091\n",
      "7631 : Training: loss:  0.06632155\n",
      "7632 : Training: loss:  0.056736335\n",
      "7633 : Training: loss:  0.04835921\n",
      "7634 : Training: loss:  0.025610976\n",
      "7635 : Training: loss:  0.061339706\n",
      "7636 : Training: loss:  0.04328279\n",
      "7637 : Training: loss:  0.02469335\n",
      "7638 : Training: loss:  0.04851029\n",
      "7639 : Training: loss:  0.050209876\n",
      "7640 : Training: loss:  0.069264084\n",
      "Validation: Loss:  0.059050366  Accuracy:  0.7692308\n",
      "7641 : Training: loss:  0.040691957\n",
      "7642 : Training: loss:  0.06193382\n",
      "7643 : Training: loss:  0.044950627\n",
      "7644 : Training: loss:  0.028855443\n",
      "7645 : Training: loss:  0.049770992\n",
      "7646 : Training: loss:  0.048434347\n",
      "7647 : Training: loss:  0.040994376\n",
      "7648 : Training: loss:  0.024145722\n",
      "7649 : Training: loss:  0.047232002\n",
      "7650 : Training: loss:  0.058620647\n",
      "7651 : Training: loss:  0.06853117\n",
      "7652 : Training: loss:  0.030475382\n",
      "7653 : Training: loss:  0.03805594\n",
      "7654 : Training: loss:  0.06411442\n",
      "7655 : Training: loss:  0.062160064\n",
      "7656 : Training: loss:  0.047152378\n",
      "7657 : Training: loss:  0.054760266\n",
      "7658 : Training: loss:  0.028262708\n",
      "7659 : Training: loss:  0.033581838\n",
      "7660 : Training: loss:  0.04624735\n",
      "Validation: Loss:  0.058855813  Accuracy:  0.7692308\n",
      "7661 : Training: loss:  0.026425565\n",
      "7662 : Training: loss:  0.032660585\n",
      "7663 : Training: loss:  0.04543564\n",
      "7664 : Training: loss:  0.041516237\n",
      "7665 : Training: loss:  0.017882772\n",
      "7666 : Training: loss:  0.039363425\n",
      "7667 : Training: loss:  0.04603168\n",
      "7668 : Training: loss:  0.043407496\n",
      "7669 : Training: loss:  0.02488347\n",
      "7670 : Training: loss:  0.061197955\n",
      "7671 : Training: loss:  0.043082025\n",
      "7672 : Training: loss:  0.05907233\n",
      "7673 : Training: loss:  0.031519514\n",
      "7674 : Training: loss:  0.023589449\n",
      "7675 : Training: loss:  0.06083248\n",
      "7676 : Training: loss:  0.03483649\n",
      "7677 : Training: loss:  0.02171879\n",
      "7678 : Training: loss:  0.061696425\n",
      "7679 : Training: loss:  0.04272835\n",
      "7680 : Training: loss:  0.05151672\n",
      "Validation: Loss:  0.05893621  Accuracy:  0.7692308\n",
      "7681 : Training: loss:  0.03309571\n",
      "7682 : Training: loss:  0.030429391\n",
      "7683 : Training: loss:  0.05376827\n",
      "7684 : Training: loss:  0.053750347\n",
      "7685 : Training: loss:  0.021517059\n",
      "7686 : Training: loss:  0.055862706\n",
      "7687 : Training: loss:  0.006416341\n",
      "7688 : Training: loss:  0.011220663\n",
      "7689 : Training: loss:  0.02260246\n",
      "7690 : Training: loss:  0.01948551\n",
      "7691 : Training: loss:  0.0391239\n",
      "7692 : Training: loss:  0.016846253\n",
      "7693 : Training: loss:  0.04005775\n",
      "7694 : Training: loss:  0.028257335\n",
      "7695 : Training: loss:  0.06242275\n",
      "7696 : Training: loss:  0.019459317\n",
      "7697 : Training: loss:  0.04282309\n",
      "7698 : Training: loss:  0.020669265\n",
      "7699 : Training: loss:  0.020943554\n",
      "7700 : Training: loss:  0.040732708\n",
      "Validation: Loss:  0.059318308  Accuracy:  0.78846157\n",
      "7701 : Training: loss:  0.050250575\n",
      "7702 : Training: loss:  0.05764337\n",
      "7703 : Training: loss:  0.04997919\n",
      "7704 : Training: loss:  0.047475815\n",
      "7705 : Training: loss:  0.03237393\n",
      "7706 : Training: loss:  0.025957426\n",
      "7707 : Training: loss:  0.038923357\n",
      "7708 : Training: loss:  0.010980503\n",
      "7709 : Training: loss:  0.013450525\n",
      "7710 : Training: loss:  0.03405148\n",
      "7711 : Training: loss:  0.073922485\n",
      "7712 : Training: loss:  0.02660304\n",
      "7713 : Training: loss:  0.039336942\n",
      "7714 : Training: loss:  0.026764944\n",
      "7715 : Training: loss:  0.031337894\n",
      "7716 : Training: loss:  0.03236234\n",
      "7717 : Training: loss:  0.017546311\n",
      "7718 : Training: loss:  0.018442236\n",
      "7719 : Training: loss:  0.069591835\n",
      "7720 : Training: loss:  0.03094609\n",
      "Validation: Loss:  0.05907412  Accuracy:  0.78846157\n",
      "7721 : Training: loss:  0.037092917\n",
      "7722 : Training: loss:  0.030441048\n",
      "7723 : Training: loss:  0.04943724\n",
      "7724 : Training: loss:  0.06555137\n",
      "7725 : Training: loss:  0.026290217\n",
      "7726 : Training: loss:  0.012087338\n",
      "7727 : Training: loss:  0.046461627\n",
      "7728 : Training: loss:  0.052080434\n",
      "7729 : Training: loss:  0.045009017\n",
      "7730 : Training: loss:  0.033630796\n",
      "7731 : Training: loss:  0.027160523\n",
      "7732 : Training: loss:  0.035094712\n",
      "7733 : Training: loss:  0.0360393\n",
      "7734 : Training: loss:  0.04311385\n",
      "7735 : Training: loss:  0.033006337\n",
      "7736 : Training: loss:  0.025399365\n",
      "7737 : Training: loss:  0.03155329\n",
      "7738 : Training: loss:  0.030760365\n",
      "7739 : Training: loss:  0.04114111\n",
      "7740 : Training: loss:  0.0733593\n",
      "Validation: Loss:  0.059144657  Accuracy:  0.78846157\n",
      "7741 : Training: loss:  0.029042281\n",
      "7742 : Training: loss:  0.026204344\n",
      "7743 : Training: loss:  0.016676\n",
      "7744 : Training: loss:  0.041461997\n",
      "7745 : Training: loss:  0.05554223\n",
      "7746 : Training: loss:  0.030930527\n",
      "7747 : Training: loss:  0.057058793\n",
      "7748 : Training: loss:  0.023433046\n",
      "7749 : Training: loss:  0.04345613\n",
      "7750 : Training: loss:  0.039614096\n",
      "7751 : Training: loss:  0.021502601\n",
      "7752 : Training: loss:  0.034654982\n",
      "7753 : Training: loss:  0.040249757\n",
      "7754 : Training: loss:  0.04430127\n",
      "7755 : Training: loss:  0.052141253\n",
      "7756 : Training: loss:  0.02479125\n",
      "7757 : Training: loss:  0.016265975\n",
      "7758 : Training: loss:  0.02184149\n",
      "7759 : Training: loss:  0.058055982\n",
      "7760 : Training: loss:  0.0361815\n",
      "Validation: Loss:  0.058895007  Accuracy:  0.78846157\n",
      "7761 : Training: loss:  0.040821023\n",
      "7762 : Training: loss:  0.05567765\n",
      "7763 : Training: loss:  0.047088865\n",
      "7764 : Training: loss:  0.056854453\n",
      "7765 : Training: loss:  0.03819771\n",
      "7766 : Training: loss:  0.022428606\n",
      "7767 : Training: loss:  0.0052808104\n",
      "7768 : Training: loss:  0.030358665\n",
      "7769 : Training: loss:  0.05204137\n",
      "7770 : Training: loss:  0.020994473\n",
      "7771 : Training: loss:  0.04422567\n",
      "7772 : Training: loss:  0.03590861\n",
      "7773 : Training: loss:  0.031430546\n",
      "7774 : Training: loss:  0.046523143\n",
      "7775 : Training: loss:  0.042024698\n",
      "7776 : Training: loss:  0.030380115\n",
      "7777 : Training: loss:  0.014441066\n",
      "7778 : Training: loss:  0.04191731\n",
      "7779 : Training: loss:  0.04233374\n",
      "7780 : Training: loss:  0.020257575\n",
      "Validation: Loss:  0.05858991  Accuracy:  0.78846157\n",
      "7781 : Training: loss:  0.04079676\n",
      "7782 : Training: loss:  0.06804702\n",
      "7783 : Training: loss:  0.02356691\n",
      "7784 : Training: loss:  0.024082135\n",
      "7785 : Training: loss:  0.039065074\n",
      "7786 : Training: loss:  0.009982748\n",
      "7787 : Training: loss:  0.015104457\n",
      "7788 : Training: loss:  0.041401885\n",
      "7789 : Training: loss:  0.036445398\n",
      "7790 : Training: loss:  0.034102317\n",
      "7791 : Training: loss:  0.04878993\n",
      "7792 : Training: loss:  0.013563334\n",
      "7793 : Training: loss:  0.036424883\n",
      "7794 : Training: loss:  0.041158233\n",
      "7795 : Training: loss:  0.033626575\n",
      "7796 : Training: loss:  0.006635285\n",
      "7797 : Training: loss:  0.02847858\n",
      "7798 : Training: loss:  0.010362246\n",
      "7799 : Training: loss:  0.032922108\n",
      "7800 : Training: loss:  0.04666451\n",
      "Validation: Loss:  0.05886812  Accuracy:  0.78846157\n",
      "7801 : Training: loss:  0.02567571\n",
      "7802 : Training: loss:  0.02095026\n",
      "7803 : Training: loss:  0.035844296\n",
      "7804 : Training: loss:  0.010933809\n",
      "7805 : Training: loss:  0.02897643\n",
      "7806 : Training: loss:  0.024468686\n",
      "7807 : Training: loss:  0.030274726\n",
      "7808 : Training: loss:  0.03417525\n",
      "7809 : Training: loss:  0.024225365\n",
      "7810 : Training: loss:  0.026535101\n",
      "7811 : Training: loss:  0.03984492\n",
      "7812 : Training: loss:  0.015851632\n",
      "7813 : Training: loss:  0.049501207\n",
      "7814 : Training: loss:  0.013929612\n",
      "7815 : Training: loss:  0.02822217\n",
      "7816 : Training: loss:  0.016776012\n",
      "7817 : Training: loss:  0.007332042\n",
      "7818 : Training: loss:  0.05735438\n",
      "7819 : Training: loss:  0.023082444\n",
      "7820 : Training: loss:  0.04062286\n",
      "Validation: Loss:  0.05919683  Accuracy:  0.78846157\n",
      "7821 : Training: loss:  0.02778714\n",
      "7822 : Training: loss:  0.042071883\n",
      "7823 : Training: loss:  0.030801168\n",
      "7824 : Training: loss:  0.06808886\n",
      "7825 : Training: loss:  0.031120107\n",
      "7826 : Training: loss:  0.044149995\n",
      "7827 : Training: loss:  0.048554674\n",
      "7828 : Training: loss:  0.07197476\n",
      "7829 : Training: loss:  0.059642058\n",
      "7830 : Training: loss:  0.021805843\n",
      "7831 : Training: loss:  0.033376377\n",
      "7832 : Training: loss:  0.044911724\n",
      "7833 : Training: loss:  0.05190033\n",
      "7834 : Training: loss:  0.013584972\n",
      "7835 : Training: loss:  0.05105393\n",
      "7836 : Training: loss:  0.029933585\n",
      "7837 : Training: loss:  0.019038673\n",
      "7838 : Training: loss:  0.02799394\n",
      "7839 : Training: loss:  0.03744367\n",
      "7840 : Training: loss:  0.021965198\n",
      "Validation: Loss:  0.059048943  Accuracy:  0.78846157\n",
      "7841 : Training: loss:  0.018631212\n",
      "7842 : Training: loss:  0.022658035\n",
      "7843 : Training: loss:  0.026822325\n",
      "7844 : Training: loss:  0.03459409\n",
      "7845 : Training: loss:  0.046760637\n",
      "7846 : Training: loss:  0.035846278\n",
      "7847 : Training: loss:  0.017441621\n",
      "7848 : Training: loss:  0.06521662\n",
      "7849 : Training: loss:  0.067951836\n",
      "7850 : Training: loss:  0.035320457\n",
      "7851 : Training: loss:  0.014508839\n",
      "7852 : Training: loss:  0.027180735\n",
      "7853 : Training: loss:  0.03645943\n",
      "7854 : Training: loss:  0.03796567\n",
      "7855 : Training: loss:  0.027847258\n",
      "7856 : Training: loss:  0.01904973\n",
      "7857 : Training: loss:  0.04515782\n",
      "7858 : Training: loss:  0.03767199\n",
      "7859 : Training: loss:  0.063596524\n",
      "7860 : Training: loss:  0.02611358\n",
      "Validation: Loss:  0.059042666  Accuracy:  0.78846157\n",
      "7861 : Training: loss:  0.060729403\n",
      "7862 : Training: loss:  0.03310681\n",
      "7863 : Training: loss:  0.033251766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7864 : Training: loss:  0.041533347\n",
      "7865 : Training: loss:  0.0494524\n",
      "7866 : Training: loss:  0.055589534\n",
      "7867 : Training: loss:  0.04512115\n",
      "7868 : Training: loss:  0.010865215\n",
      "7869 : Training: loss:  0.054559845\n",
      "7870 : Training: loss:  0.026635794\n",
      "7871 : Training: loss:  0.022317259\n",
      "7872 : Training: loss:  0.0746648\n",
      "7873 : Training: loss:  0.02944138\n",
      "7874 : Training: loss:  0.053281352\n",
      "7875 : Training: loss:  0.016463261\n",
      "7876 : Training: loss:  0.019068655\n",
      "7877 : Training: loss:  0.033534825\n",
      "7878 : Training: loss:  0.040056154\n",
      "7879 : Training: loss:  0.03148046\n",
      "7880 : Training: loss:  0.026104411\n",
      "Validation: Loss:  0.05899631  Accuracy:  0.7692308\n",
      "7881 : Training: loss:  0.01566747\n",
      "7882 : Training: loss:  0.029620556\n",
      "7883 : Training: loss:  0.023862183\n",
      "7884 : Training: loss:  0.02049533\n",
      "7885 : Training: loss:  0.06310949\n",
      "7886 : Training: loss:  0.037403975\n",
      "7887 : Training: loss:  0.04234571\n",
      "7888 : Training: loss:  0.036265787\n",
      "7889 : Training: loss:  0.043536868\n",
      "7890 : Training: loss:  0.02063466\n",
      "7891 : Training: loss:  0.009485065\n",
      "7892 : Training: loss:  0.07637793\n",
      "7893 : Training: loss:  0.016337087\n",
      "7894 : Training: loss:  0.012832917\n",
      "7895 : Training: loss:  0.03226072\n",
      "7896 : Training: loss:  0.049686432\n",
      "7897 : Training: loss:  0.038071323\n",
      "7898 : Training: loss:  0.057141297\n",
      "7899 : Training: loss:  0.025636267\n",
      "7900 : Training: loss:  0.011370121\n",
      "Validation: Loss:  0.059045922  Accuracy:  0.7692308\n",
      "7901 : Training: loss:  0.042881314\n",
      "7902 : Training: loss:  0.02932991\n",
      "7903 : Training: loss:  0.06115869\n",
      "7904 : Training: loss:  0.035760816\n",
      "7905 : Training: loss:  0.06856206\n",
      "7906 : Training: loss:  0.04649482\n",
      "7907 : Training: loss:  0.02701784\n",
      "7908 : Training: loss:  0.016647039\n",
      "7909 : Training: loss:  0.024824314\n",
      "7910 : Training: loss:  0.03940895\n",
      "7911 : Training: loss:  0.06206866\n",
      "7912 : Training: loss:  0.058452494\n",
      "7913 : Training: loss:  0.026598608\n",
      "7914 : Training: loss:  0.018966807\n",
      "7915 : Training: loss:  0.028032437\n",
      "7916 : Training: loss:  0.012292651\n",
      "7917 : Training: loss:  0.025554439\n",
      "7918 : Training: loss:  0.024333045\n",
      "7919 : Training: loss:  0.01573171\n",
      "7920 : Training: loss:  0.0034458388\n",
      "Validation: Loss:  0.058760222  Accuracy:  0.7692308\n",
      "7921 : Training: loss:  0.0326905\n",
      "7922 : Training: loss:  0.024946548\n",
      "7923 : Training: loss:  0.04172974\n",
      "7924 : Training: loss:  0.04300567\n",
      "7925 : Training: loss:  0.05838718\n",
      "7926 : Training: loss:  0.024496218\n",
      "7927 : Training: loss:  0.038631286\n",
      "7928 : Training: loss:  0.043539356\n",
      "7929 : Training: loss:  0.042043786\n",
      "7930 : Training: loss:  0.037542038\n",
      "7931 : Training: loss:  0.012555007\n",
      "7932 : Training: loss:  0.027888428\n",
      "7933 : Training: loss:  0.049174234\n",
      "7934 : Training: loss:  0.045414183\n",
      "7935 : Training: loss:  0.061103344\n",
      "7936 : Training: loss:  0.047293637\n",
      "7937 : Training: loss:  0.06470662\n",
      "7938 : Training: loss:  0.02581384\n",
      "7939 : Training: loss:  0.04218706\n",
      "7940 : Training: loss:  0.054632917\n",
      "Validation: Loss:  0.058885623  Accuracy:  0.7692308\n",
      "7941 : Training: loss:  0.019068114\n",
      "7942 : Training: loss:  0.044933803\n",
      "7943 : Training: loss:  0.03248288\n",
      "7944 : Training: loss:  0.014170524\n",
      "7945 : Training: loss:  0.017918529\n",
      "7946 : Training: loss:  0.051108375\n",
      "7947 : Training: loss:  0.0055025727\n",
      "7948 : Training: loss:  0.04312153\n",
      "7949 : Training: loss:  0.04817341\n",
      "7950 : Training: loss:  0.017068839\n",
      "7951 : Training: loss:  0.013452189\n",
      "7952 : Training: loss:  0.041049607\n",
      "7953 : Training: loss:  0.052397568\n",
      "7954 : Training: loss:  0.027386831\n",
      "7955 : Training: loss:  0.067314446\n",
      "7956 : Training: loss:  0.024039416\n",
      "7957 : Training: loss:  0.05534019\n",
      "7958 : Training: loss:  0.047687523\n",
      "7959 : Training: loss:  0.04364662\n",
      "7960 : Training: loss:  0.05690682\n",
      "Validation: Loss:  0.058853302  Accuracy:  0.78846157\n",
      "7961 : Training: loss:  0.021488845\n",
      "7962 : Training: loss:  0.016813524\n",
      "7963 : Training: loss:  0.025727179\n",
      "7964 : Training: loss:  0.024706734\n",
      "7965 : Training: loss:  0.024086835\n",
      "7966 : Training: loss:  0.020375665\n",
      "7967 : Training: loss:  0.044316545\n",
      "7968 : Training: loss:  0.062251616\n",
      "7969 : Training: loss:  0.026754092\n",
      "7970 : Training: loss:  0.03759784\n",
      "7971 : Training: loss:  0.029430665\n",
      "7972 : Training: loss:  0.0098451385\n",
      "7973 : Training: loss:  0.026533268\n",
      "7974 : Training: loss:  0.007752147\n",
      "7975 : Training: loss:  0.024182223\n",
      "7976 : Training: loss:  0.033406764\n",
      "7977 : Training: loss:  0.009844273\n",
      "7978 : Training: loss:  0.040601064\n",
      "7979 : Training: loss:  0.048669223\n",
      "7980 : Training: loss:  0.025884915\n",
      "Validation: Loss:  0.058901813  Accuracy:  0.7307692\n",
      "7981 : Training: loss:  0.0636192\n",
      "7982 : Training: loss:  0.028797327\n",
      "7983 : Training: loss:  0.04765616\n",
      "7984 : Training: loss:  0.03479659\n",
      "7985 : Training: loss:  0.037827753\n",
      "7986 : Training: loss:  0.05347826\n",
      "7987 : Training: loss:  0.027113672\n",
      "7988 : Training: loss:  0.04424059\n",
      "7989 : Training: loss:  0.012608403\n",
      "7990 : Training: loss:  0.045786213\n",
      "7991 : Training: loss:  0.020566156\n",
      "7992 : Training: loss:  0.047906537\n",
      "7993 : Training: loss:  0.04128657\n",
      "7994 : Training: loss:  0.0314842\n",
      "7995 : Training: loss:  0.017965721\n",
      "7996 : Training: loss:  0.03717309\n",
      "7997 : Training: loss:  0.031603068\n",
      "7998 : Training: loss:  0.04796587\n",
      "7999 : Training: loss:  0.02441825\n",
      "8000 : Training: loss:  0.01080872\n",
      "Validation: Loss:  0.05953508  Accuracy:  0.7307692\n",
      "8001 : Training: loss:  0.018462254\n",
      "8002 : Training: loss:  0.017742416\n",
      "8003 : Training: loss:  0.046989765\n",
      "8004 : Training: loss:  0.030967832\n",
      "8005 : Training: loss:  0.032543674\n",
      "8006 : Training: loss:  0.04669595\n",
      "8007 : Training: loss:  0.040889405\n",
      "8008 : Training: loss:  0.033451576\n",
      "8009 : Training: loss:  0.041164633\n",
      "8010 : Training: loss:  0.0064085955\n",
      "8011 : Training: loss:  0.057917792\n",
      "8012 : Training: loss:  0.039386656\n",
      "8013 : Training: loss:  0.02253367\n",
      "8014 : Training: loss:  0.032711092\n",
      "8015 : Training: loss:  0.0351837\n",
      "8016 : Training: loss:  0.041270524\n",
      "8017 : Training: loss:  0.041144613\n",
      "8018 : Training: loss:  0.05366099\n",
      "8019 : Training: loss:  0.0090899095\n",
      "8020 : Training: loss:  0.037593644\n",
      "Validation: Loss:  0.05992015  Accuracy:  0.7307692\n",
      "8021 : Training: loss:  0.012069045\n",
      "8022 : Training: loss:  0.03508939\n",
      "8023 : Training: loss:  0.05126791\n",
      "8024 : Training: loss:  0.03865787\n",
      "8025 : Training: loss:  0.038105626\n",
      "8026 : Training: loss:  0.027147105\n",
      "8027 : Training: loss:  0.03773295\n",
      "8028 : Training: loss:  0.050982565\n",
      "8029 : Training: loss:  0.006877368\n",
      "8030 : Training: loss:  0.029135846\n",
      "8031 : Training: loss:  0.020126406\n",
      "8032 : Training: loss:  0.026650809\n",
      "8033 : Training: loss:  0.03880993\n",
      "8034 : Training: loss:  0.033212952\n",
      "8035 : Training: loss:  0.054591063\n",
      "8036 : Training: loss:  0.03810284\n",
      "8037 : Training: loss:  0.024289798\n",
      "8038 : Training: loss:  0.018364092\n",
      "8039 : Training: loss:  0.045833133\n",
      "8040 : Training: loss:  0.027424967\n",
      "Validation: Loss:  0.059975997  Accuracy:  0.71153843\n",
      "8041 : Training: loss:  0.020854833\n",
      "8042 : Training: loss:  0.0354144\n",
      "8043 : Training: loss:  0.020930298\n",
      "8044 : Training: loss:  0.048056167\n",
      "8045 : Training: loss:  0.036575146\n",
      "8046 : Training: loss:  0.026280915\n",
      "8047 : Training: loss:  0.024132371\n",
      "8048 : Training: loss:  0.066658996\n",
      "8049 : Training: loss:  0.032740038\n",
      "8050 : Training: loss:  0.02112664\n",
      "8051 : Training: loss:  0.011284632\n",
      "8052 : Training: loss:  0.012024779\n",
      "8053 : Training: loss:  0.038016435\n",
      "8054 : Training: loss:  0.020128129\n",
      "8055 : Training: loss:  0.05273526\n",
      "8056 : Training: loss:  0.031722654\n",
      "8057 : Training: loss:  0.020376762\n",
      "8058 : Training: loss:  0.065215066\n",
      "8059 : Training: loss:  0.034899194\n",
      "8060 : Training: loss:  0.039760962\n",
      "Validation: Loss:  0.05981013  Accuracy:  0.71153843\n",
      "8061 : Training: loss:  0.033434276\n",
      "8062 : Training: loss:  0.02615333\n",
      "8063 : Training: loss:  0.014691258\n",
      "8064 : Training: loss:  0.018329907\n",
      "8065 : Training: loss:  0.035693128\n",
      "8066 : Training: loss:  0.043734737\n",
      "8067 : Training: loss:  0.018447574\n",
      "8068 : Training: loss:  0.028582962\n",
      "8069 : Training: loss:  0.022316797\n",
      "8070 : Training: loss:  0.019861741\n",
      "8071 : Training: loss:  0.03154134\n",
      "8072 : Training: loss:  0.0213022\n",
      "8073 : Training: loss:  0.029573083\n",
      "8074 : Training: loss:  0.03587531\n",
      "8075 : Training: loss:  0.03200341\n",
      "8076 : Training: loss:  0.048728727\n",
      "8077 : Training: loss:  0.030062947\n",
      "8078 : Training: loss:  0.04772901\n",
      "8079 : Training: loss:  0.014420655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8080 : Training: loss:  0.038528506\n",
      "Validation: Loss:  0.059638564  Accuracy:  0.71153843\n",
      "8081 : Training: loss:  0.061721012\n",
      "8082 : Training: loss:  0.05251015\n",
      "8083 : Training: loss:  0.017529765\n",
      "8084 : Training: loss:  0.03735656\n",
      "8085 : Training: loss:  0.018914929\n",
      "8086 : Training: loss:  0.05698227\n",
      "8087 : Training: loss:  0.029183974\n",
      "8088 : Training: loss:  0.099803194\n",
      "8089 : Training: loss:  0.055994302\n",
      "8090 : Training: loss:  0.019341735\n",
      "8091 : Training: loss:  0.029883258\n",
      "8092 : Training: loss:  0.005143489\n",
      "8093 : Training: loss:  0.02159977\n",
      "8094 : Training: loss:  0.041143317\n",
      "8095 : Training: loss:  0.030480715\n",
      "8096 : Training: loss:  0.018788934\n",
      "8097 : Training: loss:  0.037397116\n",
      "8098 : Training: loss:  0.0062212427\n",
      "8099 : Training: loss:  0.039921675\n",
      "8100 : Training: loss:  0.057822403\n",
      "Validation: Loss:  0.059391707  Accuracy:  0.7307692\n",
      "8101 : Training: loss:  0.067586385\n",
      "8102 : Training: loss:  0.05720304\n",
      "8103 : Training: loss:  0.043017317\n",
      "8104 : Training: loss:  0.02764061\n",
      "8105 : Training: loss:  0.0316516\n",
      "8106 : Training: loss:  0.035658594\n",
      "8107 : Training: loss:  0.03939874\n",
      "8108 : Training: loss:  0.025721554\n",
      "8109 : Training: loss:  0.02376074\n",
      "8110 : Training: loss:  0.034950882\n",
      "8111 : Training: loss:  0.05211461\n",
      "8112 : Training: loss:  0.021967638\n",
      "8113 : Training: loss:  0.029528352\n",
      "8114 : Training: loss:  0.03403806\n",
      "8115 : Training: loss:  0.017520037\n",
      "8116 : Training: loss:  0.01795226\n",
      "8117 : Training: loss:  0.03441415\n",
      "8118 : Training: loss:  0.061669216\n",
      "8119 : Training: loss:  0.019340705\n",
      "8120 : Training: loss:  0.023289425\n",
      "Validation: Loss:  0.058865365  Accuracy:  0.75\n",
      "8121 : Training: loss:  0.024222784\n",
      "8122 : Training: loss:  0.030095251\n",
      "8123 : Training: loss:  0.046297427\n",
      "8124 : Training: loss:  0.044349592\n",
      "8125 : Training: loss:  0.03035296\n",
      "8126 : Training: loss:  0.045847513\n",
      "8127 : Training: loss:  0.030363388\n",
      "8128 : Training: loss:  0.052352067\n",
      "8129 : Training: loss:  0.040814213\n",
      "8130 : Training: loss:  0.0385043\n",
      "8131 : Training: loss:  0.030158272\n",
      "8132 : Training: loss:  0.0668812\n",
      "8133 : Training: loss:  0.03391426\n",
      "8134 : Training: loss:  0.03135119\n",
      "8135 : Training: loss:  0.012201779\n",
      "8136 : Training: loss:  0.023567673\n",
      "8137 : Training: loss:  0.0070736054\n",
      "8138 : Training: loss:  0.037303828\n",
      "8139 : Training: loss:  0.033884235\n",
      "8140 : Training: loss:  0.05000235\n",
      "Validation: Loss:  0.058637284  Accuracy:  0.7307692\n",
      "8141 : Training: loss:  0.03614889\n",
      "8142 : Training: loss:  0.018010667\n",
      "8143 : Training: loss:  0.05207864\n",
      "8144 : Training: loss:  0.042840313\n",
      "8145 : Training: loss:  0.0054776766\n",
      "8146 : Training: loss:  0.02295241\n",
      "8147 : Training: loss:  0.013648157\n",
      "8148 : Training: loss:  0.0497672\n",
      "8149 : Training: loss:  0.008592702\n",
      "8150 : Training: loss:  0.034494136\n",
      "8151 : Training: loss:  0.03376538\n",
      "8152 : Training: loss:  0.021066394\n",
      "8153 : Training: loss:  0.061286025\n",
      "8154 : Training: loss:  0.05282934\n",
      "8155 : Training: loss:  0.032774042\n",
      "8156 : Training: loss:  0.07405695\n",
      "8157 : Training: loss:  0.0163009\n",
      "8158 : Training: loss:  0.044957884\n",
      "8159 : Training: loss:  0.025164751\n",
      "8160 : Training: loss:  0.04808566\n",
      "Validation: Loss:  0.058461003  Accuracy:  0.75\n",
      "8161 : Training: loss:  0.009115194\n",
      "8162 : Training: loss:  0.022241168\n",
      "8163 : Training: loss:  0.054092083\n",
      "8164 : Training: loss:  0.01741786\n",
      "8165 : Training: loss:  0.022360697\n",
      "8166 : Training: loss:  0.012132095\n",
      "8167 : Training: loss:  0.04197369\n",
      "8168 : Training: loss:  0.027983204\n",
      "8169 : Training: loss:  0.048875567\n",
      "8170 : Training: loss:  0.019073833\n",
      "8171 : Training: loss:  0.011809729\n",
      "8172 : Training: loss:  0.048214093\n",
      "8173 : Training: loss:  0.015369256\n",
      "8174 : Training: loss:  0.04258704\n",
      "8175 : Training: loss:  0.015833987\n",
      "8176 : Training: loss:  0.03321226\n",
      "8177 : Training: loss:  0.017021172\n",
      "8178 : Training: loss:  0.00790357\n",
      "8179 : Training: loss:  0.01914327\n",
      "8180 : Training: loss:  0.014725033\n",
      "Validation: Loss:  0.058321755  Accuracy:  0.71153843\n",
      "8181 : Training: loss:  0.050525095\n",
      "8182 : Training: loss:  0.0684463\n",
      "8183 : Training: loss:  0.04190714\n",
      "8184 : Training: loss:  0.047009066\n",
      "8185 : Training: loss:  0.031378873\n",
      "8186 : Training: loss:  0.057007693\n",
      "8187 : Training: loss:  0.0061671\n",
      "8188 : Training: loss:  0.03367727\n",
      "8189 : Training: loss:  0.0043456494\n",
      "8190 : Training: loss:  0.046729118\n",
      "8191 : Training: loss:  0.032986827\n",
      "8192 : Training: loss:  0.0666558\n",
      "8193 : Training: loss:  0.014505905\n",
      "8194 : Training: loss:  0.05608627\n",
      "8195 : Training: loss:  0.033021796\n",
      "8196 : Training: loss:  0.022559237\n",
      "8197 : Training: loss:  0.019232562\n",
      "8198 : Training: loss:  0.014613091\n",
      "8199 : Training: loss:  0.049500126\n",
      "8200 : Training: loss:  0.0331747\n",
      "Validation: Loss:  0.058086667  Accuracy:  0.75\n",
      "8201 : Training: loss:  0.052309114\n",
      "8202 : Training: loss:  0.02817379\n",
      "8203 : Training: loss:  0.032221325\n",
      "8204 : Training: loss:  0.045443136\n",
      "8205 : Training: loss:  0.040170375\n",
      "8206 : Training: loss:  0.03297181\n",
      "8207 : Training: loss:  0.01587062\n",
      "8208 : Training: loss:  0.042344302\n",
      "8209 : Training: loss:  0.010735674\n",
      "8210 : Training: loss:  0.013379281\n",
      "8211 : Training: loss:  0.039581545\n",
      "8212 : Training: loss:  0.028545525\n",
      "8213 : Training: loss:  0.023151102\n",
      "8214 : Training: loss:  0.029635713\n",
      "8215 : Training: loss:  0.035577163\n",
      "8216 : Training: loss:  0.016974827\n",
      "8217 : Training: loss:  0.045141358\n",
      "8218 : Training: loss:  0.019568391\n",
      "8219 : Training: loss:  0.049036436\n",
      "8220 : Training: loss:  0.040989257\n",
      "Validation: Loss:  0.058503546  Accuracy:  0.75\n",
      "8221 : Training: loss:  0.019751513\n",
      "8222 : Training: loss:  0.035050042\n",
      "8223 : Training: loss:  0.019018142\n",
      "8224 : Training: loss:  0.048743248\n",
      "8225 : Training: loss:  0.038844213\n",
      "8226 : Training: loss:  0.01810246\n",
      "8227 : Training: loss:  0.024718536\n",
      "8228 : Training: loss:  0.07049427\n",
      "8229 : Training: loss:  0.020916108\n",
      "8230 : Training: loss:  0.031167086\n",
      "8231 : Training: loss:  0.047974676\n",
      "8232 : Training: loss:  0.025721963\n",
      "8233 : Training: loss:  0.06948563\n",
      "8234 : Training: loss:  0.03768182\n",
      "8235 : Training: loss:  0.018761568\n",
      "8236 : Training: loss:  0.020540366\n",
      "8237 : Training: loss:  0.04915658\n",
      "8238 : Training: loss:  0.037363\n",
      "8239 : Training: loss:  0.010206405\n",
      "8240 : Training: loss:  0.044339083\n",
      "Validation: Loss:  0.058441374  Accuracy:  0.7692308\n",
      "8241 : Training: loss:  0.023372138\n",
      "8242 : Training: loss:  0.047255058\n",
      "8243 : Training: loss:  0.04555938\n",
      "8244 : Training: loss:  0.026315691\n",
      "8245 : Training: loss:  0.04476791\n",
      "8246 : Training: loss:  0.0137361465\n",
      "8247 : Training: loss:  0.02468599\n",
      "8248 : Training: loss:  0.037748978\n",
      "8249 : Training: loss:  0.046065763\n",
      "8250 : Training: loss:  0.03167074\n",
      "8251 : Training: loss:  0.025498137\n",
      "8252 : Training: loss:  0.024169939\n",
      "8253 : Training: loss:  0.037360985\n",
      "8254 : Training: loss:  0.0109654\n",
      "8255 : Training: loss:  0.049015146\n",
      "8256 : Training: loss:  0.03316525\n",
      "8257 : Training: loss:  0.02658553\n",
      "8258 : Training: loss:  0.026305517\n",
      "8259 : Training: loss:  0.055967595\n",
      "8260 : Training: loss:  0.03435669\n",
      "Validation: Loss:  0.057733037  Accuracy:  0.7692308\n",
      "8261 : Training: loss:  0.037368745\n",
      "8262 : Training: loss:  0.014687211\n",
      "8263 : Training: loss:  0.030507728\n",
      "8264 : Training: loss:  0.04773808\n",
      "8265 : Training: loss:  0.03710149\n",
      "8266 : Training: loss:  0.017697237\n",
      "8267 : Training: loss:  0.02807703\n",
      "8268 : Training: loss:  0.0782431\n",
      "8269 : Training: loss:  0.031736583\n",
      "8270 : Training: loss:  0.019703329\n",
      "8271 : Training: loss:  0.033222172\n",
      "8272 : Training: loss:  0.04470015\n",
      "8273 : Training: loss:  0.024855996\n",
      "8274 : Training: loss:  0.009470843\n",
      "8275 : Training: loss:  0.00822298\n",
      "8276 : Training: loss:  0.023671638\n",
      "8277 : Training: loss:  0.006806484\n",
      "8278 : Training: loss:  0.006784589\n",
      "8279 : Training: loss:  0.041398123\n",
      "8280 : Training: loss:  0.06684028\n",
      "Validation: Loss:  0.05745665  Accuracy:  0.7692308\n",
      "8281 : Training: loss:  0.063311376\n",
      "8282 : Training: loss:  0.038621083\n",
      "8283 : Training: loss:  0.02964662\n",
      "8284 : Training: loss:  0.01590522\n",
      "8285 : Training: loss:  0.03486\n",
      "8286 : Training: loss:  0.020779122\n",
      "8287 : Training: loss:  0.017352896\n",
      "8288 : Training: loss:  0.044840302\n",
      "8289 : Training: loss:  0.030191515\n",
      "8290 : Training: loss:  0.038133353\n",
      "8291 : Training: loss:  0.04238438\n",
      "8292 : Training: loss:  0.04247658\n",
      "8293 : Training: loss:  0.038459644\n",
      "8294 : Training: loss:  0.057660744\n",
      "8295 : Training: loss:  0.015749333\n",
      "8296 : Training: loss:  0.015038453\n",
      "8297 : Training: loss:  0.01629906\n",
      "8298 : Training: loss:  0.02117957\n",
      "8299 : Training: loss:  0.0534743\n",
      "8300 : Training: loss:  0.030290527\n",
      "Validation: Loss:  0.057550594  Accuracy:  0.7307692\n",
      "8301 : Training: loss:  0.043223754\n",
      "8302 : Training: loss:  0.021219244\n",
      "8303 : Training: loss:  0.023847116\n",
      "8304 : Training: loss:  0.026678212\n",
      "8305 : Training: loss:  0.02146851\n",
      "8306 : Training: loss:  0.025961218\n",
      "8307 : Training: loss:  0.01688164\n",
      "8308 : Training: loss:  0.050254915\n",
      "8309 : Training: loss:  0.019309124\n",
      "8310 : Training: loss:  0.035760213\n",
      "8311 : Training: loss:  0.030950682\n",
      "8312 : Training: loss:  0.014624304\n",
      "8313 : Training: loss:  0.025088474\n",
      "8314 : Training: loss:  0.03269775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8315 : Training: loss:  0.065978\n",
      "8316 : Training: loss:  0.041093517\n",
      "8317 : Training: loss:  0.013390474\n",
      "8318 : Training: loss:  0.07070877\n",
      "8319 : Training: loss:  0.017641766\n",
      "8320 : Training: loss:  0.04550088\n",
      "Validation: Loss:  0.057799533  Accuracy:  0.75\n",
      "8321 : Training: loss:  0.017618971\n",
      "8322 : Training: loss:  0.037816994\n",
      "8323 : Training: loss:  0.04360285\n",
      "8324 : Training: loss:  0.03554766\n",
      "8325 : Training: loss:  0.044061404\n",
      "8326 : Training: loss:  0.030707994\n",
      "8327 : Training: loss:  0.032798797\n",
      "8328 : Training: loss:  0.021803526\n",
      "8329 : Training: loss:  0.007693358\n",
      "8330 : Training: loss:  0.031792566\n",
      "8331 : Training: loss:  0.08249274\n",
      "8332 : Training: loss:  0.022591589\n",
      "8333 : Training: loss:  0.056471992\n",
      "8334 : Training: loss:  0.023398507\n",
      "8335 : Training: loss:  0.043039273\n",
      "8336 : Training: loss:  0.021482604\n",
      "8337 : Training: loss:  0.03198578\n",
      "8338 : Training: loss:  0.028891979\n",
      "8339 : Training: loss:  0.02181639\n",
      "8340 : Training: loss:  0.017307952\n",
      "Validation: Loss:  0.057780936  Accuracy:  0.75\n",
      "8341 : Training: loss:  0.031871088\n",
      "8342 : Training: loss:  0.04009686\n",
      "8343 : Training: loss:  0.041384116\n",
      "8344 : Training: loss:  0.020180764\n",
      "8345 : Training: loss:  0.041196518\n",
      "8346 : Training: loss:  0.046781175\n",
      "8347 : Training: loss:  0.04316987\n",
      "8348 : Training: loss:  0.009114032\n",
      "8349 : Training: loss:  0.033146016\n",
      "8350 : Training: loss:  0.02097331\n",
      "8351 : Training: loss:  0.028404064\n",
      "8352 : Training: loss:  0.023815969\n",
      "8353 : Training: loss:  0.018126454\n",
      "8354 : Training: loss:  0.008108081\n",
      "8355 : Training: loss:  0.03346209\n",
      "8356 : Training: loss:  0.018975854\n",
      "8357 : Training: loss:  0.030591743\n",
      "8358 : Training: loss:  0.018350527\n",
      "8359 : Training: loss:  0.035106678\n",
      "8360 : Training: loss:  0.045194343\n",
      "Validation: Loss:  0.05747867  Accuracy:  0.7692308\n",
      "8361 : Training: loss:  0.05324371\n",
      "8362 : Training: loss:  0.042916153\n",
      "8363 : Training: loss:  0.068334065\n",
      "8364 : Training: loss:  0.052245498\n",
      "8365 : Training: loss:  0.021078961\n",
      "8366 : Training: loss:  0.038765978\n",
      "8367 : Training: loss:  0.062106103\n",
      "8368 : Training: loss:  0.019633837\n",
      "8369 : Training: loss:  0.024848359\n",
      "8370 : Training: loss:  0.03962156\n",
      "8371 : Training: loss:  0.03853929\n",
      "8372 : Training: loss:  0.043410663\n",
      "8373 : Training: loss:  0.027787313\n",
      "8374 : Training: loss:  0.027297843\n",
      "8375 : Training: loss:  0.018568777\n",
      "8376 : Training: loss:  0.04635038\n",
      "8377 : Training: loss:  0.059262153\n",
      "8378 : Training: loss:  0.02422397\n",
      "8379 : Training: loss:  0.03532369\n",
      "8380 : Training: loss:  0.03264273\n",
      "Validation: Loss:  0.057301115  Accuracy:  0.75\n",
      "8381 : Training: loss:  0.014606995\n",
      "8382 : Training: loss:  0.024580672\n",
      "8383 : Training: loss:  0.016702445\n",
      "8384 : Training: loss:  0.025343893\n",
      "8385 : Training: loss:  0.019027589\n",
      "8386 : Training: loss:  0.03140462\n",
      "8387 : Training: loss:  0.025694571\n",
      "8388 : Training: loss:  0.040527258\n",
      "8389 : Training: loss:  0.051812634\n",
      "8390 : Training: loss:  0.04159306\n",
      "8391 : Training: loss:  0.03844728\n",
      "8392 : Training: loss:  0.080938116\n",
      "8393 : Training: loss:  0.021586915\n",
      "8394 : Training: loss:  0.022771552\n",
      "8395 : Training: loss:  0.012674139\n",
      "8396 : Training: loss:  0.012133908\n",
      "8397 : Training: loss:  0.021846449\n",
      "8398 : Training: loss:  0.034565583\n",
      "8399 : Training: loss:  0.046516184\n",
      "8400 : Training: loss:  0.034025636\n",
      "Validation: Loss:  0.057247803  Accuracy:  0.75\n",
      "8401 : Training: loss:  0.05698319\n",
      "8402 : Training: loss:  0.02452382\n",
      "8403 : Training: loss:  0.02936326\n",
      "8404 : Training: loss:  0.03464413\n",
      "8405 : Training: loss:  0.03799229\n",
      "8406 : Training: loss:  0.022772515\n",
      "8407 : Training: loss:  0.037991595\n",
      "8408 : Training: loss:  0.04778135\n",
      "8409 : Training: loss:  0.03933534\n",
      "8410 : Training: loss:  0.023116443\n",
      "8411 : Training: loss:  0.032611176\n",
      "8412 : Training: loss:  0.023392074\n",
      "8413 : Training: loss:  0.015237021\n",
      "8414 : Training: loss:  0.040668283\n",
      "8415 : Training: loss:  0.054633394\n",
      "8416 : Training: loss:  0.030640211\n",
      "8417 : Training: loss:  0.007712885\n",
      "8418 : Training: loss:  0.025836442\n",
      "8419 : Training: loss:  0.036326412\n",
      "8420 : Training: loss:  0.014951322\n",
      "Validation: Loss:  0.05721757  Accuracy:  0.78846157\n",
      "8421 : Training: loss:  0.03993122\n",
      "8422 : Training: loss:  0.06244722\n",
      "8423 : Training: loss:  0.033333983\n",
      "8424 : Training: loss:  0.020691419\n",
      "8425 : Training: loss:  0.03978088\n",
      "8426 : Training: loss:  0.04582904\n",
      "8427 : Training: loss:  0.037458047\n",
      "8428 : Training: loss:  0.024281077\n",
      "8429 : Training: loss:  0.03474642\n",
      "8430 : Training: loss:  0.008463464\n",
      "8431 : Training: loss:  0.010706001\n",
      "8432 : Training: loss:  0.051018216\n",
      "8433 : Training: loss:  0.018726878\n",
      "8434 : Training: loss:  0.029202629\n",
      "8435 : Training: loss:  0.013361978\n",
      "8436 : Training: loss:  0.024368368\n",
      "8437 : Training: loss:  0.030713726\n",
      "8438 : Training: loss:  0.03638387\n",
      "8439 : Training: loss:  0.029930245\n",
      "8440 : Training: loss:  0.028781917\n",
      "Validation: Loss:  0.057279956  Accuracy:  0.78846157\n",
      "8441 : Training: loss:  0.046668623\n",
      "8442 : Training: loss:  0.012110405\n",
      "8443 : Training: loss:  0.054920405\n",
      "8444 : Training: loss:  0.018173192\n",
      "8445 : Training: loss:  0.042808894\n",
      "8446 : Training: loss:  0.029956562\n",
      "8447 : Training: loss:  0.010653883\n",
      "8448 : Training: loss:  0.036717147\n",
      "8449 : Training: loss:  0.009240628\n",
      "8450 : Training: loss:  0.015558267\n",
      "8451 : Training: loss:  0.021133445\n",
      "8452 : Training: loss:  0.012021949\n",
      "8453 : Training: loss:  0.031782154\n",
      "8454 : Training: loss:  0.022286948\n",
      "8455 : Training: loss:  0.010435965\n",
      "8456 : Training: loss:  0.018812992\n",
      "8457 : Training: loss:  0.03460984\n",
      "8458 : Training: loss:  0.032354157\n",
      "8459 : Training: loss:  0.040501907\n",
      "8460 : Training: loss:  0.043712895\n",
      "Validation: Loss:  0.05728646  Accuracy:  0.8076923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4092, 0.0229, 0.0154, 0.0042, 0.0011, 0.000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.057286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5563, 0.0446, 0.0014, 0.0016, 0.0114, 0.023...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.035, 0.1271, 0.0013, 0.0021, 0.0002, 0.0066...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.005, 0.0332, 0.0022, 0.0, 0.0046, 0.0279, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0643, 0.0837, 0.0031, 0.0016, 0.0016, 0.001...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0099, 0.0011, 0.4059, 0.4058, 0.0003, 0.001...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0048, 0.0006, 0.0561, 0.98, 0.0008, 1e-04, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0035, 0.0006, 0.0412, 0.9674, 0.0011, 1e-04...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0185, 0.0004, 0.105, 0.4762, 0.0036, 0.0019...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.3209, 0.0153, 1e-04, 0.0002, 0.7129, 0.056,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0253, 0.1743, 0.0, 0.0006, 0.4069, 0.1536, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0025, 0.0705, 1e-04, 1e-04, 0.0015, 0.6082,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-04, 0.0684, 1e-04, 1e-04, 0.0116, 0.0111, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0, 0.0005, 0.0067, 0.0029, 0.0, 0.0, 0.0059...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0118, 0.0112, 0.0012, 0.0002, 0.2121, 0.001...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.28, 0.0146, 1e-04, 0.0, 0.0017, 0.0008, 0.0...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0044, 0.0013, 0.076, 0.0104, 0.0013, 0.0046...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0006, 0.0012, 0.0024, 0.0017, 0.0016, 0.002...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0056, 0.0946, 0.0003, 1e-04, 0.0256, 0.2121...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0579, 0.173, 0.0004, 1e-04, 0.0516, 0.1965,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.03, 0.004, 0.0048, 1e-04, 0.0007, 0.0098, 0...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0004, 0.0192, 0.0002, 0.0017, 0.0002, 0.000...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0058, 0.0024, 0.0003, 0.0, 0.0019, 0.0149, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0044, 0.1799, 0.0002, 0.0, 0.01, 0.6875, 0....</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0609, 0.0068, 0.1138, 0.0095, 0.0016, 0.007...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0386, 0.0102, 0.017, 0.008, 0.0005, 1e-04, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0182, 0.0215, 0.0017, 0.0002, 0.0595, 0.000...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0019, 0.0045, 0.0029, 0.001, 0.0027, 0.0021...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0, 0.0003, 0.0002, 0.0037, 1e-04, 0.0, 0.00...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1e-04, 0.0016, 1e-04, 0.0005, 0.0004, 0.0, 0....</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0, 0.0006, 0.0002, 0.0, 1e-04, 1e-04, 0.0, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.002, 0.001, 0.0056, 0.0021, 0.0018, 0.0045,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0014, 0.001, 0.0011, 0.0073, 0.0004, 0.0005...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0309, 0.0226, 0.0028, 0.0017, 0.0036, 0.001...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.007, 0.0392, 0.0008, 1e-04, 0.0027, 0.0301,...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.065, 0.0049, 0.0069, 0.0145, 0.0003, 1e-04,...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0005, 0.0088, 0.0037, 0.0173, 0.0002, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 1e-04, 1e-04, 0.0004, 0.0, 0.0007, 0.000...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[1e-04, 0.0057, 0.005, 0.0022, 0.0138, 0.0, 1e...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0009, 0.005, 0.0164, 0.0141, 0.0273, 0.0003...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0019, 0.0259, 0.003, 0.0008, 0.0005, 0.0, 0...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0054, 0.0038, 0.0342, 0.064, 0.0008, 0.0003...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0004, 0.0073, 0.0011, 0.0344, 0.0, 0.002, 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0, 0.0056, 1e-04, 0.0081, 0.0, 1e-04, 1e-04...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0, 0.0022, 1e-04, 0.0059, 0.0, 0.0, 0.0, 1e...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0, 0.0028, 0.0004, 0.027, 0.0, 1e-04, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[1e-04, 1e-04, 0.0075, 0.0, 0.0006, 1e-04, 0.0...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2413, 0.0025, 0.0254, 0.0107, 0.0007, 0.001...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0009, 0.0755, 1e-04, 0.0, 0.0397, 0.0024, 0...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0112, 0.382, 0.0, 0.0, 0.0016, 0.0342, 1e-0...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.061, 0.0012, 0.0014, 0.0002, 1e-04, 0.001, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0219, 0.0079, 0.0007, 0.0003, 0.0006, 0.042...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4092, 0.0229, 0.0154, 0.0042, 0.0011, 0.000...               0   \n",
       "1   [0.5563, 0.0446, 0.0014, 0.0016, 0.0114, 0.023...               0   \n",
       "2   [0.035, 0.1271, 0.0013, 0.0021, 0.0002, 0.0066...               0   \n",
       "3   [0.005, 0.0332, 0.0022, 0.0, 0.0046, 0.0279, 0...               1   \n",
       "4   [0.0643, 0.0837, 0.0031, 0.0016, 0.0016, 0.001...               1   \n",
       "5   [0.0099, 0.0011, 0.4059, 0.4058, 0.0003, 0.001...               2   \n",
       "6   [0.0048, 0.0006, 0.0561, 0.98, 0.0008, 1e-04, ...               3   \n",
       "7   [0.0035, 0.0006, 0.0412, 0.9674, 0.0011, 1e-04...               3   \n",
       "8   [0.0185, 0.0004, 0.105, 0.4762, 0.0036, 0.0019...               3   \n",
       "9   [0.3209, 0.0153, 1e-04, 0.0002, 0.7129, 0.056,...               4   \n",
       "10  [0.0253, 0.1743, 0.0, 0.0006, 0.4069, 0.1536, ...               4   \n",
       "11  [0.0025, 0.0705, 1e-04, 1e-04, 0.0015, 0.6082,...               5   \n",
       "12  [1e-04, 0.0684, 1e-04, 1e-04, 0.0116, 0.0111, ...               6   \n",
       "13  [0.0, 0.0005, 0.0067, 0.0029, 0.0, 0.0, 0.0059...               7   \n",
       "14  [0.0118, 0.0112, 0.0012, 0.0002, 0.2121, 0.001...               8   \n",
       "15  [0.28, 0.0146, 1e-04, 0.0, 0.0017, 0.0008, 0.0...               8   \n",
       "16  [0.0044, 0.0013, 0.076, 0.0104, 0.0013, 0.0046...               9   \n",
       "17  [0.0006, 0.0012, 0.0024, 0.0017, 0.0016, 0.002...               9   \n",
       "18  [0.0056, 0.0946, 0.0003, 1e-04, 0.0256, 0.2121...              10   \n",
       "19  [0.0579, 0.173, 0.0004, 1e-04, 0.0516, 0.1965,...              10   \n",
       "20  [0.03, 0.004, 0.0048, 1e-04, 0.0007, 0.0098, 0...              11   \n",
       "21  [0.0004, 0.0192, 0.0002, 0.0017, 0.0002, 0.000...              11   \n",
       "22  [0.0058, 0.0024, 0.0003, 0.0, 0.0019, 0.0149, ...              12   \n",
       "23  [0.0044, 0.1799, 0.0002, 0.0, 0.01, 0.6875, 0....              13   \n",
       "24  [0.0609, 0.0068, 0.1138, 0.0095, 0.0016, 0.007...              13   \n",
       "25  [0.0386, 0.0102, 0.017, 0.008, 0.0005, 1e-04, ...              14   \n",
       "26  [0.0182, 0.0215, 0.0017, 0.0002, 0.0595, 0.000...              14   \n",
       "27  [0.0019, 0.0045, 0.0029, 0.001, 0.0027, 0.0021...              15   \n",
       "28  [0.0, 0.0003, 0.0002, 0.0037, 1e-04, 0.0, 0.00...              15   \n",
       "29  [1e-04, 0.0016, 1e-04, 0.0005, 0.0004, 0.0, 0....              15   \n",
       "30  [0.0, 0.0006, 0.0002, 0.0, 1e-04, 1e-04, 0.0, ...              16   \n",
       "31  [0.002, 0.001, 0.0056, 0.0021, 0.0018, 0.0045,...              17   \n",
       "32  [0.0014, 0.001, 0.0011, 0.0073, 0.0004, 0.0005...              17   \n",
       "33  [0.0309, 0.0226, 0.0028, 0.0017, 0.0036, 0.001...              18   \n",
       "34  [0.007, 0.0392, 0.0008, 1e-04, 0.0027, 0.0301,...              19   \n",
       "35  [0.065, 0.0049, 0.0069, 0.0145, 0.0003, 1e-04,...              20   \n",
       "36  [0.0005, 0.0088, 0.0037, 0.0173, 0.0002, 0.021...              21   \n",
       "37  [0.0, 1e-04, 1e-04, 0.0004, 0.0, 0.0007, 0.000...              21   \n",
       "38  [1e-04, 0.0057, 0.005, 0.0022, 0.0138, 0.0, 1e...              22   \n",
       "39  [0.0009, 0.005, 0.0164, 0.0141, 0.0273, 0.0003...              22   \n",
       "40  [0.0019, 0.0259, 0.003, 0.0008, 0.0005, 0.0, 0...              22   \n",
       "41  [0.0054, 0.0038, 0.0342, 0.064, 0.0008, 0.0003...              22   \n",
       "42  [0.0004, 0.0073, 0.0011, 0.0344, 0.0, 0.002, 0...              23   \n",
       "43  [0.0, 0.0056, 1e-04, 0.0081, 0.0, 1e-04, 1e-04...              23   \n",
       "44  [0.0, 0.0022, 1e-04, 0.0059, 0.0, 0.0, 0.0, 1e...              23   \n",
       "45  [0.0, 0.0028, 0.0004, 0.027, 0.0, 1e-04, 0.000...              23   \n",
       "46  [1e-04, 1e-04, 0.0075, 0.0, 0.0006, 1e-04, 0.0...              24   \n",
       "47  [0.2413, 0.0025, 0.0254, 0.0107, 0.0007, 0.001...              24   \n",
       "48  [0.0009, 0.0755, 1e-04, 0.0, 0.0397, 0.0024, 0...              25   \n",
       "49  [0.0112, 0.382, 0.0, 0.0, 0.0016, 0.0342, 1e-0...              25   \n",
       "50  [0.061, 0.0012, 0.0014, 0.0002, 1e-04, 0.001, ...              26   \n",
       "51  [0.0219, 0.0079, 0.0007, 0.0003, 0.0006, 0.042...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.807692  0.057286  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                  1       NaN       NaN  \n",
       "4                  1       NaN       NaN  \n",
       "5                  2       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                11       NaN       NaN  \n",
       "21                23       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                10       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                11       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                21       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                11       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                 0       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                26       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8461 : Training: loss:  0.04235682\n",
      "8462 : Training: loss:  0.0201745\n",
      "8463 : Training: loss:  0.009916286\n",
      "8464 : Training: loss:  0.0743594\n",
      "8465 : Training: loss:  0.026178764\n",
      "8466 : Training: loss:  0.0715681\n",
      "8467 : Training: loss:  0.023071537\n",
      "8468 : Training: loss:  0.035481315\n",
      "8469 : Training: loss:  0.079998836\n",
      "8470 : Training: loss:  0.034460407\n",
      "8471 : Training: loss:  0.025595142\n",
      "8472 : Training: loss:  0.034384076\n",
      "8473 : Training: loss:  0.018041559\n",
      "8474 : Training: loss:  0.02280152\n",
      "8475 : Training: loss:  0.045290135\n",
      "8476 : Training: loss:  0.013182577\n",
      "8477 : Training: loss:  0.083331876\n",
      "8478 : Training: loss:  0.028235044\n",
      "8479 : Training: loss:  0.025217783\n",
      "8480 : Training: loss:  0.007601172\n",
      "Validation: Loss:  0.057075903  Accuracy:  0.8076923\n",
      "8481 : Training: loss:  0.043437928\n",
      "8482 : Training: loss:  0.021571793\n",
      "8483 : Training: loss:  0.032903966\n",
      "8484 : Training: loss:  0.0431607\n",
      "8485 : Training: loss:  0.027540488\n",
      "8486 : Training: loss:  0.04062325\n",
      "8487 : Training: loss:  0.028954266\n",
      "8488 : Training: loss:  0.034955055\n",
      "8489 : Training: loss:  0.044984385\n",
      "8490 : Training: loss:  0.021448001\n",
      "8491 : Training: loss:  0.049891517\n",
      "8492 : Training: loss:  0.026727851\n",
      "8493 : Training: loss:  0.007951364\n",
      "8494 : Training: loss:  0.04499271\n",
      "8495 : Training: loss:  0.032572128\n",
      "8496 : Training: loss:  0.03802009\n",
      "8497 : Training: loss:  0.03060971\n",
      "8498 : Training: loss:  0.05720666\n",
      "8499 : Training: loss:  0.04095964\n",
      "8500 : Training: loss:  0.015303753\n",
      "Validation: Loss:  0.057188805  Accuracy:  0.78846157\n",
      "8501 : Training: loss:  0.046201948\n",
      "8502 : Training: loss:  0.034124795\n",
      "8503 : Training: loss:  0.061797436\n",
      "8504 : Training: loss:  0.020646509\n",
      "8505 : Training: loss:  0.02165892\n",
      "8506 : Training: loss:  0.056689642\n",
      "8507 : Training: loss:  0.034198735\n",
      "8508 : Training: loss:  0.046712227\n",
      "8509 : Training: loss:  0.065250106\n",
      "8510 : Training: loss:  0.028881965\n",
      "8511 : Training: loss:  0.043281727\n",
      "8512 : Training: loss:  0.031351388\n",
      "8513 : Training: loss:  0.033909973\n",
      "8514 : Training: loss:  0.05711939\n",
      "8515 : Training: loss:  0.028730866\n",
      "8516 : Training: loss:  0.025631418\n",
      "8517 : Training: loss:  0.01334227\n",
      "8518 : Training: loss:  0.032826286\n",
      "8519 : Training: loss:  0.035268717\n",
      "8520 : Training: loss:  0.04868679\n",
      "Validation: Loss:  0.056941938  Accuracy:  0.78846157\n",
      "8521 : Training: loss:  0.021154765\n",
      "8522 : Training: loss:  0.015890911\n",
      "8523 : Training: loss:  0.041479085\n",
      "8524 : Training: loss:  0.025618859\n",
      "8525 : Training: loss:  0.021733485\n",
      "8526 : Training: loss:  0.038703755\n",
      "8527 : Training: loss:  0.014930712\n",
      "8528 : Training: loss:  0.043500084\n",
      "8529 : Training: loss:  0.013191668\n",
      "8530 : Training: loss:  0.019539395\n",
      "8531 : Training: loss:  0.056395046\n",
      "8532 : Training: loss:  0.0191895\n",
      "8533 : Training: loss:  0.06155197\n",
      "8534 : Training: loss:  0.051340126\n",
      "8535 : Training: loss:  0.0151443705\n",
      "8536 : Training: loss:  0.046018988\n",
      "8537 : Training: loss:  0.021040203\n",
      "8538 : Training: loss:  0.023990877\n",
      "8539 : Training: loss:  0.012972915\n",
      "8540 : Training: loss:  0.063787095\n",
      "Validation: Loss:  0.056910876  Accuracy:  0.78846157\n",
      "8541 : Training: loss:  0.040592767\n",
      "8542 : Training: loss:  0.036218658\n",
      "8543 : Training: loss:  0.038387127\n",
      "8544 : Training: loss:  0.03945088\n",
      "8545 : Training: loss:  0.056042936\n",
      "8546 : Training: loss:  0.022017535\n",
      "8547 : Training: loss:  0.009261816\n",
      "8548 : Training: loss:  0.039293937\n",
      "8549 : Training: loss:  0.025987696\n",
      "8550 : Training: loss:  0.03299375\n",
      "8551 : Training: loss:  0.016742663\n",
      "8552 : Training: loss:  0.052138\n",
      "8553 : Training: loss:  0.02420289\n",
      "8554 : Training: loss:  0.02523898\n",
      "8555 : Training: loss:  0.02677259\n",
      "8556 : Training: loss:  0.038315788\n",
      "8557 : Training: loss:  0.033375997\n",
      "8558 : Training: loss:  0.039553713\n",
      "8559 : Training: loss:  0.030996691\n",
      "8560 : Training: loss:  0.0345467\n",
      "Validation: Loss:  0.056849886  Accuracy:  0.78846157\n",
      "8561 : Training: loss:  0.004277282\n",
      "8562 : Training: loss:  0.04163668\n",
      "8563 : Training: loss:  0.017916312\n",
      "8564 : Training: loss:  0.021318961\n",
      "8565 : Training: loss:  0.06302014\n",
      "8566 : Training: loss:  0.036597542\n",
      "8567 : Training: loss:  0.03183952\n",
      "8568 : Training: loss:  0.038413115\n",
      "8569 : Training: loss:  0.015870279\n",
      "8570 : Training: loss:  0.023443982\n",
      "8571 : Training: loss:  0.037193608\n",
      "8572 : Training: loss:  0.026730444\n",
      "8573 : Training: loss:  0.018364606\n",
      "8574 : Training: loss:  0.026438609\n",
      "8575 : Training: loss:  0.018016733\n",
      "8576 : Training: loss:  0.04279042\n",
      "8577 : Training: loss:  0.034799855\n",
      "8578 : Training: loss:  0.06021304\n",
      "8579 : Training: loss:  0.011109235\n",
      "8580 : Training: loss:  0.03825881\n",
      "Validation: Loss:  0.056897774  Accuracy:  0.8076923\n",
      "8581 : Training: loss:  0.06714557\n",
      "8582 : Training: loss:  0.018099742\n",
      "8583 : Training: loss:  0.017984273\n",
      "8584 : Training: loss:  0.022075852\n",
      "8585 : Training: loss:  0.023686774\n",
      "8586 : Training: loss:  0.040576603\n",
      "8587 : Training: loss:  0.04738631\n",
      "8588 : Training: loss:  0.062220614\n",
      "8589 : Training: loss:  0.024898803\n",
      "8590 : Training: loss:  0.02458427\n",
      "8591 : Training: loss:  0.03761687\n",
      "8592 : Training: loss:  0.043532353\n",
      "8593 : Training: loss:  0.03599937\n",
      "8594 : Training: loss:  0.032613534\n",
      "8595 : Training: loss:  0.057735667\n",
      "8596 : Training: loss:  0.043206096\n",
      "8597 : Training: loss:  0.051796746\n",
      "8598 : Training: loss:  0.0392878\n",
      "8599 : Training: loss:  0.06752891\n",
      "8600 : Training: loss:  0.025133243\n",
      "Validation: Loss:  0.056775857  Accuracy:  0.78846157\n",
      "8601 : Training: loss:  0.017009867\n",
      "8602 : Training: loss:  0.060522642\n",
      "8603 : Training: loss:  0.0123347035\n",
      "8604 : Training: loss:  0.031428188\n",
      "8605 : Training: loss:  0.051439546\n",
      "8606 : Training: loss:  0.028368294\n",
      "8607 : Training: loss:  0.03385082\n",
      "8608 : Training: loss:  0.025597788\n",
      "8609 : Training: loss:  0.04243364\n",
      "8610 : Training: loss:  0.019368373\n",
      "8611 : Training: loss:  0.03787697\n",
      "8612 : Training: loss:  0.009852743\n",
      "8613 : Training: loss:  0.08168745\n",
      "8614 : Training: loss:  0.013722837\n",
      "8615 : Training: loss:  0.032428164\n",
      "8616 : Training: loss:  0.022177933\n",
      "8617 : Training: loss:  0.06380316\n",
      "8618 : Training: loss:  0.020329578\n",
      "8619 : Training: loss:  0.052366477\n",
      "8620 : Training: loss:  0.047886547\n",
      "Validation: Loss:  0.056910947  Accuracy:  0.78846157\n",
      "8621 : Training: loss:  0.039137986\n",
      "8622 : Training: loss:  0.052851986\n",
      "8623 : Training: loss:  0.045697328\n",
      "8624 : Training: loss:  0.017405946\n",
      "8625 : Training: loss:  0.03439007\n",
      "8626 : Training: loss:  0.041577417\n",
      "8627 : Training: loss:  0.01984576\n",
      "8628 : Training: loss:  0.025438245\n",
      "8629 : Training: loss:  0.037393093\n",
      "8630 : Training: loss:  0.022486292\n",
      "8631 : Training: loss:  0.025295982\n",
      "8632 : Training: loss:  0.022242632\n",
      "8633 : Training: loss:  0.023707688\n",
      "8634 : Training: loss:  0.01060363\n",
      "8635 : Training: loss:  0.03162003\n",
      "8636 : Training: loss:  0.043393735\n",
      "8637 : Training: loss:  0.038000755\n",
      "8638 : Training: loss:  0.027028147\n",
      "8639 : Training: loss:  0.050626483\n",
      "8640 : Training: loss:  0.030351512\n",
      "Validation: Loss:  0.057010684  Accuracy:  0.8076923\n",
      "8641 : Training: loss:  0.018821962\n",
      "8642 : Training: loss:  0.036370482\n",
      "8643 : Training: loss:  0.043456055\n",
      "8644 : Training: loss:  0.04795576\n",
      "8645 : Training: loss:  0.019131301\n",
      "8646 : Training: loss:  0.011286009\n",
      "8647 : Training: loss:  0.060991812\n",
      "8648 : Training: loss:  0.03023484\n",
      "8649 : Training: loss:  0.0481632\n",
      "8650 : Training: loss:  0.015756324\n",
      "8651 : Training: loss:  0.0032103385\n",
      "8652 : Training: loss:  0.017410738\n",
      "8653 : Training: loss:  0.03806913\n",
      "8654 : Training: loss:  0.0061241295\n",
      "8655 : Training: loss:  0.027509943\n",
      "8656 : Training: loss:  0.057980753\n",
      "8657 : Training: loss:  0.025017932\n",
      "8658 : Training: loss:  0.020107904\n",
      "8659 : Training: loss:  0.057072897\n",
      "8660 : Training: loss:  0.029261028\n",
      "Validation: Loss:  0.05743285  Accuracy:  0.8269231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3863, 0.0211, 0.0154, 0.0032, 0.0008, 0.000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.057433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5219, 0.0388, 0.0015, 0.0012, 0.0097, 0.022...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0313, 0.1157, 0.0013, 0.0017, 0.0002, 0.005...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0039, 0.0284, 0.0024, 0.0, 0.0037, 0.0256, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.056, 0.0775, 0.0032, 0.0013, 0.0012, 0.0016...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.009, 0.001, 0.4479, 0.3558, 0.0003, 0.001, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0041, 0.0005, 0.0609, 0.9769, 0.0006, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.003, 0.0005, 0.044, 0.9636, 0.0009, 0.0, 1e...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0154, 0.0003, 0.1138, 0.4381, 0.003, 0.0015...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.2865, 0.0132, 0.0, 0.0002, 0.7029, 0.054, 0...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0186, 0.147, 0.0, 0.0005, 0.398, 0.155, 0.0...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0018, 0.0505, 1e-04, 1e-04, 0.0014, 0.6048,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-04, 0.0654, 1e-04, 1e-04, 0.0114, 0.0118, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0, 0.0005, 0.0075, 0.0024, 0.0, 0.0, 0.0049...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0093, 0.0102, 0.0013, 0.0002, 0.2253, 0.001...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.2693, 0.0122, 1e-04, 0.0, 0.0015, 0.0007, 0...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0039, 0.0012, 0.0855, 0.0079, 0.0011, 0.004...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0005, 0.0011, 0.0024, 0.0013, 0.0013, 0.002...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.004, 0.0712, 0.0003, 1e-04, 0.0199, 0.1861,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0384, 0.1224, 0.0004, 0.0, 0.0452, 0.1682, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0255, 0.0034, 0.005, 0.0, 0.0006, 0.0086, 0...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0003, 0.0155, 0.0002, 0.0014, 0.0002, 0.000...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0047, 0.0019, 0.0003, 0.0, 0.0015, 0.0129, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0027, 0.1237, 0.0002, 0.0, 0.0081, 0.6607, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0488, 0.0057, 0.1175, 0.007, 0.0012, 0.0066...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0324, 0.0098, 0.0169, 0.0058, 0.0004, 0.0, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0146, 0.0196, 0.0016, 0.0002, 0.0471, 0.000...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0016, 0.0043, 0.003, 0.0007, 0.0023, 0.0019...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0, 0.0003, 0.0002, 0.0032, 1e-04, 0.0, 0.00...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1e-04, 0.0015, 1e-04, 0.0004, 0.0004, 0.0, 0....</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0, 0.0004, 0.0002, 0.0, 1e-04, 1e-04, 0.0, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0017, 0.0009, 0.0059, 0.0016, 0.0015, 0.004...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0011, 0.0008, 0.001, 0.0054, 0.0003, 0.0004...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0286, 0.0213, 0.0029, 0.0013, 0.0028, 0.001...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0061, 0.0359, 0.0008, 0.0, 0.0022, 0.0276, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0553, 0.0041, 0.007, 0.011, 0.0003, 1e-04, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0004, 0.0067, 0.0038, 0.0133, 1e-04, 0.018,...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 1e-04, 1e-04, 0.0003, 0.0, 0.0006, 1e-04...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[1e-04, 0.0057, 0.0058, 0.0018, 0.0126, 0.0, 1...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0007, 0.0047, 0.0194, 0.0122, 0.0241, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0016, 0.0262, 0.0032, 0.0006, 0.0004, 0.0, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0045, 0.0033, 0.0384, 0.052, 0.0006, 0.0002...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0003, 0.0059, 0.0011, 0.0278, 0.0, 0.0017, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0, 0.0046, 1e-04, 0.0065, 0.0, 0.0, 0.0, 1e...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0, 0.0018, 1e-04, 0.0044, 0.0, 0.0, 0.0, 1e...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0, 0.0023, 0.0003, 0.021, 0.0, 1e-04, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[1e-04, 1e-04, 0.0072, 0.0, 0.0005, 1e-04, 0.0...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2087, 0.0019, 0.0256, 0.0084, 0.0005, 0.001...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0006, 0.0725, 1e-04, 0.0, 0.0365, 0.0022, 0...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0091, 0.3458, 0.0, 0.0, 0.0015, 0.0317, 1e-...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0518, 0.0009, 0.0013, 1e-04, 1e-04, 0.0008,...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0163, 0.0055, 0.0007, 0.0002, 0.0004, 0.035...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3863, 0.0211, 0.0154, 0.0032, 0.0008, 0.000...               0   \n",
       "1   [0.5219, 0.0388, 0.0015, 0.0012, 0.0097, 0.022...               0   \n",
       "2   [0.0313, 0.1157, 0.0013, 0.0017, 0.0002, 0.005...               0   \n",
       "3   [0.0039, 0.0284, 0.0024, 0.0, 0.0037, 0.0256, ...               1   \n",
       "4   [0.056, 0.0775, 0.0032, 0.0013, 0.0012, 0.0016...               1   \n",
       "5   [0.009, 0.001, 0.4479, 0.3558, 0.0003, 0.001, ...               2   \n",
       "6   [0.0041, 0.0005, 0.0609, 0.9769, 0.0006, 0.0, ...               3   \n",
       "7   [0.003, 0.0005, 0.044, 0.9636, 0.0009, 0.0, 1e...               3   \n",
       "8   [0.0154, 0.0003, 0.1138, 0.4381, 0.003, 0.0015...               3   \n",
       "9   [0.2865, 0.0132, 0.0, 0.0002, 0.7029, 0.054, 0...               4   \n",
       "10  [0.0186, 0.147, 0.0, 0.0005, 0.398, 0.155, 0.0...               4   \n",
       "11  [0.0018, 0.0505, 1e-04, 1e-04, 0.0014, 0.6048,...               5   \n",
       "12  [1e-04, 0.0654, 1e-04, 1e-04, 0.0114, 0.0118, ...               6   \n",
       "13  [0.0, 0.0005, 0.0075, 0.0024, 0.0, 0.0, 0.0049...               7   \n",
       "14  [0.0093, 0.0102, 0.0013, 0.0002, 0.2253, 0.001...               8   \n",
       "15  [0.2693, 0.0122, 1e-04, 0.0, 0.0015, 0.0007, 0...               8   \n",
       "16  [0.0039, 0.0012, 0.0855, 0.0079, 0.0011, 0.004...               9   \n",
       "17  [0.0005, 0.0011, 0.0024, 0.0013, 0.0013, 0.002...               9   \n",
       "18  [0.004, 0.0712, 0.0003, 1e-04, 0.0199, 0.1861,...              10   \n",
       "19  [0.0384, 0.1224, 0.0004, 0.0, 0.0452, 0.1682, ...              10   \n",
       "20  [0.0255, 0.0034, 0.005, 0.0, 0.0006, 0.0086, 0...              11   \n",
       "21  [0.0003, 0.0155, 0.0002, 0.0014, 0.0002, 0.000...              11   \n",
       "22  [0.0047, 0.0019, 0.0003, 0.0, 0.0015, 0.0129, ...              12   \n",
       "23  [0.0027, 0.1237, 0.0002, 0.0, 0.0081, 0.6607, ...              13   \n",
       "24  [0.0488, 0.0057, 0.1175, 0.007, 0.0012, 0.0066...              13   \n",
       "25  [0.0324, 0.0098, 0.0169, 0.0058, 0.0004, 0.0, ...              14   \n",
       "26  [0.0146, 0.0196, 0.0016, 0.0002, 0.0471, 0.000...              14   \n",
       "27  [0.0016, 0.0043, 0.003, 0.0007, 0.0023, 0.0019...              15   \n",
       "28  [0.0, 0.0003, 0.0002, 0.0032, 1e-04, 0.0, 0.00...              15   \n",
       "29  [1e-04, 0.0015, 1e-04, 0.0004, 0.0004, 0.0, 0....              15   \n",
       "30  [0.0, 0.0004, 0.0002, 0.0, 1e-04, 1e-04, 0.0, ...              16   \n",
       "31  [0.0017, 0.0009, 0.0059, 0.0016, 0.0015, 0.004...              17   \n",
       "32  [0.0011, 0.0008, 0.001, 0.0054, 0.0003, 0.0004...              17   \n",
       "33  [0.0286, 0.0213, 0.0029, 0.0013, 0.0028, 0.001...              18   \n",
       "34  [0.0061, 0.0359, 0.0008, 0.0, 0.0022, 0.0276, ...              19   \n",
       "35  [0.0553, 0.0041, 0.007, 0.011, 0.0003, 1e-04, ...              20   \n",
       "36  [0.0004, 0.0067, 0.0038, 0.0133, 1e-04, 0.018,...              21   \n",
       "37  [0.0, 1e-04, 1e-04, 0.0003, 0.0, 0.0006, 1e-04...              21   \n",
       "38  [1e-04, 0.0057, 0.0058, 0.0018, 0.0126, 0.0, 1...              22   \n",
       "39  [0.0007, 0.0047, 0.0194, 0.0122, 0.0241, 0.000...              22   \n",
       "40  [0.0016, 0.0262, 0.0032, 0.0006, 0.0004, 0.0, ...              22   \n",
       "41  [0.0045, 0.0033, 0.0384, 0.052, 0.0006, 0.0002...              22   \n",
       "42  [0.0003, 0.0059, 0.0011, 0.0278, 0.0, 0.0017, ...              23   \n",
       "43  [0.0, 0.0046, 1e-04, 0.0065, 0.0, 0.0, 0.0, 1e...              23   \n",
       "44  [0.0, 0.0018, 1e-04, 0.0044, 0.0, 0.0, 0.0, 1e...              23   \n",
       "45  [0.0, 0.0023, 0.0003, 0.021, 0.0, 1e-04, 0.000...              23   \n",
       "46  [1e-04, 1e-04, 0.0072, 0.0, 0.0005, 1e-04, 0.0...              24   \n",
       "47  [0.2087, 0.0019, 0.0256, 0.0084, 0.0005, 0.001...              24   \n",
       "48  [0.0006, 0.0725, 1e-04, 0.0, 0.0365, 0.0022, 0...              25   \n",
       "49  [0.0091, 0.3458, 0.0, 0.0, 0.0015, 0.0317, 1e-...              25   \n",
       "50  [0.0518, 0.0009, 0.0013, 1e-04, 1e-04, 0.0008,...              26   \n",
       "51  [0.0163, 0.0055, 0.0007, 0.0002, 0.0004, 0.035...              26   \n",
       "\n",
       "    Predicted labels  Accuracy      Loss  \n",
       "0                  0  0.826923  0.057433  \n",
       "1                  0       NaN       NaN  \n",
       "2                 19       NaN       NaN  \n",
       "3                 10       NaN       NaN  \n",
       "4                  1       NaN       NaN  \n",
       "5                  2       NaN       NaN  \n",
       "6                  3       NaN       NaN  \n",
       "7                  3       NaN       NaN  \n",
       "8                  3       NaN       NaN  \n",
       "9                  4       NaN       NaN  \n",
       "10                 4       NaN       NaN  \n",
       "11                 5       NaN       NaN  \n",
       "12                17       NaN       NaN  \n",
       "13                 7       NaN       NaN  \n",
       "14                 8       NaN       NaN  \n",
       "15                 8       NaN       NaN  \n",
       "16                 9       NaN       NaN  \n",
       "17                 9       NaN       NaN  \n",
       "18                10       NaN       NaN  \n",
       "19                10       NaN       NaN  \n",
       "20                11       NaN       NaN  \n",
       "21                11       NaN       NaN  \n",
       "22                12       NaN       NaN  \n",
       "23                 5       NaN       NaN  \n",
       "24                13       NaN       NaN  \n",
       "25                14       NaN       NaN  \n",
       "26                13       NaN       NaN  \n",
       "27                10       NaN       NaN  \n",
       "28                15       NaN       NaN  \n",
       "29                15       NaN       NaN  \n",
       "30                16       NaN       NaN  \n",
       "31                17       NaN       NaN  \n",
       "32                17       NaN       NaN  \n",
       "33                11       NaN       NaN  \n",
       "34                19       NaN       NaN  \n",
       "35                20       NaN       NaN  \n",
       "36                21       NaN       NaN  \n",
       "37                21       NaN       NaN  \n",
       "38                22       NaN       NaN  \n",
       "39                22       NaN       NaN  \n",
       "40                11       NaN       NaN  \n",
       "41                22       NaN       NaN  \n",
       "42                23       NaN       NaN  \n",
       "43                23       NaN       NaN  \n",
       "44                23       NaN       NaN  \n",
       "45                23       NaN       NaN  \n",
       "46                24       NaN       NaN  \n",
       "47                24       NaN       NaN  \n",
       "48                25       NaN       NaN  \n",
       "49                 1       NaN       NaN  \n",
       "50                26       NaN       NaN  \n",
       "51                26       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8661 : Training: loss:  0.014307963\n",
      "8662 : Training: loss:  0.030245619\n",
      "8663 : Training: loss:  0.009547353\n",
      "8664 : Training: loss:  0.032098934\n",
      "8665 : Training: loss:  0.03720734\n",
      "8666 : Training: loss:  0.03852687\n",
      "8667 : Training: loss:  0.025982752\n",
      "8668 : Training: loss:  0.023522275\n",
      "8669 : Training: loss:  0.05342652\n",
      "8670 : Training: loss:  0.049203254\n",
      "8671 : Training: loss:  0.018740645\n",
      "8672 : Training: loss:  0.007142183\n",
      "8673 : Training: loss:  0.06172977\n",
      "8674 : Training: loss:  0.032041714\n",
      "8675 : Training: loss:  0.030686248\n",
      "8676 : Training: loss:  0.043682\n",
      "8677 : Training: loss:  0.022700123\n",
      "8678 : Training: loss:  0.05300104\n",
      "8679 : Training: loss:  0.0440317\n",
      "8680 : Training: loss:  0.025297547\n",
      "Validation: Loss:  0.05708436  Accuracy:  0.8076923\n",
      "8681 : Training: loss:  0.021459857\n",
      "8682 : Training: loss:  0.021789182\n",
      "8683 : Training: loss:  0.046101388\n",
      "8684 : Training: loss:  0.032063562\n",
      "8685 : Training: loss:  0.034542542\n",
      "8686 : Training: loss:  0.032955784\n",
      "8687 : Training: loss:  0.035129994\n",
      "8688 : Training: loss:  0.04091182\n",
      "8689 : Training: loss:  0.016886698\n",
      "8690 : Training: loss:  0.014196437\n",
      "8691 : Training: loss:  0.04044628\n",
      "8692 : Training: loss:  0.029747205\n",
      "8693 : Training: loss:  0.043180116\n",
      "8694 : Training: loss:  0.032422014\n",
      "8695 : Training: loss:  0.039291754\n",
      "8696 : Training: loss:  0.046700176\n",
      "8697 : Training: loss:  0.032471612\n",
      "8698 : Training: loss:  0.018506715\n",
      "8699 : Training: loss:  0.03285757\n",
      "8700 : Training: loss:  0.03660776\n",
      "Validation: Loss:  0.056405224  Accuracy:  0.7692308\n",
      "8701 : Training: loss:  0.02471387\n",
      "8702 : Training: loss:  0.04624119\n",
      "8703 : Training: loss:  0.04546781\n",
      "8704 : Training: loss:  0.042998012\n",
      "8705 : Training: loss:  0.009053801\n",
      "8706 : Training: loss:  0.01725812\n",
      "8707 : Training: loss:  0.010555016\n",
      "8708 : Training: loss:  0.02118446\n",
      "8709 : Training: loss:  0.042748377\n",
      "8710 : Training: loss:  0.0120621845\n",
      "8711 : Training: loss:  0.028455842\n",
      "8712 : Training: loss:  0.04628729\n",
      "8713 : Training: loss:  0.019159924\n",
      "8714 : Training: loss:  0.026536342\n",
      "8715 : Training: loss:  0.023072602\n",
      "8716 : Training: loss:  0.05209367\n",
      "8717 : Training: loss:  0.03812531\n",
      "8718 : Training: loss:  0.050384317\n",
      "8719 : Training: loss:  0.025899008\n",
      "8720 : Training: loss:  0.064957485\n",
      "Validation: Loss:  0.05599181  Accuracy:  0.7692308\n",
      "8721 : Training: loss:  0.02090659\n",
      "8722 : Training: loss:  0.045505013\n",
      "8723 : Training: loss:  0.04776029\n",
      "8724 : Training: loss:  0.016457396\n",
      "8725 : Training: loss:  0.040626675\n",
      "8726 : Training: loss:  0.035776887\n",
      "8727 : Training: loss:  0.03796585\n",
      "8728 : Training: loss:  0.051085733\n",
      "8729 : Training: loss:  0.046351053\n",
      "8730 : Training: loss:  0.021755042\n",
      "8731 : Training: loss:  0.032766644\n",
      "8732 : Training: loss:  0.025934275\n",
      "8733 : Training: loss:  0.029204812\n",
      "8734 : Training: loss:  0.04424782\n",
      "8735 : Training: loss:  0.052396506\n",
      "8736 : Training: loss:  0.038554415\n",
      "8737 : Training: loss:  0.011882831\n",
      "8738 : Training: loss:  0.0350654\n",
      "8739 : Training: loss:  0.065524176\n",
      "8740 : Training: loss:  0.042165972\n",
      "Validation: Loss:  0.056090966  Accuracy:  0.75\n",
      "8741 : Training: loss:  0.02902162\n",
      "8742 : Training: loss:  0.026150778\n",
      "8743 : Training: loss:  0.04974163\n",
      "8744 : Training: loss:  0.028977605\n",
      "8745 : Training: loss:  0.042486217\n",
      "8746 : Training: loss:  0.01808021\n",
      "8747 : Training: loss:  0.006401703\n",
      "8748 : Training: loss:  0.032266248\n",
      "8749 : Training: loss:  0.013459726\n",
      "8750 : Training: loss:  0.01742324\n",
      "8751 : Training: loss:  0.03843035\n",
      "8752 : Training: loss:  0.0071529415\n",
      "8753 : Training: loss:  0.059694495\n",
      "8754 : Training: loss:  0.033164278\n",
      "8755 : Training: loss:  0.02046049\n",
      "8756 : Training: loss:  0.062449034\n",
      "8757 : Training: loss:  0.013143768\n",
      "8758 : Training: loss:  0.025411855\n",
      "8759 : Training: loss:  0.022331942\n",
      "8760 : Training: loss:  0.047608595\n",
      "Validation: Loss:  0.056236684  Accuracy:  0.7307692\n",
      "8761 : Training: loss:  0.044755384\n",
      "8762 : Training: loss:  0.036151502\n",
      "8763 : Training: loss:  0.008960114\n",
      "8764 : Training: loss:  0.004381686\n",
      "8765 : Training: loss:  0.029245608\n",
      "8766 : Training: loss:  0.014019368\n",
      "8767 : Training: loss:  0.04929411\n",
      "8768 : Training: loss:  0.037983634\n",
      "8769 : Training: loss:  0.061583396\n",
      "8770 : Training: loss:  0.02879803\n",
      "8771 : Training: loss:  0.039994974\n",
      "8772 : Training: loss:  0.014329904\n",
      "8773 : Training: loss:  0.03227684\n",
      "8774 : Training: loss:  0.011867992\n",
      "8775 : Training: loss:  0.034175046\n",
      "8776 : Training: loss:  0.021394525\n",
      "8777 : Training: loss:  0.029224858\n",
      "8778 : Training: loss:  0.008138123\n",
      "8779 : Training: loss:  0.01875557\n",
      "8780 : Training: loss:  0.044272695\n",
      "Validation: Loss:  0.05637017  Accuracy:  0.7307692\n",
      "8781 : Training: loss:  0.007872583\n",
      "8782 : Training: loss:  0.03971594\n",
      "8783 : Training: loss:  0.027146654\n",
      "8784 : Training: loss:  0.030809714\n",
      "8785 : Training: loss:  0.015399728\n",
      "8786 : Training: loss:  0.043401018\n",
      "8787 : Training: loss:  0.0153512005\n",
      "8788 : Training: loss:  0.015215774\n",
      "8789 : Training: loss:  0.017041838\n",
      "8790 : Training: loss:  0.023638705\n",
      "8791 : Training: loss:  0.036465745\n",
      "8792 : Training: loss:  0.03001053\n",
      "8793 : Training: loss:  0.025592724\n",
      "8794 : Training: loss:  0.057853486\n",
      "8795 : Training: loss:  0.0553634\n",
      "8796 : Training: loss:  0.014681769\n",
      "8797 : Training: loss:  0.013933103\n",
      "8798 : Training: loss:  0.010181322\n",
      "8799 : Training: loss:  0.036237415\n",
      "8800 : Training: loss:  0.04258602\n",
      "Validation: Loss:  0.056056056  Accuracy:  0.75\n",
      "8801 : Training: loss:  0.019709548\n",
      "8802 : Training: loss:  0.022484602\n",
      "8803 : Training: loss:  0.03181974\n",
      "8804 : Training: loss:  0.01306984\n",
      "8805 : Training: loss:  0.055611927\n",
      "8806 : Training: loss:  0.027147893\n",
      "8807 : Training: loss:  0.061626654\n",
      "8808 : Training: loss:  0.014753561\n",
      "8809 : Training: loss:  0.029168557\n",
      "8810 : Training: loss:  0.0142631065\n",
      "8811 : Training: loss:  0.023592055\n",
      "8812 : Training: loss:  0.021691367\n",
      "8813 : Training: loss:  0.039854903\n",
      "8814 : Training: loss:  0.04985948\n",
      "8815 : Training: loss:  0.006895652\n",
      "8816 : Training: loss:  0.06764496\n",
      "8817 : Training: loss:  0.0213914\n",
      "8818 : Training: loss:  0.07353714\n",
      "8819 : Training: loss:  0.03267645\n",
      "8820 : Training: loss:  0.018612517\n",
      "Validation: Loss:  0.0559943  Accuracy:  0.7692308\n",
      "8821 : Training: loss:  0.035302147\n",
      "8822 : Training: loss:  0.0136970645\n",
      "8823 : Training: loss:  0.07190451\n",
      "8824 : Training: loss:  0.012718755\n",
      "8825 : Training: loss:  0.058937017\n",
      "8826 : Training: loss:  0.0592131\n",
      "8827 : Training: loss:  0.02073405\n",
      "8828 : Training: loss:  0.025061216\n",
      "8829 : Training: loss:  0.02454339\n",
      "8830 : Training: loss:  0.016291419\n",
      "8831 : Training: loss:  0.008687054\n",
      "8832 : Training: loss:  0.007826067\n",
      "8833 : Training: loss:  0.063412964\n",
      "8834 : Training: loss:  0.025453128\n",
      "8835 : Training: loss:  0.041062836\n",
      "8836 : Training: loss:  0.026239077\n",
      "8837 : Training: loss:  0.033945862\n",
      "8838 : Training: loss:  0.027515532\n",
      "8839 : Training: loss:  0.018799186\n",
      "8840 : Training: loss:  0.0331466\n",
      "Validation: Loss:  0.055893872  Accuracy:  0.8076923\n",
      "8841 : Training: loss:  0.010834181\n",
      "8842 : Training: loss:  0.03413186\n",
      "8843 : Training: loss:  0.019220386\n",
      "8844 : Training: loss:  0.02020482\n",
      "8845 : Training: loss:  0.02722489\n",
      "8846 : Training: loss:  0.054006424\n",
      "8847 : Training: loss:  0.024028419\n",
      "8848 : Training: loss:  0.016865985\n",
      "8849 : Training: loss:  0.013748604\n",
      "8850 : Training: loss:  0.022429036\n",
      "8851 : Training: loss:  0.052790474\n",
      "8852 : Training: loss:  0.023365594\n",
      "8853 : Training: loss:  0.020387627\n",
      "8854 : Training: loss:  0.029144583\n",
      "8855 : Training: loss:  0.04384733\n",
      "8856 : Training: loss:  0.030860212\n",
      "8857 : Training: loss:  0.026894208\n",
      "8858 : Training: loss:  0.05296903\n",
      "8859 : Training: loss:  0.03573736\n",
      "8860 : Training: loss:  0.03658109\n",
      "Validation: Loss:  0.055998445  Accuracy:  0.78846157\n",
      "8861 : Training: loss:  0.030262461\n",
      "8862 : Training: loss:  0.015765244\n",
      "8863 : Training: loss:  0.004375098\n",
      "8864 : Training: loss:  0.0048973938\n",
      "8865 : Training: loss:  0.039786547\n",
      "8866 : Training: loss:  0.060565587\n",
      "8867 : Training: loss:  0.03746008\n",
      "8868 : Training: loss:  0.01366828\n",
      "8869 : Training: loss:  0.024510853\n",
      "8870 : Training: loss:  0.034259554\n",
      "8871 : Training: loss:  0.016543325\n",
      "8872 : Training: loss:  0.020249419\n",
      "8873 : Training: loss:  0.031059101\n",
      "8874 : Training: loss:  0.04257162\n",
      "8875 : Training: loss:  0.03181615\n",
      "8876 : Training: loss:  0.052971475\n",
      "8877 : Training: loss:  0.05524723\n",
      "8878 : Training: loss:  0.032954533\n",
      "8879 : Training: loss:  0.027970279\n",
      "8880 : Training: loss:  0.043875985\n",
      "Validation: Loss:  0.056015242  Accuracy:  0.8076923\n",
      "8881 : Training: loss:  0.019856406\n",
      "8882 : Training: loss:  0.05342869\n",
      "8883 : Training: loss:  0.014401792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8884 : Training: loss:  0.028889287\n",
      "8885 : Training: loss:  0.030828826\n",
      "8886 : Training: loss:  0.044934995\n",
      "8887 : Training: loss:  0.027814893\n",
      "8888 : Training: loss:  0.040228456\n",
      "8889 : Training: loss:  0.016760418\n",
      "8890 : Training: loss:  0.028418152\n",
      "8891 : Training: loss:  0.062103648\n",
      "8892 : Training: loss:  0.054237526\n",
      "8893 : Training: loss:  0.029160837\n",
      "8894 : Training: loss:  0.074529715\n",
      "8895 : Training: loss:  0.03241928\n",
      "8896 : Training: loss:  0.01771017\n",
      "8897 : Training: loss:  0.010349142\n",
      "8898 : Training: loss:  0.03486611\n",
      "8899 : Training: loss:  0.037276533\n",
      "8900 : Training: loss:  0.01134796\n",
      "Validation: Loss:  0.055923104  Accuracy:  0.8076923\n",
      "8901 : Training: loss:  0.00845453\n",
      "8902 : Training: loss:  0.02061152\n",
      "8903 : Training: loss:  0.032667782\n",
      "8904 : Training: loss:  0.046027116\n",
      "8905 : Training: loss:  0.021744827\n",
      "8906 : Training: loss:  0.012663863\n",
      "8907 : Training: loss:  0.0043963557\n",
      "8908 : Training: loss:  0.0070515787\n",
      "8909 : Training: loss:  0.045257874\n",
      "8910 : Training: loss:  0.034592636\n",
      "8911 : Training: loss:  0.013583325\n",
      "8912 : Training: loss:  0.019844301\n",
      "8913 : Training: loss:  0.02624909\n",
      "8914 : Training: loss:  0.05653157\n",
      "8915 : Training: loss:  0.0042529427\n",
      "8916 : Training: loss:  0.034767978\n",
      "8917 : Training: loss:  0.016289081\n",
      "8918 : Training: loss:  0.020767495\n",
      "8919 : Training: loss:  0.010036939\n",
      "8920 : Training: loss:  0.026803683\n",
      "Validation: Loss:  0.0555974  Accuracy:  0.8076923\n",
      "8921 : Training: loss:  0.008946824\n",
      "8922 : Training: loss:  0.045707133\n",
      "8923 : Training: loss:  0.03421876\n",
      "8924 : Training: loss:  0.030924942\n",
      "8925 : Training: loss:  0.04178882\n",
      "8926 : Training: loss:  0.018261526\n",
      "8927 : Training: loss:  0.014972419\n",
      "8928 : Training: loss:  0.040705893\n",
      "8929 : Training: loss:  0.026153758\n",
      "8930 : Training: loss:  0.027203323\n",
      "8931 : Training: loss:  0.050347097\n",
      "8932 : Training: loss:  0.036870226\n",
      "8933 : Training: loss:  0.030218197\n",
      "8934 : Training: loss:  0.032350298\n",
      "8935 : Training: loss:  0.077736385\n",
      "8936 : Training: loss:  0.024215182\n",
      "8937 : Training: loss:  0.05266897\n",
      "8938 : Training: loss:  0.037799716\n",
      "8939 : Training: loss:  0.041298978\n",
      "8940 : Training: loss:  0.013507915\n",
      "Validation: Loss:  0.05565809  Accuracy:  0.8076923\n",
      "8941 : Training: loss:  0.022669503\n",
      "8942 : Training: loss:  0.015123963\n",
      "8943 : Training: loss:  0.013191853\n",
      "8944 : Training: loss:  0.016556349\n",
      "8945 : Training: loss:  0.013856806\n",
      "8946 : Training: loss:  0.040295735\n",
      "8947 : Training: loss:  0.015257228\n",
      "8948 : Training: loss:  0.05547103\n",
      "8949 : Training: loss:  0.017573545\n",
      "8950 : Training: loss:  0.019772265\n",
      "8951 : Training: loss:  0.060206715\n",
      "8952 : Training: loss:  0.0499481\n",
      "8953 : Training: loss:  0.021190157\n",
      "8954 : Training: loss:  0.054847315\n",
      "8955 : Training: loss:  0.022778181\n",
      "8956 : Training: loss:  0.023891715\n",
      "8957 : Training: loss:  0.04034941\n",
      "8958 : Training: loss:  0.01431549\n",
      "8959 : Training: loss:  0.028388843\n",
      "8960 : Training: loss:  0.026588332\n",
      "Validation: Loss:  0.056137074  Accuracy:  0.8076923\n",
      "8961 : Training: loss:  0.014644823\n",
      "8962 : Training: loss:  0.018340418\n",
      "8963 : Training: loss:  0.02645675\n",
      "8964 : Training: loss:  0.02720879\n",
      "8965 : Training: loss:  0.048762914\n",
      "8966 : Training: loss:  0.013179393\n",
      "8967 : Training: loss:  0.046613947\n",
      "8968 : Training: loss:  0.038278986\n",
      "8969 : Training: loss:  0.060052387\n",
      "8970 : Training: loss:  0.058664635\n",
      "8971 : Training: loss:  0.024339024\n",
      "8972 : Training: loss:  0.05160409\n",
      "8973 : Training: loss:  0.010107588\n",
      "8974 : Training: loss:  0.0368479\n",
      "8975 : Training: loss:  0.007758942\n",
      "8976 : Training: loss:  0.019793268\n",
      "8977 : Training: loss:  0.045534316\n",
      "8978 : Training: loss:  0.046816222\n",
      "8979 : Training: loss:  0.050032362\n",
      "8980 : Training: loss:  0.03188588\n",
      "Validation: Loss:  0.056177948  Accuracy:  0.8076923\n",
      "8981 : Training: loss:  0.020303624\n",
      "8982 : Training: loss:  0.032517705\n",
      "8983 : Training: loss:  0.038794797\n",
      "8984 : Training: loss:  0.041772876\n",
      "8985 : Training: loss:  0.018401576\n",
      "8986 : Training: loss:  0.018257516\n",
      "8987 : Training: loss:  0.024661317\n",
      "8988 : Training: loss:  0.037076212\n",
      "8989 : Training: loss:  0.019240983\n",
      "8990 : Training: loss:  0.026068797\n",
      "8991 : Training: loss:  0.012841062\n",
      "8992 : Training: loss:  0.042782497\n",
      "8993 : Training: loss:  0.026407931\n",
      "8994 : Training: loss:  0.037299093\n",
      "8995 : Training: loss:  0.04394153\n",
      "8996 : Training: loss:  0.032448966\n",
      "8997 : Training: loss:  0.040538527\n",
      "8998 : Training: loss:  0.00835212\n",
      "8999 : Training: loss:  0.042766616\n",
      "9000 : Training: loss:  0.012155692\n",
      "Validation: Loss:  0.056171503  Accuracy:  0.78846157\n",
      "9001 : Training: loss:  0.051374827\n",
      "9002 : Training: loss:  0.013275115\n",
      "9003 : Training: loss:  0.03404728\n",
      "9004 : Training: loss:  0.011515658\n",
      "9005 : Training: loss:  0.029353539\n",
      "9006 : Training: loss:  0.026368175\n",
      "9007 : Training: loss:  0.06891584\n",
      "9008 : Training: loss:  0.008781794\n",
      "9009 : Training: loss:  0.02693695\n",
      "9010 : Training: loss:  0.006078158\n",
      "9011 : Training: loss:  0.040256865\n",
      "9012 : Training: loss:  0.018801915\n",
      "9013 : Training: loss:  0.043618873\n",
      "9014 : Training: loss:  0.045637038\n",
      "9015 : Training: loss:  0.026103044\n",
      "9016 : Training: loss:  0.03672567\n",
      "9017 : Training: loss:  0.03660301\n",
      "9018 : Training: loss:  0.02657359\n",
      "9019 : Training: loss:  0.053317692\n",
      "9020 : Training: loss:  0.03219108\n",
      "Validation: Loss:  0.056062233  Accuracy:  0.8076923\n",
      "9021 : Training: loss:  0.040084448\n",
      "9022 : Training: loss:  0.0044417568\n",
      "9023 : Training: loss:  0.028654978\n",
      "9024 : Training: loss:  0.030981498\n",
      "9025 : Training: loss:  0.020905826\n",
      "9026 : Training: loss:  0.056504894\n",
      "9027 : Training: loss:  0.026355756\n",
      "9028 : Training: loss:  0.052453034\n",
      "9029 : Training: loss:  0.056933638\n",
      "9030 : Training: loss:  0.03417899\n",
      "9031 : Training: loss:  0.027851515\n",
      "9032 : Training: loss:  0.030676534\n",
      "9033 : Training: loss:  0.021193007\n",
      "9034 : Training: loss:  0.043113954\n",
      "9035 : Training: loss:  0.02855527\n",
      "9036 : Training: loss:  0.06412624\n",
      "9037 : Training: loss:  0.02411972\n",
      "9038 : Training: loss:  0.02825322\n",
      "9039 : Training: loss:  0.022449652\n",
      "9040 : Training: loss:  0.024492493\n",
      "Validation: Loss:  0.05595303  Accuracy:  0.8076923\n",
      "9041 : Training: loss:  0.03624169\n",
      "9042 : Training: loss:  0.048661947\n",
      "9043 : Training: loss:  0.046070915\n",
      "9044 : Training: loss:  0.026144505\n",
      "9045 : Training: loss:  0.017530583\n",
      "9046 : Training: loss:  0.013131959\n",
      "9047 : Training: loss:  0.047251247\n",
      "9048 : Training: loss:  0.055139918\n",
      "9049 : Training: loss:  0.02133672\n",
      "9050 : Training: loss:  0.012840609\n",
      "9051 : Training: loss:  0.022435393\n",
      "9052 : Training: loss:  0.039731663\n",
      "9053 : Training: loss:  0.0042374353\n",
      "9054 : Training: loss:  0.025575574\n",
      "9055 : Training: loss:  0.00897389\n",
      "9056 : Training: loss:  0.036333997\n",
      "9057 : Training: loss:  0.029701434\n",
      "9058 : Training: loss:  0.032733057\n",
      "9059 : Training: loss:  0.04062206\n",
      "9060 : Training: loss:  0.029140487\n",
      "Validation: Loss:  0.05555817  Accuracy:  0.8076923\n",
      "9061 : Training: loss:  0.006534665\n",
      "9062 : Training: loss:  0.011855252\n",
      "9063 : Training: loss:  0.040923312\n",
      "9064 : Training: loss:  0.007829364\n",
      "9065 : Training: loss:  0.050346065\n",
      "9066 : Training: loss:  0.018648408\n",
      "9067 : Training: loss:  0.055842496\n",
      "9068 : Training: loss:  0.02497386\n",
      "9069 : Training: loss:  0.025236966\n",
      "9070 : Training: loss:  0.03619146\n",
      "9071 : Training: loss:  0.009460507\n",
      "9072 : Training: loss:  0.025797082\n",
      "9073 : Training: loss:  0.039784804\n",
      "9074 : Training: loss:  0.030343102\n",
      "9075 : Training: loss:  0.019165304\n",
      "9076 : Training: loss:  0.022908524\n",
      "9077 : Training: loss:  0.028169593\n",
      "9078 : Training: loss:  0.023292592\n",
      "9079 : Training: loss:  0.06595741\n",
      "9080 : Training: loss:  0.04804662\n",
      "Validation: Loss:  0.055488903  Accuracy:  0.78846157\n",
      "9081 : Training: loss:  0.03641459\n",
      "9082 : Training: loss:  0.027762402\n",
      "9083 : Training: loss:  0.01530739\n",
      "9084 : Training: loss:  0.026322857\n",
      "9085 : Training: loss:  0.021446634\n",
      "9086 : Training: loss:  0.017589375\n",
      "9087 : Training: loss:  0.03174942\n",
      "9088 : Training: loss:  0.027679749\n",
      "9089 : Training: loss:  0.008332183\n",
      "9090 : Training: loss:  0.036324993\n",
      "9091 : Training: loss:  0.045495413\n",
      "9092 : Training: loss:  0.035506304\n",
      "9093 : Training: loss:  0.0072842743\n",
      "9094 : Training: loss:  0.029529188\n",
      "9095 : Training: loss:  0.027058365\n",
      "9096 : Training: loss:  0.015979815\n",
      "9097 : Training: loss:  0.02421048\n",
      "9098 : Training: loss:  0.044129737\n",
      "9099 : Training: loss:  0.05496366\n",
      "9100 : Training: loss:  0.005052734\n",
      "Validation: Loss:  0.055603597  Accuracy:  0.78846157\n",
      "9101 : Training: loss:  0.04542167\n",
      "9102 : Training: loss:  0.025591342\n",
      "9103 : Training: loss:  0.019495795\n",
      "9104 : Training: loss:  0.025347464\n",
      "9105 : Training: loss:  0.017944368\n",
      "9106 : Training: loss:  0.013165762\n",
      "9107 : Training: loss:  0.02178526\n",
      "9108 : Training: loss:  0.014254535\n",
      "9109 : Training: loss:  0.022196494\n",
      "9110 : Training: loss:  0.017016646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111 : Training: loss:  0.024857476\n",
      "9112 : Training: loss:  0.029646747\n",
      "9113 : Training: loss:  0.016286936\n",
      "9114 : Training: loss:  0.05563063\n",
      "9115 : Training: loss:  0.013457686\n",
      "9116 : Training: loss:  0.009960823\n",
      "9117 : Training: loss:  0.013278947\n",
      "9118 : Training: loss:  0.029230656\n",
      "9119 : Training: loss:  0.0059226286\n",
      "9120 : Training: loss:  0.03956559\n",
      "Validation: Loss:  0.05599432  Accuracy:  0.78846157\n",
      "9121 : Training: loss:  0.038064543\n",
      "9122 : Training: loss:  0.026404455\n",
      "9123 : Training: loss:  0.029270766\n",
      "9124 : Training: loss:  0.024727698\n",
      "9125 : Training: loss:  0.08139592\n",
      "9126 : Training: loss:  0.031129347\n",
      "9127 : Training: loss:  0.03463918\n",
      "9128 : Training: loss:  0.0659228\n",
      "9129 : Training: loss:  0.046189457\n",
      "9130 : Training: loss:  0.071922004\n",
      "9131 : Training: loss:  0.033273533\n",
      "9132 : Training: loss:  0.04981436\n",
      "9133 : Training: loss:  0.040716425\n",
      "9134 : Training: loss:  0.059716064\n",
      "9135 : Training: loss:  0.07818256\n",
      "9136 : Training: loss:  0.030250836\n",
      "9137 : Training: loss:  0.015087652\n",
      "9138 : Training: loss:  0.052372597\n",
      "9139 : Training: loss:  0.04169814\n",
      "9140 : Training: loss:  0.036812156\n",
      "Validation: Loss:  0.055562306  Accuracy:  0.78846157\n",
      "9141 : Training: loss:  0.030117938\n",
      "9142 : Training: loss:  0.053170905\n",
      "9143 : Training: loss:  0.011609125\n",
      "9144 : Training: loss:  0.010992919\n",
      "9145 : Training: loss:  0.032800213\n",
      "9146 : Training: loss:  0.05654752\n",
      "9147 : Training: loss:  0.04136347\n",
      "9148 : Training: loss:  0.02455899\n",
      "9149 : Training: loss:  0.020115826\n",
      "9150 : Training: loss:  0.052311\n",
      "9151 : Training: loss:  0.009250151\n",
      "9152 : Training: loss:  0.016024653\n",
      "9153 : Training: loss:  0.059480198\n",
      "9154 : Training: loss:  0.053133413\n",
      "9155 : Training: loss:  0.016720226\n",
      "9156 : Training: loss:  0.045309566\n",
      "9157 : Training: loss:  0.016453005\n",
      "9158 : Training: loss:  0.037355285\n",
      "9159 : Training: loss:  0.046258394\n",
      "9160 : Training: loss:  0.030092938\n",
      "Validation: Loss:  0.055072125  Accuracy:  0.8076923\n",
      "9161 : Training: loss:  0.062979475\n",
      "9162 : Training: loss:  0.0054116216\n",
      "9163 : Training: loss:  0.021255052\n",
      "9164 : Training: loss:  0.044543188\n",
      "9165 : Training: loss:  0.036723707\n",
      "9166 : Training: loss:  0.015618635\n",
      "9167 : Training: loss:  0.042387914\n",
      "9168 : Training: loss:  0.04958729\n",
      "9169 : Training: loss:  0.031941254\n",
      "9170 : Training: loss:  0.045758765\n",
      "9171 : Training: loss:  0.029290782\n",
      "9172 : Training: loss:  0.013237501\n",
      "9173 : Training: loss:  0.011511423\n",
      "9174 : Training: loss:  0.013095699\n",
      "9175 : Training: loss:  0.031710166\n",
      "9176 : Training: loss:  0.03361459\n",
      "9177 : Training: loss:  0.024909329\n",
      "9178 : Training: loss:  0.0651347\n",
      "9179 : Training: loss:  0.03712279\n",
      "9180 : Training: loss:  0.019445\n",
      "Validation: Loss:  0.055298097  Accuracy:  0.7692308\n",
      "9181 : Training: loss:  0.049501017\n",
      "9182 : Training: loss:  0.022853477\n",
      "9183 : Training: loss:  0.022537256\n",
      "9184 : Training: loss:  0.033283833\n",
      "9185 : Training: loss:  0.019808033\n",
      "9186 : Training: loss:  0.018479478\n",
      "9187 : Training: loss:  0.05022065\n",
      "9188 : Training: loss:  0.04141095\n",
      "9189 : Training: loss:  0.05175427\n",
      "9190 : Training: loss:  0.03681779\n",
      "9191 : Training: loss:  0.016658233\n",
      "9192 : Training: loss:  0.039501097\n",
      "9193 : Training: loss:  0.0348284\n",
      "9194 : Training: loss:  0.021705642\n",
      "9195 : Training: loss:  0.012413187\n",
      "9196 : Training: loss:  0.004133976\n",
      "9197 : Training: loss:  0.0074749617\n",
      "9198 : Training: loss:  0.009863709\n",
      "9199 : Training: loss:  0.042860493\n",
      "9200 : Training: loss:  0.008626378\n",
      "Validation: Loss:  0.05523721  Accuracy:  0.78846157\n",
      "9201 : Training: loss:  0.034565452\n",
      "9202 : Training: loss:  0.036443003\n",
      "9203 : Training: loss:  0.022092707\n",
      "9204 : Training: loss:  0.032416973\n",
      "9205 : Training: loss:  0.0407337\n",
      "9206 : Training: loss:  0.042018443\n",
      "9207 : Training: loss:  0.031567045\n",
      "9208 : Training: loss:  0.07897586\n",
      "9209 : Training: loss:  0.035135694\n",
      "9210 : Training: loss:  0.06496174\n",
      "9211 : Training: loss:  0.033734363\n",
      "9212 : Training: loss:  0.019094203\n",
      "9213 : Training: loss:  0.04047161\n",
      "9214 : Training: loss:  0.0062819626\n",
      "9215 : Training: loss:  0.032383535\n",
      "9216 : Training: loss:  0.03353259\n",
      "9217 : Training: loss:  0.041291934\n",
      "9218 : Training: loss:  0.041604616\n",
      "9219 : Training: loss:  0.03525884\n",
      "9220 : Training: loss:  0.013813613\n",
      "Validation: Loss:  0.055206396  Accuracy:  0.8076923\n",
      "9221 : Training: loss:  0.013463282\n",
      "9222 : Training: loss:  0.024658993\n",
      "9223 : Training: loss:  0.036339637\n",
      "9224 : Training: loss:  0.030313037\n",
      "9225 : Training: loss:  0.046370577\n",
      "9226 : Training: loss:  0.056830533\n",
      "9227 : Training: loss:  0.054583453\n",
      "9228 : Training: loss:  0.027586034\n",
      "9229 : Training: loss:  0.038027376\n",
      "9230 : Training: loss:  0.020757856\n",
      "9231 : Training: loss:  0.04872107\n",
      "9232 : Training: loss:  0.028628893\n",
      "9233 : Training: loss:  0.034529846\n",
      "9234 : Training: loss:  0.03331252\n",
      "9235 : Training: loss:  0.042527266\n",
      "9236 : Training: loss:  0.054297596\n",
      "9237 : Training: loss:  0.020158209\n",
      "9238 : Training: loss:  0.025710814\n",
      "9239 : Training: loss:  0.050476998\n",
      "9240 : Training: loss:  0.014640594\n",
      "Validation: Loss:  0.054856475  Accuracy:  0.78846157\n",
      "9241 : Training: loss:  0.019508602\n",
      "9242 : Training: loss:  0.02659868\n",
      "9243 : Training: loss:  0.03821824\n",
      "9244 : Training: loss:  0.0126501685\n",
      "9245 : Training: loss:  0.030093236\n",
      "9246 : Training: loss:  0.022290226\n",
      "9247 : Training: loss:  0.029250314\n",
      "9248 : Training: loss:  0.0072494303\n",
      "9249 : Training: loss:  0.07479382\n",
      "9250 : Training: loss:  0.036461435\n",
      "9251 : Training: loss:  0.032445304\n",
      "9252 : Training: loss:  0.030499509\n",
      "9253 : Training: loss:  0.006166857\n",
      "9254 : Training: loss:  0.026214447\n",
      "9255 : Training: loss:  0.05792752\n",
      "9256 : Training: loss:  0.042978153\n",
      "9257 : Training: loss:  0.04804309\n",
      "9258 : Training: loss:  0.02626141\n",
      "9259 : Training: loss:  0.04753183\n",
      "9260 : Training: loss:  0.05578274\n",
      "Validation: Loss:  0.054804746  Accuracy:  0.78846157\n",
      "9261 : Training: loss:  0.034077175\n",
      "9262 : Training: loss:  0.04504869\n",
      "9263 : Training: loss:  0.025066726\n",
      "9264 : Training: loss:  0.023663295\n",
      "9265 : Training: loss:  0.013329678\n",
      "9266 : Training: loss:  0.026440462\n",
      "9267 : Training: loss:  0.025305765\n",
      "9268 : Training: loss:  0.05784607\n",
      "9269 : Training: loss:  0.043101\n",
      "9270 : Training: loss:  0.03615374\n",
      "9271 : Training: loss:  0.012118127\n",
      "9272 : Training: loss:  0.013619583\n",
      "9273 : Training: loss:  0.017813955\n",
      "9274 : Training: loss:  0.02781819\n",
      "9275 : Training: loss:  0.013414306\n",
      "9276 : Training: loss:  0.024846233\n",
      "9277 : Training: loss:  0.050122336\n",
      "9278 : Training: loss:  0.015539551\n",
      "9279 : Training: loss:  0.039850447\n",
      "9280 : Training: loss:  0.017585311\n",
      "Validation: Loss:  0.054641005  Accuracy:  0.8076923\n",
      "9281 : Training: loss:  0.046956297\n",
      "9282 : Training: loss:  0.032088283\n",
      "9283 : Training: loss:  0.022532346\n",
      "9284 : Training: loss:  0.03365207\n",
      "9285 : Training: loss:  0.03540583\n",
      "9286 : Training: loss:  0.029942786\n",
      "9287 : Training: loss:  0.015399839\n",
      "9288 : Training: loss:  0.049075324\n",
      "9289 : Training: loss:  0.034564953\n",
      "9290 : Training: loss:  0.041818712\n",
      "9291 : Training: loss:  0.01740615\n",
      "9292 : Training: loss:  0.011425563\n",
      "9293 : Training: loss:  0.037772465\n",
      "9294 : Training: loss:  0.03027941\n",
      "9295 : Training: loss:  0.033647135\n",
      "9296 : Training: loss:  0.029973468\n",
      "9297 : Training: loss:  0.043137725\n",
      "9298 : Training: loss:  0.022006266\n",
      "9299 : Training: loss:  0.031617984\n",
      "9300 : Training: loss:  0.036301635\n",
      "Validation: Loss:  0.05454877  Accuracy:  0.78846157\n",
      "9301 : Training: loss:  0.018165769\n",
      "9302 : Training: loss:  0.03568469\n",
      "9303 : Training: loss:  0.02673701\n",
      "9304 : Training: loss:  0.032364227\n",
      "9305 : Training: loss:  0.0109022\n",
      "9306 : Training: loss:  0.04614142\n",
      "9307 : Training: loss:  0.017729405\n",
      "9308 : Training: loss:  0.03672866\n",
      "9309 : Training: loss:  0.048044734\n",
      "9310 : Training: loss:  0.05204266\n",
      "9311 : Training: loss:  0.023592409\n",
      "9312 : Training: loss:  0.029052427\n",
      "9313 : Training: loss:  0.031292744\n",
      "9314 : Training: loss:  0.004853445\n",
      "9315 : Training: loss:  0.019550756\n",
      "9316 : Training: loss:  0.033653148\n",
      "9317 : Training: loss:  0.025743593\n",
      "9318 : Training: loss:  0.022638703\n",
      "9319 : Training: loss:  0.021886183\n",
      "9320 : Training: loss:  0.0369507\n",
      "Validation: Loss:  0.05449094  Accuracy:  0.78846157\n",
      "9321 : Training: loss:  0.02519152\n",
      "9322 : Training: loss:  0.04813787\n",
      "9323 : Training: loss:  0.007720029\n",
      "9324 : Training: loss:  0.04202124\n",
      "9325 : Training: loss:  0.036212504\n",
      "9326 : Training: loss:  0.030978017\n",
      "9327 : Training: loss:  0.012378508\n",
      "9328 : Training: loss:  0.008048009\n",
      "9329 : Training: loss:  0.027487159\n",
      "9330 : Training: loss:  0.033219304\n",
      "9331 : Training: loss:  0.023657126\n",
      "9332 : Training: loss:  0.053519633\n",
      "9333 : Training: loss:  0.021167746\n",
      "9334 : Training: loss:  0.04315342\n",
      "9335 : Training: loss:  0.013216399\n",
      "9336 : Training: loss:  0.06308334\n",
      "9337 : Training: loss:  0.032462377\n",
      "9338 : Training: loss:  0.021232279\n",
      "9339 : Training: loss:  0.0206984\n",
      "9340 : Training: loss:  0.048400298\n",
      "Validation: Loss:  0.054590486  Accuracy:  0.7692308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9341 : Training: loss:  0.025585191\n",
      "9342 : Training: loss:  0.041811295\n",
      "9343 : Training: loss:  0.031725347\n",
      "9344 : Training: loss:  0.011086393\n",
      "9345 : Training: loss:  0.025467206\n",
      "9346 : Training: loss:  0.0043460103\n",
      "9347 : Training: loss:  0.049097985\n",
      "9348 : Training: loss:  0.019747926\n",
      "9349 : Training: loss:  0.036979154\n",
      "9350 : Training: loss:  0.022276513\n",
      "9351 : Training: loss:  0.04965965\n",
      "9352 : Training: loss:  0.010577302\n",
      "9353 : Training: loss:  0.07050065\n",
      "9354 : Training: loss:  0.052947003\n",
      "9355 : Training: loss:  0.02218421\n",
      "9356 : Training: loss:  0.035052568\n",
      "9357 : Training: loss:  0.044630613\n",
      "9358 : Training: loss:  0.024844944\n",
      "9359 : Training: loss:  0.020774985\n",
      "9360 : Training: loss:  0.052875265\n",
      "Validation: Loss:  0.054583024  Accuracy:  0.7692308\n",
      "9361 : Training: loss:  0.039458483\n",
      "9362 : Training: loss:  0.011375075\n",
      "9363 : Training: loss:  0.041388683\n",
      "9364 : Training: loss:  0.061663646\n",
      "9365 : Training: loss:  0.028659701\n",
      "9366 : Training: loss:  0.023329837\n",
      "9367 : Training: loss:  0.05862908\n",
      "9368 : Training: loss:  0.026954902\n",
      "9369 : Training: loss:  0.064088464\n",
      "9370 : Training: loss:  0.017748967\n",
      "9371 : Training: loss:  0.020791862\n",
      "9372 : Training: loss:  0.017540975\n",
      "9373 : Training: loss:  0.02926636\n",
      "9374 : Training: loss:  0.048279278\n",
      "9375 : Training: loss:  0.045972303\n",
      "9376 : Training: loss:  0.02385031\n",
      "9377 : Training: loss:  0.018404484\n",
      "9378 : Training: loss:  0.03296168\n",
      "9379 : Training: loss:  0.012866791\n",
      "9380 : Training: loss:  0.04632509\n",
      "Validation: Loss:  0.05484863  Accuracy:  0.7692308\n",
      "9381 : Training: loss:  0.0215665\n",
      "9382 : Training: loss:  0.028967919\n",
      "9383 : Training: loss:  0.03973062\n",
      "9384 : Training: loss:  0.040917564\n",
      "9385 : Training: loss:  0.033559617\n",
      "9386 : Training: loss:  0.013160759\n",
      "9387 : Training: loss:  0.028963499\n",
      "9388 : Training: loss:  0.03050686\n",
      "9389 : Training: loss:  0.03733713\n",
      "9390 : Training: loss:  0.023751097\n",
      "9391 : Training: loss:  0.023620829\n",
      "9392 : Training: loss:  0.044522278\n",
      "9393 : Training: loss:  0.05542293\n",
      "9394 : Training: loss:  0.010612447\n",
      "9395 : Training: loss:  0.01971886\n",
      "9396 : Training: loss:  0.020442184\n",
      "9397 : Training: loss:  0.010622805\n",
      "9398 : Training: loss:  0.032973137\n",
      "9399 : Training: loss:  0.024467597\n",
      "9400 : Training: loss:  0.017831016\n",
      "Validation: Loss:  0.05478726  Accuracy:  0.7692308\n",
      "9401 : Training: loss:  0.028679134\n",
      "9402 : Training: loss:  0.026631713\n",
      "9403 : Training: loss:  0.010684526\n",
      "9404 : Training: loss:  0.034878224\n",
      "9405 : Training: loss:  0.01774414\n",
      "9406 : Training: loss:  0.0350031\n",
      "9407 : Training: loss:  0.04756012\n",
      "9408 : Training: loss:  0.01990873\n",
      "9409 : Training: loss:  0.03776309\n",
      "9410 : Training: loss:  0.051104967\n",
      "9411 : Training: loss:  0.019551944\n",
      "9412 : Training: loss:  0.027613463\n",
      "9413 : Training: loss:  0.012798079\n",
      "9414 : Training: loss:  0.042432196\n",
      "9415 : Training: loss:  0.09137534\n",
      "9416 : Training: loss:  0.036678653\n",
      "9417 : Training: loss:  0.0072562317\n",
      "9418 : Training: loss:  0.027228916\n",
      "9419 : Training: loss:  0.025421146\n",
      "9420 : Training: loss:  0.029058289\n",
      "Validation: Loss:  0.0547454  Accuracy:  0.7692308\n",
      "9421 : Training: loss:  0.025777452\n",
      "9422 : Training: loss:  0.05855853\n",
      "9423 : Training: loss:  0.032255683\n",
      "9424 : Training: loss:  0.03348781\n",
      "9425 : Training: loss:  0.026522096\n",
      "9426 : Training: loss:  0.03421556\n",
      "9427 : Training: loss:  0.026307018\n",
      "9428 : Training: loss:  0.04344347\n",
      "9429 : Training: loss:  0.041015357\n",
      "9430 : Training: loss:  0.007170468\n",
      "9431 : Training: loss:  0.045741938\n",
      "9432 : Training: loss:  0.025039498\n",
      "9433 : Training: loss:  0.038728133\n",
      "9434 : Training: loss:  0.036085356\n",
      "9435 : Training: loss:  0.06900149\n",
      "9436 : Training: loss:  0.017870266\n",
      "9437 : Training: loss:  0.009102002\n",
      "9438 : Training: loss:  0.045591027\n",
      "9439 : Training: loss:  0.038110923\n",
      "9440 : Training: loss:  0.0368636\n",
      "Validation: Loss:  0.05479217  Accuracy:  0.78846157\n",
      "9441 : Training: loss:  0.032805476\n",
      "9442 : Training: loss:  0.019458236\n",
      "9443 : Training: loss:  0.022398766\n",
      "9444 : Training: loss:  0.05142257\n",
      "9445 : Training: loss:  0.036750536\n",
      "9446 : Training: loss:  0.030210547\n",
      "9447 : Training: loss:  0.025832985\n",
      "9448 : Training: loss:  0.013004669\n",
      "9449 : Training: loss:  0.035052773\n",
      "9450 : Training: loss:  0.03812355\n",
      "9451 : Training: loss:  0.023289056\n",
      "9452 : Training: loss:  0.02565\n",
      "9453 : Training: loss:  0.054964304\n",
      "9454 : Training: loss:  0.02294297\n",
      "9455 : Training: loss:  0.042678792\n",
      "9456 : Training: loss:  0.007600352\n",
      "9457 : Training: loss:  0.044519596\n",
      "9458 : Training: loss:  0.01770335\n",
      "9459 : Training: loss:  0.038121946\n",
      "9460 : Training: loss:  0.025758002\n",
      "Validation: Loss:  0.055290427  Accuracy:  0.78846157\n",
      "9461 : Training: loss:  0.039975304\n",
      "9462 : Training: loss:  0.029645741\n",
      "9463 : Training: loss:  0.0355313\n",
      "9464 : Training: loss:  0.03800408\n",
      "9465 : Training: loss:  0.008606314\n",
      "9466 : Training: loss:  0.0037741498\n",
      "9467 : Training: loss:  0.015079862\n",
      "9468 : Training: loss:  0.005242587\n",
      "9469 : Training: loss:  0.004883852\n",
      "9470 : Training: loss:  0.004837096\n",
      "9471 : Training: loss:  0.0062244274\n",
      "9472 : Training: loss:  0.021902991\n",
      "9473 : Training: loss:  0.021647273\n",
      "9474 : Training: loss:  0.017415842\n",
      "9475 : Training: loss:  0.013911829\n",
      "9476 : Training: loss:  0.033590693\n",
      "9477 : Training: loss:  0.026413629\n",
      "9478 : Training: loss:  0.013418119\n",
      "9479 : Training: loss:  0.0073822415\n",
      "9480 : Training: loss:  0.038825326\n",
      "Validation: Loss:  0.055424627  Accuracy:  0.7692308\n",
      "9481 : Training: loss:  0.04424668\n",
      "9482 : Training: loss:  0.049812023\n",
      "9483 : Training: loss:  0.012883835\n",
      "9484 : Training: loss:  0.017634613\n",
      "9485 : Training: loss:  0.030881105\n",
      "9486 : Training: loss:  0.03193\n",
      "9487 : Training: loss:  0.027743522\n",
      "9488 : Training: loss:  0.023861768\n",
      "9489 : Training: loss:  0.037223402\n",
      "9490 : Training: loss:  0.048524454\n",
      "9491 : Training: loss:  0.040438455\n",
      "9492 : Training: loss:  0.031210115\n",
      "9493 : Training: loss:  0.019672535\n",
      "9494 : Training: loss:  0.05942228\n",
      "9495 : Training: loss:  0.0055471305\n",
      "9496 : Training: loss:  0.0103146415\n",
      "9497 : Training: loss:  0.0050412803\n",
      "9498 : Training: loss:  0.014609348\n",
      "9499 : Training: loss:  0.010047318\n",
      "9500 : Training: loss:  0.02466214\n",
      "Validation: Loss:  0.05528635  Accuracy:  0.7692308\n",
      "9501 : Training: loss:  0.010723867\n",
      "9502 : Training: loss:  0.020543382\n",
      "9503 : Training: loss:  0.026473435\n",
      "9504 : Training: loss:  0.022750769\n",
      "9505 : Training: loss:  0.035486307\n",
      "9506 : Training: loss:  0.051809505\n",
      "9507 : Training: loss:  0.021834992\n",
      "9508 : Training: loss:  0.043826167\n",
      "9509 : Training: loss:  0.08114747\n",
      "9510 : Training: loss:  0.046850886\n",
      "9511 : Training: loss:  0.015042825\n",
      "9512 : Training: loss:  0.023197297\n",
      "9513 : Training: loss:  0.029281082\n",
      "9514 : Training: loss:  0.042918064\n",
      "9515 : Training: loss:  0.025972737\n",
      "9516 : Training: loss:  0.034287807\n",
      "9517 : Training: loss:  0.017523997\n",
      "9518 : Training: loss:  0.0055973283\n",
      "9519 : Training: loss:  0.028118491\n",
      "9520 : Training: loss:  0.007407894\n",
      "Validation: Loss:  0.055110574  Accuracy:  0.7692308\n",
      "9521 : Training: loss:  0.04628615\n",
      "9522 : Training: loss:  0.011428857\n",
      "9523 : Training: loss:  0.03656207\n",
      "9524 : Training: loss:  0.03813994\n",
      "9525 : Training: loss:  0.032469247\n",
      "9526 : Training: loss:  0.0070382184\n",
      "9527 : Training: loss:  0.026710901\n",
      "9528 : Training: loss:  0.023775436\n",
      "9529 : Training: loss:  0.020529458\n",
      "9530 : Training: loss:  0.03162708\n",
      "9531 : Training: loss:  0.05196281\n",
      "9532 : Training: loss:  0.014696353\n",
      "9533 : Training: loss:  0.01619793\n",
      "9534 : Training: loss:  0.016561283\n",
      "9535 : Training: loss:  0.041991428\n",
      "9536 : Training: loss:  0.009567185\n",
      "9537 : Training: loss:  0.033907\n",
      "9538 : Training: loss:  0.049982116\n",
      "9539 : Training: loss:  0.0217204\n",
      "9540 : Training: loss:  0.012482409\n",
      "Validation: Loss:  0.05479072  Accuracy:  0.8076923\n",
      "9541 : Training: loss:  0.053313944\n",
      "9542 : Training: loss:  0.055697225\n",
      "9543 : Training: loss:  0.017678158\n",
      "9544 : Training: loss:  0.030728824\n",
      "9545 : Training: loss:  0.030429887\n",
      "9546 : Training: loss:  0.034084592\n",
      "9547 : Training: loss:  0.04352381\n",
      "9548 : Training: loss:  0.015047471\n",
      "9549 : Training: loss:  0.05598348\n",
      "9550 : Training: loss:  0.022675453\n",
      "9551 : Training: loss:  0.04463227\n",
      "9552 : Training: loss:  0.009249813\n",
      "9553 : Training: loss:  0.036275625\n",
      "9554 : Training: loss:  0.013748152\n",
      "9555 : Training: loss:  0.034505855\n",
      "9556 : Training: loss:  0.051121138\n",
      "9557 : Training: loss:  0.017278923\n",
      "9558 : Training: loss:  0.022666765\n",
      "9559 : Training: loss:  0.030655287\n",
      "9560 : Training: loss:  0.040368333\n",
      "Validation: Loss:  0.054905012  Accuracy:  0.8076923\n",
      "9561 : Training: loss:  0.027069714\n",
      "9562 : Training: loss:  0.021740282\n",
      "9563 : Training: loss:  0.036283694\n",
      "9564 : Training: loss:  0.039703418\n",
      "9565 : Training: loss:  0.030311188\n",
      "9566 : Training: loss:  0.027273558\n",
      "9567 : Training: loss:  0.014753567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9568 : Training: loss:  0.05183477\n",
      "9569 : Training: loss:  0.022691157\n",
      "9570 : Training: loss:  0.064509705\n",
      "9571 : Training: loss:  0.020528777\n",
      "9572 : Training: loss:  0.01900647\n",
      "9573 : Training: loss:  0.016120536\n",
      "9574 : Training: loss:  0.03898849\n",
      "9575 : Training: loss:  0.046544\n",
      "9576 : Training: loss:  0.018085029\n",
      "9577 : Training: loss:  0.0405268\n",
      "9578 : Training: loss:  0.050543804\n",
      "9579 : Training: loss:  0.031333525\n",
      "9580 : Training: loss:  0.033880476\n",
      "Validation: Loss:  0.0552575  Accuracy:  0.8076923\n",
      "9581 : Training: loss:  0.05503748\n",
      "9582 : Training: loss:  0.014253313\n",
      "9583 : Training: loss:  0.0057247197\n",
      "9584 : Training: loss:  0.034024086\n",
      "9585 : Training: loss:  0.018247167\n",
      "9586 : Training: loss:  0.011332693\n",
      "9587 : Training: loss:  0.022271644\n",
      "9588 : Training: loss:  0.02851427\n",
      "9589 : Training: loss:  0.037604164\n",
      "9590 : Training: loss:  0.0465879\n",
      "9591 : Training: loss:  0.03943699\n",
      "9592 : Training: loss:  0.01670802\n",
      "9593 : Training: loss:  0.033163697\n",
      "9594 : Training: loss:  0.043228135\n",
      "9595 : Training: loss:  0.030404745\n",
      "9596 : Training: loss:  0.02363213\n",
      "9597 : Training: loss:  0.027315697\n",
      "9598 : Training: loss:  0.040135622\n",
      "9599 : Training: loss:  0.03633161\n",
      "9600 : Training: loss:  0.01135118\n",
      "Validation: Loss:  0.055694684  Accuracy:  0.78846157\n",
      "9601 : Training: loss:  0.026218653\n",
      "9602 : Training: loss:  0.041883223\n",
      "9603 : Training: loss:  0.013083714\n",
      "9604 : Training: loss:  0.040357497\n",
      "9605 : Training: loss:  0.01620447\n",
      "9606 : Training: loss:  0.012924576\n",
      "9607 : Training: loss:  0.018906139\n",
      "9608 : Training: loss:  0.015896665\n",
      "9609 : Training: loss:  0.018199757\n",
      "9610 : Training: loss:  0.01712329\n",
      "9611 : Training: loss:  0.028335143\n",
      "9612 : Training: loss:  0.027025947\n",
      "9613 : Training: loss:  0.040981013\n",
      "9614 : Training: loss:  0.026329717\n",
      "9615 : Training: loss:  0.010385804\n",
      "9616 : Training: loss:  0.012757263\n",
      "9617 : Training: loss:  0.03873503\n",
      "9618 : Training: loss:  0.02074504\n",
      "9619 : Training: loss:  0.020929344\n",
      "9620 : Training: loss:  0.02644783\n",
      "Validation: Loss:  0.055414297  Accuracy:  0.7692308\n",
      "9621 : Training: loss:  0.029077878\n",
      "9622 : Training: loss:  0.03932095\n",
      "9623 : Training: loss:  0.011690517\n",
      "9624 : Training: loss:  0.015414876\n",
      "9625 : Training: loss:  0.034319893\n",
      "9626 : Training: loss:  0.0378416\n",
      "9627 : Training: loss:  0.029308109\n",
      "9628 : Training: loss:  0.011565981\n",
      "9629 : Training: loss:  0.011055142\n",
      "9630 : Training: loss:  0.030166969\n",
      "9631 : Training: loss:  0.028354075\n",
      "9632 : Training: loss:  0.027983593\n",
      "9633 : Training: loss:  0.030656667\n",
      "9634 : Training: loss:  0.057790525\n",
      "9635 : Training: loss:  0.024890227\n",
      "9636 : Training: loss:  0.0428374\n",
      "9637 : Training: loss:  0.035038777\n",
      "9638 : Training: loss:  0.050330576\n",
      "9639 : Training: loss:  0.033597466\n",
      "9640 : Training: loss:  0.015159828\n",
      "Validation: Loss:  0.055272806  Accuracy:  0.78846157\n",
      "9641 : Training: loss:  0.045363214\n",
      "9642 : Training: loss:  0.045185026\n",
      "9643 : Training: loss:  0.048583105\n",
      "9644 : Training: loss:  0.025125861\n",
      "9645 : Training: loss:  0.033709012\n",
      "9646 : Training: loss:  0.03762956\n",
      "9647 : Training: loss:  0.03130656\n",
      "9648 : Training: loss:  0.015415347\n",
      "9649 : Training: loss:  0.020032708\n",
      "9650 : Training: loss:  0.03655734\n",
      "9651 : Training: loss:  0.027257888\n",
      "9652 : Training: loss:  0.019551788\n",
      "9653 : Training: loss:  0.01464267\n",
      "9654 : Training: loss:  0.029935833\n",
      "9655 : Training: loss:  0.024669899\n",
      "9656 : Training: loss:  0.024488144\n",
      "9657 : Training: loss:  0.038577415\n",
      "9658 : Training: loss:  0.021110114\n",
      "9659 : Training: loss:  0.024250938\n",
      "9660 : Training: loss:  0.0170495\n",
      "Validation: Loss:  0.054624192  Accuracy:  0.78846157\n",
      "9661 : Training: loss:  0.020631371\n",
      "9662 : Training: loss:  0.014058424\n",
      "9663 : Training: loss:  0.041371293\n",
      "9664 : Training: loss:  0.027613752\n",
      "9665 : Training: loss:  0.033416025\n",
      "9666 : Training: loss:  0.021419052\n",
      "9667 : Training: loss:  0.01106738\n",
      "9668 : Training: loss:  0.039057177\n",
      "9669 : Training: loss:  0.04560602\n",
      "9670 : Training: loss:  0.041212335\n",
      "9671 : Training: loss:  0.03694771\n",
      "9672 : Training: loss:  0.031966448\n",
      "9673 : Training: loss:  0.022053482\n",
      "9674 : Training: loss:  0.037943512\n",
      "9675 : Training: loss:  0.018042803\n",
      "9676 : Training: loss:  0.04524882\n",
      "9677 : Training: loss:  0.03131488\n",
      "9678 : Training: loss:  0.035527684\n",
      "9679 : Training: loss:  0.0071393973\n",
      "9680 : Training: loss:  0.03968912\n",
      "Validation: Loss:  0.054430135  Accuracy:  0.78846157\n",
      "9681 : Training: loss:  0.046059392\n",
      "9682 : Training: loss:  0.08739113\n",
      "9683 : Training: loss:  0.014704601\n",
      "9684 : Training: loss:  0.021771742\n",
      "9685 : Training: loss:  0.010128611\n",
      "9686 : Training: loss:  0.028801508\n",
      "9687 : Training: loss:  0.01931605\n",
      "9688 : Training: loss:  0.03345771\n",
      "9689 : Training: loss:  0.023734128\n",
      "9690 : Training: loss:  0.023801744\n",
      "9691 : Training: loss:  0.012916275\n",
      "9692 : Training: loss:  0.028511142\n",
      "9693 : Training: loss:  0.03377319\n",
      "9694 : Training: loss:  0.012898294\n",
      "9695 : Training: loss:  0.041597065\n",
      "9696 : Training: loss:  0.02770648\n",
      "9697 : Training: loss:  0.039136205\n",
      "9698 : Training: loss:  0.055482052\n",
      "9699 : Training: loss:  0.026937768\n",
      "9700 : Training: loss:  0.025685271\n",
      "Validation: Loss:  0.05435991  Accuracy:  0.78846157\n",
      "9701 : Training: loss:  0.015471802\n",
      "9702 : Training: loss:  0.047025476\n",
      "9703 : Training: loss:  0.013792764\n",
      "9704 : Training: loss:  0.045360822\n",
      "9705 : Training: loss:  0.026127541\n",
      "9706 : Training: loss:  0.02302388\n",
      "9707 : Training: loss:  0.017165806\n",
      "9708 : Training: loss:  0.00297825\n",
      "9709 : Training: loss:  0.032088142\n",
      "9710 : Training: loss:  0.045037527\n",
      "9711 : Training: loss:  0.027159955\n",
      "9712 : Training: loss:  0.027553435\n",
      "9713 : Training: loss:  0.026645115\n",
      "9714 : Training: loss:  0.041920375\n",
      "9715 : Training: loss:  0.0102007985\n",
      "9716 : Training: loss:  0.05460155\n",
      "9717 : Training: loss:  0.022797143\n",
      "9718 : Training: loss:  0.05316076\n",
      "9719 : Training: loss:  0.010131156\n",
      "9720 : Training: loss:  0.04192572\n",
      "Validation: Loss:  0.054587487  Accuracy:  0.78846157\n",
      "9721 : Training: loss:  0.02390831\n",
      "9722 : Training: loss:  0.018110396\n",
      "9723 : Training: loss:  0.02950711\n",
      "9724 : Training: loss:  0.052695636\n",
      "9725 : Training: loss:  0.01536869\n",
      "9726 : Training: loss:  0.018294841\n",
      "9727 : Training: loss:  0.02850275\n",
      "9728 : Training: loss:  0.026732173\n",
      "9729 : Training: loss:  0.048681088\n",
      "9730 : Training: loss:  0.00814803\n",
      "9731 : Training: loss:  0.035965025\n",
      "9732 : Training: loss:  0.005897918\n",
      "9733 : Training: loss:  0.017611071\n",
      "9734 : Training: loss:  0.03666809\n",
      "9735 : Training: loss:  0.025928954\n",
      "9736 : Training: loss:  0.010226464\n",
      "9737 : Training: loss:  0.01565832\n",
      "9738 : Training: loss:  0.027483393\n",
      "9739 : Training: loss:  0.021441648\n",
      "9740 : Training: loss:  0.008021122\n",
      "Validation: Loss:  0.05441319  Accuracy:  0.7692308\n",
      "9741 : Training: loss:  0.023194935\n",
      "9742 : Training: loss:  0.050093956\n",
      "9743 : Training: loss:  0.013425834\n",
      "9744 : Training: loss:  0.034175247\n",
      "9745 : Training: loss:  0.020956576\n",
      "9746 : Training: loss:  0.01235392\n",
      "9747 : Training: loss:  0.015659794\n",
      "9748 : Training: loss:  0.002911875\n",
      "9749 : Training: loss:  0.008123286\n",
      "9750 : Training: loss:  0.07135897\n",
      "9751 : Training: loss:  0.025151202\n",
      "9752 : Training: loss:  0.017602472\n",
      "9753 : Training: loss:  0.02998774\n",
      "9754 : Training: loss:  0.00815249\n",
      "9755 : Training: loss:  0.030462502\n",
      "9756 : Training: loss:  0.05040237\n",
      "9757 : Training: loss:  0.013860423\n",
      "9758 : Training: loss:  0.003518798\n",
      "9759 : Training: loss:  0.01612492\n",
      "9760 : Training: loss:  0.027230917\n",
      "Validation: Loss:  0.054644488  Accuracy:  0.75\n",
      "9761 : Training: loss:  0.02620509\n",
      "9762 : Training: loss:  0.034439202\n",
      "9763 : Training: loss:  0.030192308\n",
      "9764 : Training: loss:  0.043547653\n",
      "9765 : Training: loss:  0.026052048\n",
      "9766 : Training: loss:  0.03621492\n",
      "9767 : Training: loss:  0.002332316\n",
      "9768 : Training: loss:  0.025047334\n",
      "9769 : Training: loss:  0.027944084\n",
      "9770 : Training: loss:  0.029948236\n",
      "9771 : Training: loss:  0.012420935\n",
      "9772 : Training: loss:  0.054374967\n",
      "9773 : Training: loss:  0.021727964\n",
      "9774 : Training: loss:  0.019234387\n",
      "9775 : Training: loss:  0.022614388\n",
      "9776 : Training: loss:  0.02492248\n",
      "9777 : Training: loss:  0.010212102\n",
      "9778 : Training: loss:  0.017428225\n",
      "9779 : Training: loss:  0.022626175\n",
      "9780 : Training: loss:  0.00977065\n",
      "Validation: Loss:  0.054587297  Accuracy:  0.75\n",
      "9781 : Training: loss:  0.02718481\n",
      "9782 : Training: loss:  0.007160328\n",
      "9783 : Training: loss:  0.016046962\n",
      "9784 : Training: loss:  0.041533895\n",
      "9785 : Training: loss:  0.0072050686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9786 : Training: loss:  0.023471644\n",
      "9787 : Training: loss:  0.026396062\n",
      "9788 : Training: loss:  0.03810196\n",
      "9789 : Training: loss:  0.0543016\n",
      "9790 : Training: loss:  0.023915336\n",
      "9791 : Training: loss:  0.037105363\n",
      "9792 : Training: loss:  0.016830916\n",
      "9793 : Training: loss:  0.007485111\n",
      "9794 : Training: loss:  0.036723062\n",
      "9795 : Training: loss:  0.04237258\n",
      "9796 : Training: loss:  0.027562793\n",
      "9797 : Training: loss:  0.010548719\n",
      "9798 : Training: loss:  0.022814356\n",
      "9799 : Training: loss:  0.008854289\n",
      "9800 : Training: loss:  0.008664917\n",
      "Validation: Loss:  0.054628193  Accuracy:  0.75\n",
      "9801 : Training: loss:  0.030565483\n",
      "9802 : Training: loss:  0.029426105\n",
      "9803 : Training: loss:  0.019329093\n",
      "9804 : Training: loss:  0.018085167\n",
      "9805 : Training: loss:  0.017944027\n",
      "9806 : Training: loss:  0.0054993182\n",
      "9807 : Training: loss:  0.015143093\n",
      "9808 : Training: loss:  0.0054034404\n",
      "9809 : Training: loss:  0.017971613\n",
      "9810 : Training: loss:  0.043323286\n",
      "9811 : Training: loss:  0.04640864\n",
      "9812 : Training: loss:  0.022957504\n",
      "9813 : Training: loss:  0.031178128\n",
      "9814 : Training: loss:  0.026515208\n",
      "9815 : Training: loss:  0.023836423\n",
      "9816 : Training: loss:  0.02237645\n",
      "9817 : Training: loss:  0.007347107\n",
      "9818 : Training: loss:  0.014520234\n",
      "9819 : Training: loss:  0.0069953334\n",
      "9820 : Training: loss:  0.03101107\n",
      "Validation: Loss:  0.054510374  Accuracy:  0.75\n",
      "9821 : Training: loss:  0.0056834966\n",
      "9822 : Training: loss:  0.022991499\n",
      "9823 : Training: loss:  0.036596097\n",
      "9824 : Training: loss:  0.023267712\n",
      "9825 : Training: loss:  0.02176891\n",
      "9826 : Training: loss:  0.027199173\n",
      "9827 : Training: loss:  0.08382281\n",
      "9828 : Training: loss:  0.01438037\n",
      "9829 : Training: loss:  0.022744633\n",
      "9830 : Training: loss:  0.04987817\n",
      "9831 : Training: loss:  0.01885527\n",
      "9832 : Training: loss:  0.03334529\n",
      "9833 : Training: loss:  0.029546501\n",
      "9834 : Training: loss:  0.010790165\n",
      "9835 : Training: loss:  0.033898454\n",
      "9836 : Training: loss:  0.033762004\n",
      "9837 : Training: loss:  0.017683586\n",
      "9838 : Training: loss:  0.055563543\n",
      "9839 : Training: loss:  0.023142697\n",
      "9840 : Training: loss:  0.02218231\n",
      "Validation: Loss:  0.05463457  Accuracy:  0.7692308\n",
      "9841 : Training: loss:  0.020095848\n",
      "9842 : Training: loss:  0.044006206\n",
      "9843 : Training: loss:  0.018319694\n",
      "9844 : Training: loss:  0.020790055\n",
      "9845 : Training: loss:  0.009059038\n",
      "9846 : Training: loss:  0.031184463\n",
      "9847 : Training: loss:  0.024045922\n",
      "9848 : Training: loss:  0.050004985\n",
      "9849 : Training: loss:  0.018349148\n",
      "9850 : Training: loss:  0.02917103\n",
      "9851 : Training: loss:  0.034639556\n",
      "9852 : Training: loss:  0.020224178\n",
      "9853 : Training: loss:  0.029797483\n",
      "9854 : Training: loss:  0.029535376\n",
      "9855 : Training: loss:  0.03156659\n",
      "9856 : Training: loss:  0.016015075\n",
      "9857 : Training: loss:  0.04632022\n",
      "9858 : Training: loss:  0.031412046\n",
      "9859 : Training: loss:  0.0134350415\n",
      "9860 : Training: loss:  0.022313688\n",
      "Validation: Loss:  0.054572776  Accuracy:  0.7692308\n",
      "9861 : Training: loss:  0.028305463\n",
      "9862 : Training: loss:  0.008269648\n",
      "9863 : Training: loss:  0.018510826\n",
      "9864 : Training: loss:  0.016239783\n",
      "9865 : Training: loss:  0.024494683\n",
      "9866 : Training: loss:  0.03660298\n",
      "9867 : Training: loss:  0.05102764\n",
      "9868 : Training: loss:  0.023443004\n",
      "9869 : Training: loss:  0.073930874\n",
      "9870 : Training: loss:  0.038736995\n",
      "9871 : Training: loss:  0.02252939\n",
      "9872 : Training: loss:  0.015679035\n",
      "9873 : Training: loss:  0.014189028\n",
      "9874 : Training: loss:  0.009723837\n",
      "9875 : Training: loss:  0.010577606\n",
      "9876 : Training: loss:  0.045075014\n",
      "9877 : Training: loss:  0.06741069\n",
      "9878 : Training: loss:  0.03445266\n",
      "9879 : Training: loss:  0.037921455\n",
      "9880 : Training: loss:  0.025636312\n",
      "Validation: Loss:  0.054368086  Accuracy:  0.7692308\n",
      "9881 : Training: loss:  0.031156633\n",
      "9882 : Training: loss:  0.015694968\n",
      "9883 : Training: loss:  0.025658501\n",
      "9884 : Training: loss:  0.032553405\n",
      "9885 : Training: loss:  0.0141844945\n",
      "9886 : Training: loss:  0.034744974\n",
      "9887 : Training: loss:  0.033610042\n",
      "9888 : Training: loss:  0.046624344\n",
      "9889 : Training: loss:  0.026957631\n",
      "9890 : Training: loss:  0.01819416\n",
      "9891 : Training: loss:  0.04671609\n",
      "9892 : Training: loss:  0.049038313\n",
      "9893 : Training: loss:  0.05685051\n",
      "9894 : Training: loss:  0.030709434\n",
      "9895 : Training: loss:  0.031867154\n",
      "9896 : Training: loss:  0.025390014\n",
      "9897 : Training: loss:  0.018387036\n",
      "9898 : Training: loss:  0.03343358\n",
      "9899 : Training: loss:  0.041630458\n",
      "9900 : Training: loss:  0.02518107\n",
      "Validation: Loss:  0.05447172  Accuracy:  0.7692308\n",
      "9901 : Training: loss:  0.009672664\n",
      "9902 : Training: loss:  0.015376559\n",
      "9903 : Training: loss:  0.030406037\n",
      "9904 : Training: loss:  0.022265581\n",
      "9905 : Training: loss:  0.04899333\n",
      "9906 : Training: loss:  0.069895945\n",
      "9907 : Training: loss:  0.008867424\n",
      "9908 : Training: loss:  0.05468846\n",
      "9909 : Training: loss:  0.024938866\n",
      "9910 : Training: loss:  0.045237515\n",
      "9911 : Training: loss:  0.01471178\n",
      "9912 : Training: loss:  0.0083844215\n",
      "9913 : Training: loss:  0.015461337\n",
      "9914 : Training: loss:  0.029175673\n",
      "9915 : Training: loss:  0.01260487\n",
      "9916 : Training: loss:  0.027197642\n",
      "9917 : Training: loss:  0.033871274\n",
      "9918 : Training: loss:  0.0077853114\n",
      "9919 : Training: loss:  0.021952882\n",
      "9920 : Training: loss:  0.010619839\n",
      "Validation: Loss:  0.054468084  Accuracy:  0.8076923\n",
      "9921 : Training: loss:  0.026798451\n",
      "9922 : Training: loss:  0.046977684\n",
      "9923 : Training: loss:  0.044525575\n",
      "9924 : Training: loss:  0.042803697\n",
      "9925 : Training: loss:  0.027561847\n",
      "9926 : Training: loss:  0.015390696\n",
      "9927 : Training: loss:  0.018883074\n",
      "9928 : Training: loss:  0.05218956\n",
      "9929 : Training: loss:  0.0024551558\n",
      "9930 : Training: loss:  0.03711455\n",
      "9931 : Training: loss:  0.0182019\n",
      "9932 : Training: loss:  0.03717342\n",
      "9933 : Training: loss:  0.02832717\n",
      "9934 : Training: loss:  0.040580604\n",
      "9935 : Training: loss:  0.029462656\n",
      "9936 : Training: loss:  0.017288515\n",
      "9937 : Training: loss:  0.026338529\n",
      "9938 : Training: loss:  0.006792986\n",
      "9939 : Training: loss:  0.04195978\n",
      "9940 : Training: loss:  0.035386685\n",
      "Validation: Loss:  0.05462748  Accuracy:  0.78846157\n",
      "9941 : Training: loss:  0.030993598\n",
      "9942 : Training: loss:  0.033616874\n",
      "9943 : Training: loss:  0.024866624\n",
      "9944 : Training: loss:  0.050270375\n",
      "9945 : Training: loss:  0.05790707\n",
      "9946 : Training: loss:  0.044481643\n",
      "9947 : Training: loss:  0.028638996\n",
      "9948 : Training: loss:  0.023283359\n",
      "9949 : Training: loss:  0.024495948\n",
      "9950 : Training: loss:  0.033517256\n",
      "9951 : Training: loss:  0.027948843\n",
      "9952 : Training: loss:  0.01834022\n",
      "9953 : Training: loss:  0.031421732\n",
      "9954 : Training: loss:  0.0036629676\n",
      "9955 : Training: loss:  0.038793907\n",
      "9956 : Training: loss:  0.03404661\n",
      "9957 : Training: loss:  0.015405929\n",
      "9958 : Training: loss:  0.031533904\n",
      "9959 : Training: loss:  0.035837557\n",
      "9960 : Training: loss:  0.029890811\n",
      "Validation: Loss:  0.05456717  Accuracy:  0.7692308\n",
      "9961 : Training: loss:  0.054914024\n",
      "9962 : Training: loss:  0.022682045\n",
      "9963 : Training: loss:  0.02925713\n",
      "9964 : Training: loss:  0.025536703\n",
      "9965 : Training: loss:  0.025207149\n",
      "9966 : Training: loss:  0.036488514\n",
      "9967 : Training: loss:  0.030800117\n",
      "9968 : Training: loss:  0.030182304\n",
      "9969 : Training: loss:  0.025658034\n",
      "9970 : Training: loss:  0.031733494\n",
      "9971 : Training: loss:  0.044348676\n",
      "9972 : Training: loss:  0.040123\n",
      "9973 : Training: loss:  0.045920365\n",
      "9974 : Training: loss:  0.004651136\n",
      "9975 : Training: loss:  0.030365702\n",
      "9976 : Training: loss:  0.022701964\n",
      "9977 : Training: loss:  0.023824861\n",
      "9978 : Training: loss:  0.03152644\n",
      "9979 : Training: loss:  0.041999374\n",
      "9980 : Training: loss:  0.03295348\n",
      "Validation: Loss:  0.054229733  Accuracy:  0.8076923\n",
      "9981 : Training: loss:  0.030070348\n",
      "9982 : Training: loss:  0.069957845\n",
      "9983 : Training: loss:  0.0326173\n",
      "9984 : Training: loss:  0.046828523\n",
      "9985 : Training: loss:  0.027926538\n",
      "9986 : Training: loss:  0.030379625\n",
      "9987 : Training: loss:  0.026686277\n",
      "9988 : Training: loss:  0.015765594\n",
      "9989 : Training: loss:  0.017507346\n",
      "9990 : Training: loss:  0.015984148\n",
      "9991 : Training: loss:  0.020773018\n",
      "9992 : Training: loss:  0.02700782\n",
      "9993 : Training: loss:  0.044543546\n",
      "9994 : Training: loss:  0.04217532\n",
      "9995 : Training: loss:  0.039982915\n",
      "9996 : Training: loss:  0.02828393\n",
      "9997 : Training: loss:  0.036286063\n",
      "9998 : Training: loss:  0.038739648\n",
      "9999 : Training: loss:  0.05492474\n",
      "10000 : Training: loss:  0.004786149\n",
      "Validation: Loss:  0.054131173  Accuracy:  0.78846157\n",
      "10001 : Training: loss:  0.06092598\n",
      "10002 : Training: loss:  0.028224025\n",
      "10003 : Training: loss:  0.038093947\n",
      "10004 : Training: loss:  0.051926002\n",
      "10005 : Training: loss:  0.015687259\n",
      "10006 : Training: loss:  0.032635286\n",
      "10007 : Training: loss:  0.019498445\n",
      "10008 : Training: loss:  0.030991718\n",
      "10009 : Training: loss:  0.027435532\n",
      "10010 : Training: loss:  0.018850658\n",
      "10011 : Training: loss:  0.035875782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10012 : Training: loss:  0.022761993\n",
      "10013 : Training: loss:  0.037480943\n",
      "10014 : Training: loss:  0.025512448\n",
      "10015 : Training: loss:  0.015628366\n",
      "10016 : Training: loss:  0.031400684\n",
      "10017 : Training: loss:  0.06225862\n",
      "10018 : Training: loss:  0.018342307\n",
      "10019 : Training: loss:  0.017716248\n",
      "10020 : Training: loss:  0.009792453\n",
      "Validation: Loss:  0.054288708  Accuracy:  0.8076923\n",
      "10021 : Training: loss:  0.03596268\n",
      "10022 : Training: loss:  0.037938856\n",
      "10023 : Training: loss:  0.03543138\n",
      "10024 : Training: loss:  0.039189544\n",
      "10025 : Training: loss:  0.013986541\n",
      "10026 : Training: loss:  0.0143715665\n",
      "10027 : Training: loss:  0.035052124\n",
      "10028 : Training: loss:  0.009041719\n",
      "10029 : Training: loss:  0.03360613\n",
      "10030 : Training: loss:  0.009662009\n",
      "10031 : Training: loss:  0.041585818\n",
      "10032 : Training: loss:  0.037281964\n",
      "10033 : Training: loss:  0.033933897\n",
      "10034 : Training: loss:  0.028365476\n",
      "10035 : Training: loss:  0.041243866\n",
      "10036 : Training: loss:  0.021498619\n",
      "10037 : Training: loss:  0.0065770173\n",
      "10038 : Training: loss:  0.032596853\n",
      "10039 : Training: loss:  0.006495897\n",
      "10040 : Training: loss:  0.038445823\n",
      "Validation: Loss:  0.05413841  Accuracy:  0.8076923\n",
      "10041 : Training: loss:  0.04125629\n",
      "10042 : Training: loss:  0.025783336\n",
      "10043 : Training: loss:  0.013952734\n",
      "10044 : Training: loss:  0.013347584\n",
      "10045 : Training: loss:  0.033158924\n",
      "10046 : Training: loss:  0.04725032\n",
      "10047 : Training: loss:  0.010404736\n",
      "10048 : Training: loss:  0.012580382\n",
      "10049 : Training: loss:  0.0260427\n",
      "10050 : Training: loss:  0.02013463\n",
      "10051 : Training: loss:  0.014795877\n",
      "10052 : Training: loss:  0.03674186\n",
      "10053 : Training: loss:  0.07199225\n",
      "10054 : Training: loss:  0.06884645\n",
      "10055 : Training: loss:  0.045401454\n",
      "10056 : Training: loss:  0.004896213\n",
      "10057 : Training: loss:  0.035896506\n",
      "10058 : Training: loss:  0.011921733\n",
      "10059 : Training: loss:  0.03682515\n",
      "10060 : Training: loss:  0.03157033\n",
      "Validation: Loss:  0.054010414  Accuracy:  0.8076923\n",
      "10061 : Training: loss:  0.026849838\n",
      "10062 : Training: loss:  0.008088478\n",
      "10063 : Training: loss:  0.0330156\n",
      "10064 : Training: loss:  0.007411388\n",
      "10065 : Training: loss:  0.013311643\n",
      "10066 : Training: loss:  0.017792366\n",
      "10067 : Training: loss:  0.013099056\n",
      "10068 : Training: loss:  0.020536661\n",
      "10069 : Training: loss:  0.0047540804\n",
      "10070 : Training: loss:  0.013499975\n",
      "10071 : Training: loss:  0.028535938\n",
      "10072 : Training: loss:  0.029810075\n",
      "10073 : Training: loss:  0.04550307\n",
      "10074 : Training: loss:  0.020309582\n",
      "10075 : Training: loss:  0.03079922\n",
      "10076 : Training: loss:  0.038702313\n",
      "10077 : Training: loss:  0.026899219\n",
      "10078 : Training: loss:  0.023625953\n",
      "10079 : Training: loss:  0.028442394\n",
      "10080 : Training: loss:  0.022309626\n",
      "Validation: Loss:  0.053975582  Accuracy:  0.8269231\n",
      "10081 : Training: loss:  0.01827232\n",
      "10082 : Training: loss:  0.028302284\n",
      "10083 : Training: loss:  0.0057449816\n",
      "10084 : Training: loss:  0.022648688\n",
      "10085 : Training: loss:  0.036228627\n",
      "10086 : Training: loss:  0.021678703\n",
      "10087 : Training: loss:  0.011133462\n",
      "10088 : Training: loss:  0.052168623\n",
      "10089 : Training: loss:  0.041672166\n",
      "10090 : Training: loss:  0.033227667\n",
      "10091 : Training: loss:  0.012288058\n",
      "10092 : Training: loss:  0.02289935\n",
      "10093 : Training: loss:  0.03987649\n",
      "10094 : Training: loss:  0.033052374\n",
      "10095 : Training: loss:  0.028515879\n",
      "10096 : Training: loss:  0.03300843\n",
      "10097 : Training: loss:  0.0111305425\n",
      "10098 : Training: loss:  0.019742608\n",
      "10099 : Training: loss:  0.0472459\n",
      "10100 : Training: loss:  0.0042792\n",
      "Validation: Loss:  0.054086223  Accuracy:  0.8269231\n",
      "10101 : Training: loss:  0.023738649\n",
      "10102 : Training: loss:  0.027196756\n",
      "10103 : Training: loss:  0.030952206\n",
      "10104 : Training: loss:  0.011060438\n",
      "10105 : Training: loss:  0.03276555\n",
      "10106 : Training: loss:  0.017945599\n",
      "10107 : Training: loss:  0.04123679\n",
      "10108 : Training: loss:  0.02128149\n",
      "10109 : Training: loss:  0.014782916\n",
      "10110 : Training: loss:  0.02904383\n",
      "10111 : Training: loss:  0.018083287\n",
      "10112 : Training: loss:  0.005278958\n",
      "10113 : Training: loss:  0.026374184\n",
      "10114 : Training: loss:  0.010719551\n",
      "10115 : Training: loss:  0.019156635\n",
      "10116 : Training: loss:  0.047396347\n",
      "10117 : Training: loss:  0.024207138\n",
      "10118 : Training: loss:  0.029533593\n",
      "10119 : Training: loss:  0.0104601635\n",
      "10120 : Training: loss:  0.06327432\n",
      "Validation: Loss:  0.054162495  Accuracy:  0.8076923\n",
      "10121 : Training: loss:  0.027251448\n",
      "10122 : Training: loss:  0.01574585\n",
      "10123 : Training: loss:  0.026445802\n",
      "10124 : Training: loss:  0.008567841\n",
      "10125 : Training: loss:  0.022523949\n",
      "10126 : Training: loss:  0.04353442\n",
      "10127 : Training: loss:  0.037879184\n",
      "10128 : Training: loss:  0.03424255\n",
      "10129 : Training: loss:  0.038895506\n",
      "10130 : Training: loss:  0.011118216\n",
      "10131 : Training: loss:  0.0112240845\n",
      "10132 : Training: loss:  0.023450945\n",
      "10133 : Training: loss:  0.005130306\n",
      "10134 : Training: loss:  0.025269862\n",
      "10135 : Training: loss:  0.019178668\n",
      "10136 : Training: loss:  0.023812745\n",
      "10137 : Training: loss:  0.0039380784\n",
      "10138 : Training: loss:  0.012443065\n",
      "10139 : Training: loss:  0.013249619\n",
      "10140 : Training: loss:  0.019145273\n",
      "Validation: Loss:  0.054264177  Accuracy:  0.78846157\n",
      "10141 : Training: loss:  0.048937067\n",
      "10142 : Training: loss:  0.043398716\n",
      "10143 : Training: loss:  0.028099652\n",
      "10144 : Training: loss:  0.021541076\n",
      "10145 : Training: loss:  0.027109208\n",
      "10146 : Training: loss:  0.010334156\n",
      "10147 : Training: loss:  0.04582501\n",
      "10148 : Training: loss:  0.03188624\n",
      "10149 : Training: loss:  0.024828732\n",
      "10150 : Training: loss:  0.041080173\n",
      "10151 : Training: loss:  0.006256614\n",
      "10152 : Training: loss:  0.02563822\n",
      "10153 : Training: loss:  0.026432013\n",
      "10154 : Training: loss:  0.045192473\n",
      "10155 : Training: loss:  0.00178658\n",
      "10156 : Training: loss:  0.015200276\n",
      "10157 : Training: loss:  0.026698254\n",
      "10158 : Training: loss:  0.0137454625\n",
      "10159 : Training: loss:  0.0141362\n",
      "10160 : Training: loss:  0.014609277\n",
      "Validation: Loss:  0.05415966  Accuracy:  0.8076923\n",
      "10161 : Training: loss:  0.040828113\n",
      "10162 : Training: loss:  0.031480376\n",
      "10163 : Training: loss:  0.033826932\n",
      "10164 : Training: loss:  0.059619483\n",
      "10165 : Training: loss:  0.031686343\n",
      "10166 : Training: loss:  0.013506192\n",
      "10167 : Training: loss:  0.03385046\n",
      "10168 : Training: loss:  0.017754786\n",
      "10169 : Training: loss:  0.010842318\n",
      "10170 : Training: loss:  0.020364119\n",
      "10171 : Training: loss:  0.03760764\n",
      "10172 : Training: loss:  0.004844799\n",
      "10173 : Training: loss:  0.010003944\n",
      "10174 : Training: loss:  0.023242533\n",
      "10175 : Training: loss:  0.008416497\n",
      "10176 : Training: loss:  0.053110734\n",
      "10177 : Training: loss:  0.021898406\n",
      "10178 : Training: loss:  0.01904461\n",
      "10179 : Training: loss:  0.020679018\n",
      "10180 : Training: loss:  0.059227426\n",
      "Validation: Loss:  0.054124244  Accuracy:  0.8076923\n",
      "10181 : Training: loss:  0.03100835\n",
      "10182 : Training: loss:  0.010410666\n",
      "10183 : Training: loss:  0.0033730597\n",
      "10184 : Training: loss:  0.017546626\n",
      "10185 : Training: loss:  0.011197295\n",
      "10186 : Training: loss:  0.02408839\n",
      "10187 : Training: loss:  0.005052013\n",
      "10188 : Training: loss:  0.020370366\n",
      "10189 : Training: loss:  0.011866389\n",
      "10190 : Training: loss:  0.012530362\n",
      "10191 : Training: loss:  0.0076140086\n",
      "10192 : Training: loss:  0.03624735\n",
      "10193 : Training: loss:  0.034356304\n",
      "10194 : Training: loss:  0.039578702\n",
      "10195 : Training: loss:  0.024039296\n",
      "10196 : Training: loss:  0.008426636\n",
      "10197 : Training: loss:  0.034955904\n",
      "10198 : Training: loss:  0.013181325\n",
      "10199 : Training: loss:  0.010361319\n",
      "10200 : Training: loss:  0.010869623\n",
      "Validation: Loss:  0.05395823  Accuracy:  0.8076923\n",
      "10201 : Training: loss:  0.04180174\n",
      "10202 : Training: loss:  0.004880392\n",
      "10203 : Training: loss:  0.0045233825\n",
      "10204 : Training: loss:  0.022236729\n",
      "10205 : Training: loss:  0.038392015\n",
      "10206 : Training: loss:  0.015166269\n",
      "10207 : Training: loss:  0.030567305\n",
      "10208 : Training: loss:  0.04029102\n",
      "10209 : Training: loss:  0.029145433\n",
      "10210 : Training: loss:  0.046380877\n",
      "10211 : Training: loss:  0.023805538\n",
      "10212 : Training: loss:  0.061760068\n",
      "10213 : Training: loss:  0.030260004\n",
      "10214 : Training: loss:  0.0025425768\n",
      "10215 : Training: loss:  0.04337788\n",
      "10216 : Training: loss:  0.017807577\n",
      "10217 : Training: loss:  0.016593061\n",
      "10218 : Training: loss:  0.014162601\n",
      "10219 : Training: loss:  0.03821275\n",
      "10220 : Training: loss:  0.029594418\n",
      "Validation: Loss:  0.0535744  Accuracy:  0.8076923\n",
      "10221 : Training: loss:  0.0179419\n",
      "10222 : Training: loss:  0.022976145\n",
      "10223 : Training: loss:  0.009020985\n",
      "10224 : Training: loss:  0.009005622\n",
      "10225 : Training: loss:  0.032510836\n",
      "10226 : Training: loss:  0.0070329006\n",
      "10227 : Training: loss:  0.024921333\n",
      "10228 : Training: loss:  0.029213276\n",
      "10229 : Training: loss:  0.053957127\n",
      "10230 : Training: loss:  0.06498195\n",
      "10231 : Training: loss:  0.023398137\n",
      "10232 : Training: loss:  0.019376004\n",
      "10233 : Training: loss:  0.012116488\n",
      "10234 : Training: loss:  0.026912415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235 : Training: loss:  0.016030373\n",
      "10236 : Training: loss:  0.040970303\n",
      "10237 : Training: loss:  0.015336231\n",
      "10238 : Training: loss:  0.005680069\n",
      "10239 : Training: loss:  0.006374683\n",
      "10240 : Training: loss:  0.014239424\n",
      "Validation: Loss:  0.053528354  Accuracy:  0.8076923\n",
      "10241 : Training: loss:  0.018611385\n",
      "10242 : Training: loss:  0.017404804\n",
      "10243 : Training: loss:  0.017730886\n",
      "10244 : Training: loss:  0.015977588\n",
      "10245 : Training: loss:  0.026374057\n",
      "10246 : Training: loss:  0.023328261\n",
      "10247 : Training: loss:  0.020058872\n",
      "10248 : Training: loss:  0.022016218\n",
      "10249 : Training: loss:  0.037476484\n",
      "10250 : Training: loss:  0.05034675\n",
      "10251 : Training: loss:  0.019336637\n",
      "10252 : Training: loss:  0.030225305\n",
      "10253 : Training: loss:  0.032135036\n",
      "10254 : Training: loss:  0.009921356\n",
      "10255 : Training: loss:  0.003574569\n",
      "10256 : Training: loss:  0.012482286\n",
      "10257 : Training: loss:  0.04537983\n",
      "10258 : Training: loss:  0.014189727\n",
      "10259 : Training: loss:  0.008675516\n",
      "10260 : Training: loss:  0.03891542\n",
      "Validation: Loss:  0.053537764  Accuracy:  0.8269231\n",
      "10261 : Training: loss:  0.005623168\n",
      "10262 : Training: loss:  0.03227567\n",
      "10263 : Training: loss:  0.024810925\n",
      "10264 : Training: loss:  0.03328445\n",
      "10265 : Training: loss:  0.034532174\n",
      "10266 : Training: loss:  0.019644322\n",
      "10267 : Training: loss:  0.028269045\n",
      "10268 : Training: loss:  0.023120973\n",
      "10269 : Training: loss:  0.027829396\n",
      "10270 : Training: loss:  0.02213455\n",
      "10271 : Training: loss:  0.011332689\n",
      "10272 : Training: loss:  0.03431036\n",
      "10273 : Training: loss:  0.01126816\n",
      "10274 : Training: loss:  0.031587552\n",
      "10275 : Training: loss:  0.040026594\n",
      "10276 : Training: loss:  0.02977387\n",
      "10277 : Training: loss:  0.016272105\n",
      "10278 : Training: loss:  0.0297078\n",
      "10279 : Training: loss:  0.042162254\n",
      "10280 : Training: loss:  0.013404459\n",
      "Validation: Loss:  0.054311503  Accuracy:  0.8076923\n",
      "10281 : Training: loss:  0.061186843\n",
      "10282 : Training: loss:  0.035039444\n",
      "10283 : Training: loss:  0.05832811\n",
      "10284 : Training: loss:  0.029626833\n",
      "10285 : Training: loss:  0.037786413\n",
      "10286 : Training: loss:  0.0029115388\n",
      "10287 : Training: loss:  0.031377845\n",
      "10288 : Training: loss:  0.04272347\n",
      "10289 : Training: loss:  0.010998796\n",
      "10290 : Training: loss:  0.03904686\n",
      "10291 : Training: loss:  0.047556315\n",
      "10292 : Training: loss:  0.038275722\n",
      "10293 : Training: loss:  0.03151138\n",
      "10294 : Training: loss:  0.019510388\n",
      "10295 : Training: loss:  0.04135756\n",
      "10296 : Training: loss:  0.021301795\n",
      "10297 : Training: loss:  0.028681545\n",
      "10298 : Training: loss:  0.01643683\n",
      "10299 : Training: loss:  0.018723333\n",
      "10300 : Training: loss:  0.042359326\n",
      "Validation: Loss:  0.053834785  Accuracy:  0.8076923\n",
      "10301 : Training: loss:  0.026627228\n",
      "10302 : Training: loss:  0.008898537\n",
      "10303 : Training: loss:  0.01713545\n",
      "10304 : Training: loss:  0.040546965\n",
      "10305 : Training: loss:  0.03175663\n",
      "10306 : Training: loss:  0.017929684\n",
      "10307 : Training: loss:  0.008422716\n",
      "10308 : Training: loss:  0.026695851\n",
      "10309 : Training: loss:  0.005731606\n",
      "10310 : Training: loss:  0.049684376\n",
      "10311 : Training: loss:  0.05677461\n",
      "10312 : Training: loss:  0.023792507\n",
      "10313 : Training: loss:  0.018154822\n",
      "10314 : Training: loss:  0.041014336\n",
      "10315 : Training: loss:  0.021418655\n",
      "10316 : Training: loss:  0.007762878\n",
      "10317 : Training: loss:  0.036814917\n",
      "10318 : Training: loss:  0.011540985\n",
      "10319 : Training: loss:  0.05526125\n",
      "10320 : Training: loss:  0.044836212\n",
      "Validation: Loss:  0.053668167  Accuracy:  0.8076923\n",
      "10321 : Training: loss:  0.036433786\n",
      "10322 : Training: loss:  0.04583787\n",
      "10323 : Training: loss:  0.049520846\n",
      "10324 : Training: loss:  0.019634007\n",
      "10325 : Training: loss:  0.035244137\n",
      "10326 : Training: loss:  0.011844464\n",
      "10327 : Training: loss:  0.019150343\n",
      "10328 : Training: loss:  0.01350954\n",
      "10329 : Training: loss:  0.02184615\n",
      "10330 : Training: loss:  0.005913384\n",
      "10331 : Training: loss:  0.01234462\n",
      "10332 : Training: loss:  0.0067077093\n",
      "10333 : Training: loss:  0.013267124\n",
      "10334 : Training: loss:  0.02747682\n",
      "10335 : Training: loss:  0.006023493\n",
      "10336 : Training: loss:  0.01699953\n",
      "10337 : Training: loss:  0.051595494\n",
      "10338 : Training: loss:  0.032853466\n",
      "10339 : Training: loss:  0.031221267\n",
      "10340 : Training: loss:  0.021053765\n",
      "Validation: Loss:  0.053483557  Accuracy:  0.8076923\n",
      "10341 : Training: loss:  0.014364539\n",
      "10342 : Training: loss:  0.006207079\n",
      "10343 : Training: loss:  0.028340613\n",
      "10344 : Training: loss:  0.017602744\n",
      "10345 : Training: loss:  0.029271761\n",
      "10346 : Training: loss:  0.015245221\n",
      "10347 : Training: loss:  0.026126532\n",
      "10348 : Training: loss:  0.0148897655\n",
      "10349 : Training: loss:  0.05456594\n",
      "10350 : Training: loss:  0.014028363\n",
      "10351 : Training: loss:  0.030193184\n",
      "10352 : Training: loss:  0.028406512\n",
      "10353 : Training: loss:  0.023930017\n",
      "10354 : Training: loss:  0.02203956\n",
      "10355 : Training: loss:  0.063033946\n",
      "10356 : Training: loss:  0.011841321\n",
      "10357 : Training: loss:  0.01889682\n",
      "10358 : Training: loss:  0.023236409\n",
      "10359 : Training: loss:  0.011524485\n",
      "10360 : Training: loss:  0.041635342\n",
      "Validation: Loss:  0.05339929  Accuracy:  0.8076923\n",
      "10361 : Training: loss:  0.052051336\n",
      "10362 : Training: loss:  0.02452947\n",
      "10363 : Training: loss:  0.01563795\n",
      "10364 : Training: loss:  0.02757795\n",
      "10365 : Training: loss:  0.03163392\n",
      "10366 : Training: loss:  0.011645787\n",
      "10367 : Training: loss:  0.004411584\n",
      "10368 : Training: loss:  0.03687146\n",
      "10369 : Training: loss:  0.023058686\n",
      "10370 : Training: loss:  0.025313511\n",
      "10371 : Training: loss:  0.031265203\n",
      "10372 : Training: loss:  0.041736227\n",
      "10373 : Training: loss:  0.025013022\n",
      "10374 : Training: loss:  0.018909832\n",
      "10375 : Training: loss:  0.031696502\n",
      "10376 : Training: loss:  0.028436664\n",
      "10377 : Training: loss:  0.017048134\n",
      "10378 : Training: loss:  0.037513264\n",
      "10379 : Training: loss:  0.01817793\n",
      "10380 : Training: loss:  0.017037194\n",
      "Validation: Loss:  0.05373649  Accuracy:  0.8076923\n",
      "10381 : Training: loss:  0.018242272\n",
      "10382 : Training: loss:  0.03916614\n",
      "10383 : Training: loss:  0.038592212\n",
      "10384 : Training: loss:  0.056868497\n",
      "10385 : Training: loss:  0.021430694\n",
      "10386 : Training: loss:  0.027567768\n",
      "10387 : Training: loss:  0.00922103\n",
      "10388 : Training: loss:  0.045434646\n",
      "10389 : Training: loss:  0.0038128633\n",
      "10390 : Training: loss:  0.05304171\n",
      "10391 : Training: loss:  0.020982552\n",
      "10392 : Training: loss:  0.010257762\n",
      "10393 : Training: loss:  0.028304854\n",
      "10394 : Training: loss:  0.015313302\n",
      "10395 : Training: loss:  0.0058230935\n",
      "10396 : Training: loss:  0.01524729\n",
      "10397 : Training: loss:  0.028916273\n",
      "10398 : Training: loss:  0.0083261235\n",
      "10399 : Training: loss:  0.038069632\n",
      "10400 : Training: loss:  0.02083639\n",
      "Validation: Loss:  0.053711366  Accuracy:  0.8269231\n",
      "10401 : Training: loss:  0.03100643\n",
      "10402 : Training: loss:  0.0031156763\n",
      "10403 : Training: loss:  0.030129347\n",
      "10404 : Training: loss:  0.033832304\n",
      "10405 : Training: loss:  0.03392059\n",
      "10406 : Training: loss:  0.03581442\n",
      "10407 : Training: loss:  0.021134341\n",
      "10408 : Training: loss:  0.011502205\n",
      "10409 : Training: loss:  0.036824707\n",
      "10410 : Training: loss:  0.015217084\n",
      "10411 : Training: loss:  0.024918307\n",
      "10412 : Training: loss:  0.04120493\n",
      "10413 : Training: loss:  0.033671632\n",
      "10414 : Training: loss:  0.031360254\n",
      "10415 : Training: loss:  0.03862163\n",
      "10416 : Training: loss:  0.015612652\n",
      "10417 : Training: loss:  0.023636946\n",
      "10418 : Training: loss:  0.048452076\n",
      "10419 : Training: loss:  0.015151715\n",
      "10420 : Training: loss:  0.013250144\n",
      "Validation: Loss:  0.053756025  Accuracy:  0.8076923\n",
      "10421 : Training: loss:  0.04377272\n",
      "10422 : Training: loss:  0.017037597\n",
      "10423 : Training: loss:  0.011544827\n",
      "10424 : Training: loss:  0.017401999\n",
      "10425 : Training: loss:  0.009690561\n",
      "10426 : Training: loss:  0.013369388\n",
      "10427 : Training: loss:  0.018322373\n",
      "10428 : Training: loss:  0.030823113\n",
      "10429 : Training: loss:  0.05654847\n",
      "10430 : Training: loss:  0.011428029\n",
      "10431 : Training: loss:  0.013599067\n",
      "10432 : Training: loss:  0.043430652\n",
      "10433 : Training: loss:  0.040653806\n",
      "10434 : Training: loss:  0.02874861\n",
      "10435 : Training: loss:  0.035018057\n",
      "10436 : Training: loss:  0.03233899\n",
      "10437 : Training: loss:  0.029749671\n",
      "10438 : Training: loss:  0.02623678\n",
      "10439 : Training: loss:  0.033603415\n",
      "10440 : Training: loss:  0.027928837\n",
      "Validation: Loss:  0.053666763  Accuracy:  0.8076923\n",
      "10441 : Training: loss:  0.04106025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10442 : Training: loss:  0.015880872\n",
      "10443 : Training: loss:  0.021225695\n",
      "10444 : Training: loss:  0.009840131\n",
      "10445 : Training: loss:  0.036251612\n",
      "10446 : Training: loss:  0.006503182\n",
      "10447 : Training: loss:  0.013307818\n",
      "10448 : Training: loss:  0.026065936\n",
      "10449 : Training: loss:  0.032341048\n",
      "10450 : Training: loss:  0.03317819\n",
      "10451 : Training: loss:  0.019503541\n",
      "10452 : Training: loss:  0.022627342\n",
      "10453 : Training: loss:  0.019364906\n",
      "10454 : Training: loss:  0.02637737\n",
      "10455 : Training: loss:  0.0052438974\n",
      "10456 : Training: loss:  0.015262063\n",
      "10457 : Training: loss:  0.00865063\n",
      "10458 : Training: loss:  0.014753717\n",
      "10459 : Training: loss:  0.011375328\n",
      "10460 : Training: loss:  0.013384699\n",
      "Validation: Loss:  0.05355013  Accuracy:  0.8076923\n",
      "10461 : Training: loss:  0.03318867\n",
      "10462 : Training: loss:  0.02552012\n",
      "10463 : Training: loss:  0.025109543\n",
      "10464 : Training: loss:  0.02251678\n",
      "10465 : Training: loss:  0.0313265\n",
      "10466 : Training: loss:  0.019339796\n",
      "10467 : Training: loss:  0.03364118\n",
      "10468 : Training: loss:  0.04382843\n",
      "10469 : Training: loss:  0.020369899\n",
      "10470 : Training: loss:  0.03933837\n",
      "10471 : Training: loss:  0.01902688\n",
      "10472 : Training: loss:  0.009351075\n",
      "10473 : Training: loss:  0.03565102\n",
      "10474 : Training: loss:  0.017277882\n",
      "10475 : Training: loss:  0.032382097\n",
      "10476 : Training: loss:  0.0055576945\n",
      "10477 : Training: loss:  0.016811725\n",
      "10478 : Training: loss:  0.009858286\n",
      "10479 : Training: loss:  0.049307294\n",
      "10480 : Training: loss:  0.00907347\n",
      "Validation: Loss:  0.053749014  Accuracy:  0.8076923\n",
      "10481 : Training: loss:  0.05212101\n",
      "10482 : Training: loss:  0.046215244\n",
      "10483 : Training: loss:  0.018430637\n",
      "10484 : Training: loss:  0.030979333\n",
      "10485 : Training: loss:  0.058733568\n",
      "10486 : Training: loss:  0.032644983\n",
      "10487 : Training: loss:  0.022574984\n",
      "10488 : Training: loss:  0.031072674\n",
      "10489 : Training: loss:  0.019035127\n",
      "10490 : Training: loss:  0.017974902\n",
      "10491 : Training: loss:  0.031621493\n",
      "10492 : Training: loss:  0.04371427\n",
      "10493 : Training: loss:  0.04820807\n",
      "10494 : Training: loss:  0.019178625\n",
      "10495 : Training: loss:  0.007872831\n",
      "10496 : Training: loss:  0.008006732\n",
      "10497 : Training: loss:  0.028681748\n",
      "10498 : Training: loss:  0.023580743\n",
      "10499 : Training: loss:  0.023008386\n",
      "10500 : Training: loss:  0.008647892\n",
      "Validation: Loss:  0.054007865  Accuracy:  0.8269231\n",
      "10501 : Training: loss:  0.019535434\n",
      "10502 : Training: loss:  0.014997405\n",
      "10503 : Training: loss:  0.015169715\n",
      "10504 : Training: loss:  0.023972746\n",
      "10505 : Training: loss:  0.058714475\n",
      "10506 : Training: loss:  0.042499527\n",
      "10507 : Training: loss:  0.025272856\n",
      "10508 : Training: loss:  0.031076107\n",
      "10509 : Training: loss:  0.021924892\n",
      "10510 : Training: loss:  0.0376137\n",
      "10511 : Training: loss:  0.03603447\n",
      "10512 : Training: loss:  0.011268304\n",
      "10513 : Training: loss:  0.022096423\n",
      "10514 : Training: loss:  0.0052747508\n",
      "10515 : Training: loss:  0.045158703\n",
      "10516 : Training: loss:  0.035367284\n",
      "10517 : Training: loss:  0.027682284\n",
      "10518 : Training: loss:  0.008353705\n",
      "10519 : Training: loss:  0.0064749205\n",
      "10520 : Training: loss:  0.03975003\n",
      "Validation: Loss:  0.054048054  Accuracy:  0.78846157\n",
      "10521 : Training: loss:  0.010502689\n",
      "10522 : Training: loss:  0.014378231\n",
      "10523 : Training: loss:  0.019361846\n",
      "10524 : Training: loss:  0.029844536\n",
      "10525 : Training: loss:  0.019069476\n",
      "10526 : Training: loss:  0.017829144\n",
      "10527 : Training: loss:  0.011161725\n",
      "10528 : Training: loss:  0.01794988\n",
      "10529 : Training: loss:  0.042179536\n",
      "10530 : Training: loss:  0.018925026\n",
      "10531 : Training: loss:  0.014282041\n",
      "10532 : Training: loss:  0.02601296\n",
      "10533 : Training: loss:  0.04320165\n",
      "10534 : Training: loss:  0.020947639\n",
      "10535 : Training: loss:  0.032387976\n",
      "10536 : Training: loss:  0.04367206\n",
      "10537 : Training: loss:  0.011286918\n",
      "10538 : Training: loss:  0.021274235\n",
      "10539 : Training: loss:  0.017500184\n",
      "10540 : Training: loss:  0.02023795\n",
      "Validation: Loss:  0.054203168  Accuracy:  0.78846157\n",
      "10541 : Training: loss:  0.018455911\n",
      "10542 : Training: loss:  0.03365831\n",
      "10543 : Training: loss:  0.015264048\n",
      "10544 : Training: loss:  0.050580405\n",
      "10545 : Training: loss:  0.01778737\n",
      "10546 : Training: loss:  0.023580197\n",
      "10547 : Training: loss:  0.01347636\n",
      "10548 : Training: loss:  0.01656126\n",
      "10549 : Training: loss:  0.021624316\n",
      "10550 : Training: loss:  0.028928705\n",
      "10551 : Training: loss:  0.021201424\n",
      "10552 : Training: loss:  0.032562528\n",
      "10553 : Training: loss:  0.023473935\n",
      "10554 : Training: loss:  0.017740285\n",
      "10555 : Training: loss:  0.019310083\n",
      "10556 : Training: loss:  0.035500858\n",
      "10557 : Training: loss:  0.01694298\n",
      "10558 : Training: loss:  0.008083801\n",
      "10559 : Training: loss:  0.0035669093\n",
      "10560 : Training: loss:  0.0062482064\n",
      "Validation: Loss:  0.054187655  Accuracy:  0.78846157\n",
      "10561 : Training: loss:  0.026515849\n",
      "10562 : Training: loss:  0.028443133\n",
      "10563 : Training: loss:  0.04069031\n",
      "10564 : Training: loss:  0.049436323\n",
      "10565 : Training: loss:  0.038819283\n",
      "10566 : Training: loss:  0.023714932\n",
      "10567 : Training: loss:  0.050106034\n",
      "10568 : Training: loss:  0.009028865\n",
      "10569 : Training: loss:  0.048865728\n",
      "10570 : Training: loss:  0.018169586\n",
      "10571 : Training: loss:  0.019033186\n",
      "10572 : Training: loss:  0.016010467\n",
      "10573 : Training: loss:  0.024507197\n",
      "10574 : Training: loss:  0.03334193\n",
      "10575 : Training: loss:  0.019343384\n",
      "10576 : Training: loss:  0.0419107\n",
      "10577 : Training: loss:  0.031179903\n",
      "10578 : Training: loss:  0.025677655\n",
      "10579 : Training: loss:  0.029921187\n",
      "10580 : Training: loss:  0.012882967\n",
      "Validation: Loss:  0.053946514  Accuracy:  0.7692308\n",
      "10581 : Training: loss:  0.013908618\n",
      "10582 : Training: loss:  0.01728688\n",
      "10583 : Training: loss:  0.008050688\n",
      "10584 : Training: loss:  0.009704928\n",
      "10585 : Training: loss:  0.051769916\n",
      "10586 : Training: loss:  0.018611386\n",
      "10587 : Training: loss:  0.028914498\n",
      "10588 : Training: loss:  0.023011215\n",
      "10589 : Training: loss:  0.034991607\n",
      "10590 : Training: loss:  0.0221934\n",
      "10591 : Training: loss:  0.005694002\n",
      "10592 : Training: loss:  0.030265082\n",
      "10593 : Training: loss:  0.022529205\n",
      "10594 : Training: loss:  0.034695707\n",
      "10595 : Training: loss:  0.05644327\n",
      "10596 : Training: loss:  0.021891225\n",
      "10597 : Training: loss:  0.019208776\n",
      "10598 : Training: loss:  0.0061802412\n",
      "10599 : Training: loss:  0.038515184\n",
      "10600 : Training: loss:  0.017258678\n",
      "Validation: Loss:  0.054316312  Accuracy:  0.7692308\n",
      "10601 : Training: loss:  0.013695278\n",
      "10602 : Training: loss:  0.017940499\n",
      "10603 : Training: loss:  0.0063284645\n",
      "10604 : Training: loss:  0.023829238\n",
      "10605 : Training: loss:  0.029010354\n",
      "10606 : Training: loss:  0.0121789565\n",
      "10607 : Training: loss:  0.032680765\n",
      "10608 : Training: loss:  0.04886639\n",
      "10609 : Training: loss:  0.028882185\n",
      "10610 : Training: loss:  0.03795135\n",
      "10611 : Training: loss:  0.04443071\n",
      "10612 : Training: loss:  0.02164782\n",
      "10613 : Training: loss:  0.01827502\n",
      "10614 : Training: loss:  0.010843967\n",
      "10615 : Training: loss:  0.024448048\n",
      "10616 : Training: loss:  0.021103464\n",
      "10617 : Training: loss:  0.010365673\n",
      "10618 : Training: loss:  0.012488282\n",
      "10619 : Training: loss:  0.02630533\n",
      "10620 : Training: loss:  0.025343768\n",
      "Validation: Loss:  0.054423828  Accuracy:  0.7692308\n",
      "10621 : Training: loss:  0.031778995\n",
      "10622 : Training: loss:  0.020343985\n",
      "10623 : Training: loss:  0.006064931\n",
      "10624 : Training: loss:  0.009358878\n",
      "10625 : Training: loss:  0.02870969\n",
      "10626 : Training: loss:  0.036917206\n",
      "10627 : Training: loss:  0.009980812\n",
      "10628 : Training: loss:  0.008083778\n",
      "10629 : Training: loss:  0.01528538\n",
      "10630 : Training: loss:  0.0268739\n",
      "10631 : Training: loss:  0.023434334\n",
      "10632 : Training: loss:  0.015435649\n",
      "10633 : Training: loss:  0.0429558\n",
      "10634 : Training: loss:  0.030846944\n",
      "10635 : Training: loss:  0.035918657\n",
      "10636 : Training: loss:  0.030422987\n",
      "10637 : Training: loss:  0.018457823\n",
      "10638 : Training: loss:  0.026782297\n",
      "10639 : Training: loss:  0.013093601\n",
      "10640 : Training: loss:  0.022112824\n",
      "Validation: Loss:  0.054186888  Accuracy:  0.7692308\n",
      "10641 : Training: loss:  0.024370627\n",
      "10642 : Training: loss:  0.019586766\n",
      "10643 : Training: loss:  0.024486966\n",
      "10644 : Training: loss:  0.01967914\n",
      "10645 : Training: loss:  0.023568701\n",
      "10646 : Training: loss:  0.012992828\n",
      "10647 : Training: loss:  0.020257797\n",
      "10648 : Training: loss:  0.008641341\n",
      "10649 : Training: loss:  0.034244332\n",
      "10650 : Training: loss:  0.028460952\n",
      "10651 : Training: loss:  0.023523511\n",
      "10652 : Training: loss:  0.023634406\n",
      "10653 : Training: loss:  0.015162635\n",
      "10654 : Training: loss:  0.024321258\n",
      "10655 : Training: loss:  0.022636669\n",
      "10656 : Training: loss:  0.04841338\n",
      "10657 : Training: loss:  0.032404903\n",
      "10658 : Training: loss:  0.032740884\n",
      "10659 : Training: loss:  0.026631618\n",
      "10660 : Training: loss:  0.028794631\n",
      "Validation: Loss:  0.0537742  Accuracy:  0.7692308\n",
      "10661 : Training: loss:  0.0069051767\n",
      "10662 : Training: loss:  0.01557671\n",
      "10663 : Training: loss:  0.073228255\n",
      "10664 : Training: loss:  0.018627182\n",
      "10665 : Training: loss:  0.04571667\n",
      "10666 : Training: loss:  0.023386689\n",
      "10667 : Training: loss:  0.017591437\n",
      "10668 : Training: loss:  0.044650644\n",
      "10669 : Training: loss:  0.039746143\n",
      "10670 : Training: loss:  0.018649984\n",
      "10671 : Training: loss:  0.021577166\n",
      "10672 : Training: loss:  0.027275074\n",
      "10673 : Training: loss:  0.033164162\n",
      "10674 : Training: loss:  0.04917863\n",
      "10675 : Training: loss:  0.018724442\n",
      "10676 : Training: loss:  0.020944439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677 : Training: loss:  0.04263928\n",
      "10678 : Training: loss:  0.013550843\n",
      "10679 : Training: loss:  0.008083582\n",
      "10680 : Training: loss:  0.036749568\n",
      "Validation: Loss:  0.05357382  Accuracy:  0.78846157\n",
      "10681 : Training: loss:  0.0019640757\n",
      "10682 : Training: loss:  0.0049748635\n",
      "10683 : Training: loss:  0.033295546\n",
      "10684 : Training: loss:  0.02455276\n",
      "10685 : Training: loss:  0.03083512\n",
      "10686 : Training: loss:  0.036865655\n",
      "10687 : Training: loss:  0.0286214\n",
      "10688 : Training: loss:  0.016069517\n",
      "10689 : Training: loss:  0.018855348\n",
      "10690 : Training: loss:  0.024340509\n",
      "10691 : Training: loss:  0.046785146\n",
      "10692 : Training: loss:  0.0035839844\n",
      "10693 : Training: loss:  0.022412328\n",
      "10694 : Training: loss:  0.015003473\n",
      "10695 : Training: loss:  0.009690281\n",
      "10696 : Training: loss:  0.010466634\n",
      "10697 : Training: loss:  0.00970897\n",
      "10698 : Training: loss:  0.030321417\n",
      "10699 : Training: loss:  0.021427957\n",
      "10700 : Training: loss:  0.020929094\n",
      "Validation: Loss:  0.053512115  Accuracy:  0.78846157\n",
      "10701 : Training: loss:  0.04146662\n",
      "10702 : Training: loss:  0.012967654\n",
      "10703 : Training: loss:  0.022383356\n",
      "10704 : Training: loss:  0.032280702\n",
      "10705 : Training: loss:  0.038010273\n",
      "10706 : Training: loss:  0.03412576\n",
      "10707 : Training: loss:  0.01817209\n",
      "10708 : Training: loss:  0.03356178\n",
      "10709 : Training: loss:  0.018086413\n",
      "10710 : Training: loss:  0.019156467\n",
      "10711 : Training: loss:  0.033922512\n",
      "10712 : Training: loss:  0.013722565\n",
      "10713 : Training: loss:  0.03555104\n",
      "10714 : Training: loss:  0.036860302\n",
      "10715 : Training: loss:  0.025978278\n",
      "10716 : Training: loss:  0.021757904\n",
      "10717 : Training: loss:  0.023855831\n",
      "10718 : Training: loss:  0.0107823\n",
      "10719 : Training: loss:  0.0035442656\n",
      "10720 : Training: loss:  0.00998904\n",
      "Validation: Loss:  0.053633492  Accuracy:  0.7692308\n",
      "10721 : Training: loss:  0.034040324\n",
      "10722 : Training: loss:  0.01927229\n",
      "10723 : Training: loss:  0.029107574\n",
      "10724 : Training: loss:  0.00910704\n",
      "10725 : Training: loss:  0.018005433\n",
      "10726 : Training: loss:  0.033269074\n",
      "10727 : Training: loss:  0.025644245\n",
      "10728 : Training: loss:  0.011942004\n",
      "10729 : Training: loss:  0.011076047\n",
      "10730 : Training: loss:  0.04431174\n",
      "10731 : Training: loss:  0.025992047\n",
      "10732 : Training: loss:  0.025325859\n",
      "10733 : Training: loss:  0.026225396\n",
      "10734 : Training: loss:  0.033964783\n",
      "10735 : Training: loss:  0.032881994\n",
      "10736 : Training: loss:  0.012516278\n",
      "10737 : Training: loss:  0.015498608\n",
      "10738 : Training: loss:  0.021712987\n",
      "10739 : Training: loss:  0.011646456\n",
      "10740 : Training: loss:  0.012057256\n",
      "Validation: Loss:  0.05368169  Accuracy:  0.7692308\n",
      "10741 : Training: loss:  0.02036879\n",
      "10742 : Training: loss:  0.026433373\n",
      "10743 : Training: loss:  0.021985898\n",
      "10744 : Training: loss:  0.031206824\n",
      "10745 : Training: loss:  0.02083596\n",
      "10746 : Training: loss:  0.0269305\n",
      "10747 : Training: loss:  0.014938055\n",
      "10748 : Training: loss:  0.027736165\n",
      "10749 : Training: loss:  0.017989086\n",
      "10750 : Training: loss:  0.023402115\n",
      "10751 : Training: loss:  0.0113326\n",
      "10752 : Training: loss:  0.04329071\n",
      "10753 : Training: loss:  0.026920844\n",
      "10754 : Training: loss:  0.023514798\n",
      "10755 : Training: loss:  0.011185193\n",
      "10756 : Training: loss:  0.027975619\n",
      "10757 : Training: loss:  0.022873951\n",
      "10758 : Training: loss:  0.021466622\n",
      "10759 : Training: loss:  0.014327092\n",
      "10760 : Training: loss:  0.019475468\n",
      "Validation: Loss:  0.053884163  Accuracy:  0.7692308\n",
      "10761 : Training: loss:  0.010731239\n",
      "10762 : Training: loss:  0.028909102\n",
      "10763 : Training: loss:  0.0347282\n",
      "10764 : Training: loss:  0.028901206\n",
      "10765 : Training: loss:  0.01905451\n",
      "10766 : Training: loss:  0.0043483325\n",
      "10767 : Training: loss:  0.018782115\n",
      "10768 : Training: loss:  0.036436126\n",
      "10769 : Training: loss:  0.009626533\n",
      "10770 : Training: loss:  0.008269162\n",
      "10771 : Training: loss:  0.016376065\n",
      "10772 : Training: loss:  0.002640175\n",
      "10773 : Training: loss:  0.01820204\n",
      "10774 : Training: loss:  0.011124432\n",
      "10775 : Training: loss:  0.023785718\n",
      "10776 : Training: loss:  0.03263905\n",
      "10777 : Training: loss:  0.0129794115\n",
      "10778 : Training: loss:  0.046719458\n",
      "10779 : Training: loss:  0.012027316\n",
      "10780 : Training: loss:  0.017887449\n",
      "Validation: Loss:  0.053772993  Accuracy:  0.7692308\n",
      "10781 : Training: loss:  0.020540284\n",
      "10782 : Training: loss:  0.004759274\n",
      "10783 : Training: loss:  0.03223665\n",
      "10784 : Training: loss:  0.029914843\n",
      "10785 : Training: loss:  0.042863984\n",
      "10786 : Training: loss:  0.025871893\n",
      "10787 : Training: loss:  0.008381646\n",
      "10788 : Training: loss:  0.042075895\n",
      "10789 : Training: loss:  0.036924873\n",
      "10790 : Training: loss:  0.014925352\n",
      "10791 : Training: loss:  0.041469835\n",
      "10792 : Training: loss:  0.030649083\n",
      "10793 : Training: loss:  0.043454092\n",
      "10794 : Training: loss:  0.019626943\n",
      "10795 : Training: loss:  0.030787664\n",
      "10796 : Training: loss:  0.01862228\n",
      "10797 : Training: loss:  0.0328567\n",
      "10798 : Training: loss:  0.0132356165\n",
      "10799 : Training: loss:  0.0028545475\n",
      "10800 : Training: loss:  0.030193293\n",
      "Validation: Loss:  0.05353293  Accuracy:  0.7692308\n",
      "10801 : Training: loss:  0.017681804\n",
      "10802 : Training: loss:  0.008646963\n",
      "10803 : Training: loss:  0.032388717\n",
      "10804 : Training: loss:  0.027919464\n",
      "10805 : Training: loss:  0.039966032\n",
      "10806 : Training: loss:  0.009498876\n",
      "10807 : Training: loss:  0.015050175\n",
      "10808 : Training: loss:  0.0062775766\n",
      "10809 : Training: loss:  0.04640214\n",
      "10810 : Training: loss:  0.027141849\n",
      "10811 : Training: loss:  0.03050739\n",
      "10812 : Training: loss:  0.032426678\n",
      "10813 : Training: loss:  0.031452402\n",
      "10814 : Training: loss:  0.017633846\n",
      "10815 : Training: loss:  0.024365272\n",
      "10816 : Training: loss:  0.023683446\n",
      "10817 : Training: loss:  0.025841014\n",
      "10818 : Training: loss:  0.01596527\n",
      "10819 : Training: loss:  0.025545195\n",
      "10820 : Training: loss:  0.01629748\n",
      "Validation: Loss:  0.05321493  Accuracy:  0.78846157\n",
      "10821 : Training: loss:  0.009329189\n",
      "10822 : Training: loss:  0.024087317\n",
      "10823 : Training: loss:  0.006002003\n",
      "10824 : Training: loss:  0.007664761\n",
      "10825 : Training: loss:  0.01145934\n",
      "10826 : Training: loss:  0.013095759\n",
      "10827 : Training: loss:  0.010701588\n",
      "10828 : Training: loss:  0.03443161\n",
      "10829 : Training: loss:  0.020305369\n",
      "10830 : Training: loss:  0.018198583\n",
      "10831 : Training: loss:  0.025897691\n",
      "10832 : Training: loss:  0.031991936\n",
      "10833 : Training: loss:  0.012623278\n",
      "10834 : Training: loss:  0.032668337\n",
      "10835 : Training: loss:  0.009823404\n",
      "10836 : Training: loss:  0.015252595\n",
      "10837 : Training: loss:  0.019841954\n",
      "10838 : Training: loss:  0.024610981\n",
      "10839 : Training: loss:  0.03347094\n",
      "10840 : Training: loss:  0.0114257755\n",
      "Validation: Loss:  0.052999415  Accuracy:  0.78846157\n",
      "10841 : Training: loss:  0.0048015914\n",
      "10842 : Training: loss:  0.046443477\n",
      "10843 : Training: loss:  0.023091631\n",
      "10844 : Training: loss:  0.022061955\n",
      "10845 : Training: loss:  0.01885753\n",
      "10846 : Training: loss:  0.031135283\n",
      "10847 : Training: loss:  0.041335963\n",
      "10848 : Training: loss:  0.043538406\n",
      "10849 : Training: loss:  0.010550391\n",
      "10850 : Training: loss:  0.012646752\n",
      "10851 : Training: loss:  0.011549507\n",
      "10852 : Training: loss:  0.01182156\n",
      "10853 : Training: loss:  0.005059991\n",
      "10854 : Training: loss:  0.030348131\n",
      "10855 : Training: loss:  0.022081148\n",
      "10856 : Training: loss:  0.027551694\n",
      "10857 : Training: loss:  0.033625774\n",
      "10858 : Training: loss:  0.0571295\n",
      "10859 : Training: loss:  0.039776053\n",
      "10860 : Training: loss:  0.011553558\n",
      "Validation: Loss:  0.05282602  Accuracy:  0.78846157\n",
      "10861 : Training: loss:  0.03439857\n",
      "10862 : Training: loss:  0.017271755\n",
      "10863 : Training: loss:  0.03569868\n",
      "10864 : Training: loss:  0.026312485\n",
      "10865 : Training: loss:  0.027867213\n",
      "10866 : Training: loss:  0.026094913\n",
      "10867 : Training: loss:  0.04653044\n",
      "10868 : Training: loss:  0.014820881\n",
      "10869 : Training: loss:  0.04387885\n",
      "10870 : Training: loss:  0.026309792\n",
      "10871 : Training: loss:  0.026090944\n",
      "10872 : Training: loss:  0.039661143\n",
      "10873 : Training: loss:  0.038932417\n",
      "10874 : Training: loss:  0.017730964\n",
      "10875 : Training: loss:  0.012580096\n",
      "10876 : Training: loss:  0.019719303\n",
      "10877 : Training: loss:  0.028001133\n",
      "10878 : Training: loss:  0.016535817\n",
      "10879 : Training: loss:  0.060400933\n",
      "10880 : Training: loss:  0.01849263\n",
      "Validation: Loss:  0.052840892  Accuracy:  0.8076923\n",
      "10881 : Training: loss:  0.025639722\n",
      "10882 : Training: loss:  0.016606636\n",
      "10883 : Training: loss:  0.029391214\n",
      "10884 : Training: loss:  0.040441398\n",
      "10885 : Training: loss:  0.019878794\n",
      "10886 : Training: loss:  0.0139616905\n",
      "10887 : Training: loss:  0.03886969\n",
      "10888 : Training: loss:  0.010740094\n",
      "10889 : Training: loss:  0.018194728\n",
      "10890 : Training: loss:  0.010634863\n",
      "10891 : Training: loss:  0.0064295805\n",
      "10892 : Training: loss:  0.038871385\n",
      "10893 : Training: loss:  0.039574996\n",
      "10894 : Training: loss:  0.016911255\n",
      "10895 : Training: loss:  0.022327945\n",
      "10896 : Training: loss:  0.017992409\n",
      "10897 : Training: loss:  0.017702233\n",
      "10898 : Training: loss:  0.011639208\n",
      "10899 : Training: loss:  0.020299759\n",
      "10900 : Training: loss:  0.025693152\n",
      "Validation: Loss:  0.053208545  Accuracy:  0.8076923\n",
      "10901 : Training: loss:  0.01767036\n",
      "10902 : Training: loss:  0.031589095\n",
      "10903 : Training: loss:  0.017075893\n",
      "10904 : Training: loss:  0.03722551\n",
      "10905 : Training: loss:  0.028430188\n",
      "10906 : Training: loss:  0.010612189\n",
      "10907 : Training: loss:  0.008904293\n",
      "10908 : Training: loss:  0.012002263\n",
      "10909 : Training: loss:  0.04528782\n",
      "10910 : Training: loss:  0.03517691\n",
      "10911 : Training: loss:  0.027918534\n",
      "10912 : Training: loss:  0.015385761\n",
      "10913 : Training: loss:  0.012728806\n",
      "10914 : Training: loss:  0.05991468\n",
      "10915 : Training: loss:  0.004247209\n",
      "10916 : Training: loss:  0.041065115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10917 : Training: loss:  0.021210983\n",
      "10918 : Training: loss:  0.0064920927\n",
      "10919 : Training: loss:  0.033183675\n",
      "10920 : Training: loss:  0.024568748\n",
      "Validation: Loss:  0.053310372  Accuracy:  0.8076923\n",
      "10921 : Training: loss:  0.035096914\n",
      "10922 : Training: loss:  0.009437134\n",
      "10923 : Training: loss:  0.014639435\n",
      "10924 : Training: loss:  0.03815188\n",
      "10925 : Training: loss:  0.026090264\n",
      "10926 : Training: loss:  0.0064178887\n",
      "10927 : Training: loss:  0.014325089\n",
      "10928 : Training: loss:  0.03375739\n",
      "10929 : Training: loss:  0.009142403\n",
      "10930 : Training: loss:  0.021133665\n",
      "10931 : Training: loss:  0.028453322\n",
      "10932 : Training: loss:  0.019997599\n",
      "10933 : Training: loss:  0.032654688\n",
      "10934 : Training: loss:  0.017631328\n",
      "10935 : Training: loss:  0.016801702\n",
      "10936 : Training: loss:  0.010964195\n",
      "10937 : Training: loss:  0.020027328\n",
      "10938 : Training: loss:  0.018955251\n",
      "10939 : Training: loss:  0.0059610927\n",
      "10940 : Training: loss:  0.032674957\n",
      "Validation: Loss:  0.053516388  Accuracy:  0.8076923\n",
      "10941 : Training: loss:  0.053651482\n",
      "10942 : Training: loss:  0.0058680694\n",
      "10943 : Training: loss:  0.012468993\n",
      "10944 : Training: loss:  0.025763271\n",
      "10945 : Training: loss:  0.050422814\n",
      "10946 : Training: loss:  0.036868118\n",
      "10947 : Training: loss:  0.053880148\n",
      "10948 : Training: loss:  0.0064026546\n",
      "10949 : Training: loss:  0.037287\n",
      "10950 : Training: loss:  0.023788085\n",
      "10951 : Training: loss:  0.002386901\n",
      "10952 : Training: loss:  0.014462227\n",
      "10953 : Training: loss:  0.021912185\n",
      "10954 : Training: loss:  0.004984773\n",
      "10955 : Training: loss:  0.014958854\n",
      "10956 : Training: loss:  0.006198628\n",
      "10957 : Training: loss:  0.013027868\n",
      "10958 : Training: loss:  0.010419272\n",
      "10959 : Training: loss:  0.016253142\n",
      "10960 : Training: loss:  0.009424083\n",
      "Validation: Loss:  0.05277062  Accuracy:  0.8076923\n",
      "10961 : Training: loss:  0.03460606\n",
      "10962 : Training: loss:  0.014222152\n",
      "10963 : Training: loss:  0.021835182\n",
      "10964 : Training: loss:  0.041872304\n",
      "10965 : Training: loss:  0.01253749\n",
      "10966 : Training: loss:  0.027486337\n",
      "10967 : Training: loss:  0.016893454\n",
      "10968 : Training: loss:  0.017873721\n",
      "10969 : Training: loss:  0.018959045\n",
      "10970 : Training: loss:  0.037293274\n",
      "10971 : Training: loss:  0.010335593\n",
      "10972 : Training: loss:  0.0055199033\n",
      "10973 : Training: loss:  0.04905207\n",
      "10974 : Training: loss:  0.027299227\n",
      "10975 : Training: loss:  0.03509014\n",
      "10976 : Training: loss:  0.007955456\n",
      "10977 : Training: loss:  0.021376347\n",
      "10978 : Training: loss:  0.033678085\n",
      "10979 : Training: loss:  0.039500456\n",
      "10980 : Training: loss:  0.025516152\n",
      "Validation: Loss:  0.05272427  Accuracy:  0.8076923\n",
      "10981 : Training: loss:  0.04336613\n",
      "10982 : Training: loss:  0.02867166\n",
      "10983 : Training: loss:  0.030444779\n",
      "10984 : Training: loss:  0.0133935725\n",
      "10985 : Training: loss:  0.055435758\n",
      "10986 : Training: loss:  0.01378135\n",
      "10987 : Training: loss:  0.022797583\n",
      "10988 : Training: loss:  0.025010888\n",
      "10989 : Training: loss:  0.008759408\n",
      "10990 : Training: loss:  0.03817937\n",
      "10991 : Training: loss:  0.014410609\n",
      "10992 : Training: loss:  0.025157783\n",
      "10993 : Training: loss:  0.029953305\n",
      "10994 : Training: loss:  0.013063242\n",
      "10995 : Training: loss:  0.028487738\n",
      "10996 : Training: loss:  0.011616528\n",
      "10997 : Training: loss:  0.010592533\n",
      "10998 : Training: loss:  0.035528082\n",
      "10999 : Training: loss:  0.012102835\n",
      "11000 : Training: loss:  0.018008338\n",
      "Validation: Loss:  0.05257405  Accuracy:  0.8076923\n",
      "11001 : Training: loss:  0.017534738\n",
      "11002 : Training: loss:  0.027112724\n",
      "11003 : Training: loss:  0.0049351007\n",
      "11004 : Training: loss:  0.0056415563\n",
      "11005 : Training: loss:  0.030046701\n",
      "11006 : Training: loss:  0.0044992114\n",
      "11007 : Training: loss:  0.048376873\n",
      "11008 : Training: loss:  0.0062486897\n",
      "11009 : Training: loss:  0.014362174\n",
      "11010 : Training: loss:  0.03168842\n",
      "11011 : Training: loss:  0.0071393824\n",
      "11012 : Training: loss:  0.015718043\n",
      "11013 : Training: loss:  0.05386372\n",
      "11014 : Training: loss:  0.020742597\n",
      "11015 : Training: loss:  0.01228446\n",
      "11016 : Training: loss:  0.0021850283\n",
      "11017 : Training: loss:  0.04367466\n",
      "11018 : Training: loss:  0.027879464\n",
      "11019 : Training: loss:  0.008707765\n",
      "11020 : Training: loss:  0.005517992\n",
      "Validation: Loss:  0.05243691  Accuracy:  0.8076923\n",
      "11021 : Training: loss:  0.016998798\n",
      "11022 : Training: loss:  0.03029786\n",
      "11023 : Training: loss:  0.018491788\n",
      "11024 : Training: loss:  0.015264855\n",
      "11025 : Training: loss:  0.021444155\n",
      "11026 : Training: loss:  0.031045923\n",
      "11027 : Training: loss:  0.0332353\n",
      "11028 : Training: loss:  0.02069286\n",
      "11029 : Training: loss:  0.0057892227\n",
      "11030 : Training: loss:  0.029829776\n",
      "11031 : Training: loss:  0.0053482973\n",
      "11032 : Training: loss:  0.025763284\n",
      "11033 : Training: loss:  0.0052596275\n",
      "11034 : Training: loss:  0.06633284\n",
      "11035 : Training: loss:  0.03791624\n",
      "11036 : Training: loss:  0.03627986\n",
      "11037 : Training: loss:  0.018079998\n",
      "11038 : Training: loss:  0.02047921\n",
      "11039 : Training: loss:  0.022415953\n",
      "11040 : Training: loss:  0.03681883\n",
      "Validation: Loss:  0.052816648  Accuracy:  0.8076923\n",
      "11041 : Training: loss:  0.018903205\n",
      "11042 : Training: loss:  0.028695153\n",
      "11043 : Training: loss:  0.016815022\n",
      "11044 : Training: loss:  0.03201523\n",
      "11045 : Training: loss:  0.015799645\n",
      "11046 : Training: loss:  0.023548076\n",
      "11047 : Training: loss:  0.034515105\n",
      "11048 : Training: loss:  0.008253424\n",
      "11049 : Training: loss:  0.020751236\n",
      "11050 : Training: loss:  0.024728905\n",
      "11051 : Training: loss:  0.017888384\n",
      "11052 : Training: loss:  0.022854885\n",
      "11053 : Training: loss:  0.020864118\n",
      "11054 : Training: loss:  0.023403633\n",
      "11055 : Training: loss:  0.05575569\n",
      "11056 : Training: loss:  0.00970109\n",
      "11057 : Training: loss:  0.023867344\n",
      "11058 : Training: loss:  0.025815912\n",
      "11059 : Training: loss:  0.02069981\n",
      "11060 : Training: loss:  0.019835485\n",
      "Validation: Loss:  0.052855972  Accuracy:  0.78846157\n",
      "11061 : Training: loss:  0.014540142\n",
      "11062 : Training: loss:  0.021958925\n",
      "11063 : Training: loss:  0.018272154\n",
      "11064 : Training: loss:  0.031739194\n",
      "11065 : Training: loss:  0.021410445\n",
      "11066 : Training: loss:  0.033651017\n",
      "11067 : Training: loss:  0.015965002\n",
      "11068 : Training: loss:  0.014931743\n",
      "11069 : Training: loss:  0.025106192\n",
      "11070 : Training: loss:  0.061333925\n",
      "11071 : Training: loss:  0.028686155\n",
      "11072 : Training: loss:  0.048007824\n",
      "11073 : Training: loss:  0.033348233\n",
      "11074 : Training: loss:  0.024040675\n",
      "11075 : Training: loss:  0.027015898\n",
      "11076 : Training: loss:  0.0145150935\n",
      "11077 : Training: loss:  0.005020776\n",
      "11078 : Training: loss:  0.023966504\n",
      "11079 : Training: loss:  0.015131704\n",
      "11080 : Training: loss:  0.011245386\n",
      "Validation: Loss:  0.052320916  Accuracy:  0.7692308\n",
      "11081 : Training: loss:  0.010740364\n",
      "11082 : Training: loss:  0.012369243\n",
      "11083 : Training: loss:  0.037278026\n",
      "11084 : Training: loss:  0.014150567\n",
      "11085 : Training: loss:  0.012517685\n",
      "11086 : Training: loss:  0.021612803\n",
      "11087 : Training: loss:  0.036653142\n",
      "11088 : Training: loss:  0.022829078\n",
      "11089 : Training: loss:  0.050749663\n",
      "11090 : Training: loss:  0.017479047\n",
      "11091 : Training: loss:  0.009623092\n",
      "11092 : Training: loss:  0.023736497\n",
      "11093 : Training: loss:  0.014767868\n",
      "11094 : Training: loss:  0.03143321\n",
      "11095 : Training: loss:  0.0062495507\n",
      "11096 : Training: loss:  0.022250736\n",
      "11097 : Training: loss:  0.013992716\n",
      "11098 : Training: loss:  0.009141043\n",
      "11099 : Training: loss:  0.033604626\n",
      "11100 : Training: loss:  0.022233691\n",
      "Validation: Loss:  0.05244117  Accuracy:  0.7692308\n",
      "11101 : Training: loss:  0.033708528\n",
      "11102 : Training: loss:  0.028922092\n",
      "11103 : Training: loss:  0.027864406\n",
      "11104 : Training: loss:  0.019318478\n",
      "11105 : Training: loss:  0.020853898\n",
      "11106 : Training: loss:  0.035177007\n",
      "11107 : Training: loss:  0.010772864\n",
      "11108 : Training: loss:  0.011630559\n",
      "11109 : Training: loss:  0.0075595556\n",
      "11110 : Training: loss:  0.023287697\n",
      "11111 : Training: loss:  0.040528957\n",
      "11112 : Training: loss:  0.024858935\n",
      "11113 : Training: loss:  0.0045607425\n",
      "11114 : Training: loss:  0.017601816\n",
      "11115 : Training: loss:  0.017609004\n",
      "11116 : Training: loss:  0.01927313\n",
      "11117 : Training: loss:  0.020838272\n",
      "11118 : Training: loss:  0.034249563\n",
      "11119 : Training: loss:  0.034870658\n",
      "11120 : Training: loss:  0.026367338\n",
      "Validation: Loss:  0.05273275  Accuracy:  0.7692308\n",
      "11121 : Training: loss:  0.00431181\n",
      "11122 : Training: loss:  0.021939585\n",
      "11123 : Training: loss:  0.030339375\n",
      "11124 : Training: loss:  0.013444501\n",
      "11125 : Training: loss:  0.0054354854\n",
      "11126 : Training: loss:  0.013143395\n",
      "11127 : Training: loss:  0.031616047\n",
      "11128 : Training: loss:  0.02706574\n",
      "11129 : Training: loss:  0.027432835\n",
      "11130 : Training: loss:  0.02922159\n",
      "11131 : Training: loss:  0.033965696\n",
      "11132 : Training: loss:  0.005436408\n",
      "11133 : Training: loss:  0.028164793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11134 : Training: loss:  0.028498044\n",
      "11135 : Training: loss:  0.01878512\n",
      "11136 : Training: loss:  0.03995829\n",
      "11137 : Training: loss:  0.020621577\n",
      "11138 : Training: loss:  0.0220388\n",
      "11139 : Training: loss:  0.029159121\n",
      "11140 : Training: loss:  0.021806553\n",
      "Validation: Loss:  0.052510317  Accuracy:  0.7692308\n",
      "11141 : Training: loss:  0.037311204\n",
      "11142 : Training: loss:  0.021718418\n",
      "11143 : Training: loss:  0.04050179\n",
      "11144 : Training: loss:  0.005080045\n",
      "11145 : Training: loss:  0.049857777\n",
      "11146 : Training: loss:  0.026892463\n",
      "11147 : Training: loss:  0.008504586\n",
      "11148 : Training: loss:  0.03738442\n",
      "11149 : Training: loss:  0.020124404\n",
      "11150 : Training: loss:  0.020695023\n",
      "11151 : Training: loss:  0.04439973\n",
      "11152 : Training: loss:  0.01627616\n",
      "11153 : Training: loss:  0.032787148\n",
      "11154 : Training: loss:  0.01773286\n",
      "11155 : Training: loss:  0.051280465\n",
      "11156 : Training: loss:  0.015248563\n",
      "11157 : Training: loss:  0.018629597\n",
      "11158 : Training: loss:  0.01919156\n",
      "11159 : Training: loss:  0.013056649\n",
      "11160 : Training: loss:  0.035541672\n",
      "Validation: Loss:  0.05258791  Accuracy:  0.7692308\n",
      "11161 : Training: loss:  0.040938888\n",
      "11162 : Training: loss:  0.02402428\n",
      "11163 : Training: loss:  0.020696446\n",
      "11164 : Training: loss:  0.024132669\n",
      "11165 : Training: loss:  0.033860065\n",
      "11166 : Training: loss:  0.014920785\n",
      "11167 : Training: loss:  0.004170739\n",
      "11168 : Training: loss:  0.013255952\n",
      "11169 : Training: loss:  0.03710914\n",
      "11170 : Training: loss:  0.028522274\n",
      "11171 : Training: loss:  0.0068780873\n",
      "11172 : Training: loss:  0.045865554\n",
      "11173 : Training: loss:  0.008759483\n",
      "11174 : Training: loss:  0.023425974\n",
      "11175 : Training: loss:  0.010605861\n",
      "11176 : Training: loss:  0.029281482\n",
      "11177 : Training: loss:  0.023619864\n",
      "11178 : Training: loss:  0.0185009\n",
      "11179 : Training: loss:  0.022311183\n",
      "11180 : Training: loss:  0.030579066\n",
      "Validation: Loss:  0.052416336  Accuracy:  0.7692308\n",
      "11181 : Training: loss:  0.027145732\n",
      "11182 : Training: loss:  0.011110961\n",
      "11183 : Training: loss:  0.014824803\n",
      "11184 : Training: loss:  0.022873964\n",
      "11185 : Training: loss:  0.03815014\n",
      "11186 : Training: loss:  0.0105934\n",
      "11187 : Training: loss:  0.041458268\n",
      "11188 : Training: loss:  0.016939498\n",
      "11189 : Training: loss:  0.023019888\n",
      "11190 : Training: loss:  0.026921704\n",
      "11191 : Training: loss:  0.03953496\n",
      "11192 : Training: loss:  0.0017050095\n",
      "11193 : Training: loss:  0.028149601\n",
      "11194 : Training: loss:  0.033140656\n",
      "11195 : Training: loss:  0.01347212\n",
      "11196 : Training: loss:  0.0008026608\n",
      "11197 : Training: loss:  0.015603544\n",
      "11198 : Training: loss:  0.012429966\n",
      "11199 : Training: loss:  0.031130536\n",
      "11200 : Training: loss:  0.027296422\n",
      "Validation: Loss:  0.05271593  Accuracy:  0.7692308\n",
      "11201 : Training: loss:  0.02399781\n",
      "11202 : Training: loss:  0.023382649\n",
      "11203 : Training: loss:  0.0220523\n",
      "11204 : Training: loss:  0.027425412\n",
      "11205 : Training: loss:  0.020932762\n",
      "11206 : Training: loss:  0.017167209\n",
      "11207 : Training: loss:  0.023442345\n",
      "11208 : Training: loss:  0.03803615\n",
      "11209 : Training: loss:  0.03154653\n",
      "11210 : Training: loss:  0.018468434\n",
      "11211 : Training: loss:  0.014401338\n",
      "11212 : Training: loss:  0.027622294\n",
      "11213 : Training: loss:  0.029074924\n",
      "11214 : Training: loss:  0.01532755\n",
      "11215 : Training: loss:  0.028973691\n",
      "11216 : Training: loss:  0.0067884536\n",
      "11217 : Training: loss:  0.030213822\n",
      "11218 : Training: loss:  0.02113839\n",
      "11219 : Training: loss:  0.005177485\n",
      "11220 : Training: loss:  0.042606734\n",
      "Validation: Loss:  0.052490603  Accuracy:  0.7692308\n",
      "11221 : Training: loss:  0.00357641\n",
      "11222 : Training: loss:  0.010601847\n",
      "11223 : Training: loss:  0.026115749\n",
      "11224 : Training: loss:  0.017350668\n",
      "11225 : Training: loss:  0.019092675\n",
      "11226 : Training: loss:  0.0064345715\n",
      "11227 : Training: loss:  0.025498983\n",
      "11228 : Training: loss:  0.01998395\n",
      "11229 : Training: loss:  0.028461857\n",
      "11230 : Training: loss:  0.0033446997\n",
      "11231 : Training: loss:  0.03159463\n",
      "11232 : Training: loss:  0.019555189\n",
      "11233 : Training: loss:  0.009839417\n",
      "11234 : Training: loss:  0.022074739\n",
      "11235 : Training: loss:  0.026021464\n",
      "11236 : Training: loss:  0.027060036\n",
      "11237 : Training: loss:  0.037876923\n",
      "11238 : Training: loss:  0.03878212\n",
      "11239 : Training: loss:  0.02748191\n",
      "11240 : Training: loss:  0.018546702\n",
      "Validation: Loss:  0.052148618  Accuracy:  0.7692308\n",
      "11241 : Training: loss:  0.016172478\n",
      "11242 : Training: loss:  0.03953043\n",
      "11243 : Training: loss:  0.007061232\n",
      "11244 : Training: loss:  0.025267826\n",
      "11245 : Training: loss:  0.031140653\n",
      "11246 : Training: loss:  0.029832628\n",
      "11247 : Training: loss:  0.036664363\n",
      "11248 : Training: loss:  0.0026630894\n",
      "11249 : Training: loss:  0.017884945\n",
      "11250 : Training: loss:  0.02453339\n",
      "11251 : Training: loss:  0.018912802\n",
      "11252 : Training: loss:  0.0051413807\n",
      "11253 : Training: loss:  0.011486856\n",
      "11254 : Training: loss:  0.019861288\n",
      "11255 : Training: loss:  0.031173\n",
      "11256 : Training: loss:  0.0050885906\n",
      "11257 : Training: loss:  0.012670879\n",
      "11258 : Training: loss:  0.018301155\n",
      "11259 : Training: loss:  0.0437372\n",
      "11260 : Training: loss:  0.01512878\n",
      "Validation: Loss:  0.052123144  Accuracy:  0.7692308\n",
      "11261 : Training: loss:  0.01820015\n",
      "11262 : Training: loss:  0.036235996\n",
      "11263 : Training: loss:  0.019941732\n",
      "11264 : Training: loss:  0.037873264\n",
      "11265 : Training: loss:  0.036165707\n",
      "11266 : Training: loss:  0.02330613\n",
      "11267 : Training: loss:  0.012515869\n",
      "11268 : Training: loss:  0.030176343\n",
      "11269 : Training: loss:  0.035091773\n",
      "11270 : Training: loss:  0.017563231\n",
      "11271 : Training: loss:  0.009002902\n",
      "11272 : Training: loss:  0.030570315\n",
      "11273 : Training: loss:  0.015205942\n",
      "11274 : Training: loss:  0.058451224\n",
      "11275 : Training: loss:  0.03800638\n",
      "11276 : Training: loss:  0.034260232\n",
      "11277 : Training: loss:  0.0041326294\n",
      "11278 : Training: loss:  0.013691706\n",
      "11279 : Training: loss:  0.02676718\n",
      "11280 : Training: loss:  0.010266676\n",
      "Validation: Loss:  0.051943913  Accuracy:  0.78846157\n",
      "11281 : Training: loss:  0.019043097\n",
      "11282 : Training: loss:  0.00936813\n",
      "11283 : Training: loss:  0.025559906\n",
      "11284 : Training: loss:  0.026796637\n",
      "11285 : Training: loss:  0.027115492\n",
      "11286 : Training: loss:  0.012770223\n",
      "11287 : Training: loss:  0.032094292\n",
      "11288 : Training: loss:  0.024828184\n",
      "11289 : Training: loss:  0.017985256\n",
      "11290 : Training: loss:  0.0283639\n",
      "11291 : Training: loss:  0.014937182\n",
      "11292 : Training: loss:  0.022322131\n",
      "11293 : Training: loss:  0.0061039873\n",
      "11294 : Training: loss:  0.018489232\n",
      "11295 : Training: loss:  0.025689185\n",
      "11296 : Training: loss:  0.010245297\n",
      "11297 : Training: loss:  0.029749569\n",
      "11298 : Training: loss:  0.031113982\n",
      "11299 : Training: loss:  0.037604403\n",
      "11300 : Training: loss:  0.024394898\n",
      "Validation: Loss:  0.051873215  Accuracy:  0.78846157\n",
      "11301 : Training: loss:  0.02109035\n",
      "11302 : Training: loss:  0.018371304\n",
      "11303 : Training: loss:  0.0317996\n",
      "11304 : Training: loss:  0.012469256\n",
      "11305 : Training: loss:  0.017042413\n",
      "11306 : Training: loss:  0.028647466\n",
      "11307 : Training: loss:  0.0033177617\n",
      "11308 : Training: loss:  0.023283422\n",
      "11309 : Training: loss:  0.010523381\n",
      "11310 : Training: loss:  0.029493215\n",
      "11311 : Training: loss:  0.010677085\n",
      "11312 : Training: loss:  0.04211206\n",
      "11313 : Training: loss:  0.020370957\n",
      "11314 : Training: loss:  0.021386707\n",
      "11315 : Training: loss:  0.009188484\n",
      "11316 : Training: loss:  0.011380681\n",
      "11317 : Training: loss:  0.017513493\n",
      "11318 : Training: loss:  0.010148313\n",
      "11319 : Training: loss:  0.01144272\n",
      "11320 : Training: loss:  0.014953102\n",
      "Validation: Loss:  0.05226229  Accuracy:  0.7692308\n",
      "11321 : Training: loss:  0.051461075\n",
      "11322 : Training: loss:  0.011640775\n",
      "11323 : Training: loss:  0.011484977\n",
      "11324 : Training: loss:  0.025028026\n",
      "11325 : Training: loss:  0.017305499\n",
      "11326 : Training: loss:  0.012882016\n",
      "11327 : Training: loss:  0.011806355\n",
      "11328 : Training: loss:  0.03460932\n",
      "11329 : Training: loss:  0.034970183\n",
      "11330 : Training: loss:  0.026112318\n",
      "11331 : Training: loss:  0.013644732\n",
      "11332 : Training: loss:  0.011920891\n",
      "11333 : Training: loss:  0.039531987\n",
      "11334 : Training: loss:  0.025172777\n",
      "11335 : Training: loss:  0.022012366\n",
      "11336 : Training: loss:  0.030357815\n",
      "11337 : Training: loss:  0.022623802\n",
      "11338 : Training: loss:  0.020184765\n",
      "11339 : Training: loss:  0.01715871\n",
      "11340 : Training: loss:  0.021588841\n",
      "Validation: Loss:  0.05228256  Accuracy:  0.78846157\n",
      "11341 : Training: loss:  0.022910988\n",
      "11342 : Training: loss:  0.023020016\n",
      "11343 : Training: loss:  0.026002575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11344 : Training: loss:  0.037819955\n",
      "11345 : Training: loss:  0.023696413\n",
      "11346 : Training: loss:  0.025036292\n",
      "11347 : Training: loss:  0.031272475\n",
      "11348 : Training: loss:  0.041031446\n",
      "11349 : Training: loss:  0.029253386\n",
      "11350 : Training: loss:  0.019970246\n",
      "11351 : Training: loss:  0.012578867\n",
      "11352 : Training: loss:  0.022851612\n",
      "11353 : Training: loss:  0.021844901\n",
      "11354 : Training: loss:  0.019197104\n",
      "11355 : Training: loss:  0.01598268\n",
      "11356 : Training: loss:  0.021817006\n",
      "11357 : Training: loss:  0.008238339\n",
      "11358 : Training: loss:  0.02254189\n",
      "11359 : Training: loss:  0.014272268\n",
      "11360 : Training: loss:  0.041716788\n",
      "Validation: Loss:  0.052326106  Accuracy:  0.78846157\n",
      "11361 : Training: loss:  0.009472215\n",
      "11362 : Training: loss:  0.01209569\n",
      "11363 : Training: loss:  0.032307178\n",
      "11364 : Training: loss:  0.027221918\n",
      "11365 : Training: loss:  0.014320596\n",
      "11366 : Training: loss:  0.021425495\n",
      "11367 : Training: loss:  0.045546655\n",
      "11368 : Training: loss:  0.012199265\n",
      "11369 : Training: loss:  0.027722158\n",
      "11370 : Training: loss:  0.022219213\n",
      "11371 : Training: loss:  0.008488665\n",
      "11372 : Training: loss:  0.033899892\n",
      "11373 : Training: loss:  0.014119867\n",
      "11374 : Training: loss:  0.015301365\n",
      "11375 : Training: loss:  0.028331064\n",
      "11376 : Training: loss:  0.013417237\n",
      "11377 : Training: loss:  0.021883426\n",
      "11378 : Training: loss:  0.014730899\n",
      "11379 : Training: loss:  0.00667575\n",
      "11380 : Training: loss:  0.018446311\n",
      "Validation: Loss:  0.05261191  Accuracy:  0.78846157\n",
      "11381 : Training: loss:  0.021667961\n",
      "11382 : Training: loss:  0.024018256\n",
      "11383 : Training: loss:  0.008045196\n",
      "11384 : Training: loss:  0.039886933\n",
      "11385 : Training: loss:  0.014887757\n",
      "11386 : Training: loss:  0.008280215\n",
      "11387 : Training: loss:  0.054884482\n",
      "11388 : Training: loss:  0.030900942\n",
      "11389 : Training: loss:  0.05618212\n",
      "11390 : Training: loss:  0.027057633\n",
      "11391 : Training: loss:  0.035906214\n",
      "11392 : Training: loss:  0.04251884\n",
      "11393 : Training: loss:  0.028083352\n",
      "11394 : Training: loss:  0.026787521\n",
      "11395 : Training: loss:  0.033920314\n",
      "11396 : Training: loss:  0.0040590237\n",
      "11397 : Training: loss:  0.01204867\n",
      "11398 : Training: loss:  0.039124947\n",
      "11399 : Training: loss:  0.009949067\n",
      "11400 : Training: loss:  0.013508358\n",
      "Validation: Loss:  0.05250246  Accuracy:  0.7692308\n",
      "11401 : Training: loss:  0.017356263\n",
      "11402 : Training: loss:  0.012862312\n",
      "11403 : Training: loss:  0.020880139\n",
      "11404 : Training: loss:  0.028713576\n",
      "11405 : Training: loss:  0.029836586\n",
      "11406 : Training: loss:  0.043223288\n",
      "11407 : Training: loss:  0.021765461\n",
      "11408 : Training: loss:  0.019250097\n",
      "11409 : Training: loss:  0.012225361\n",
      "11410 : Training: loss:  0.016248016\n",
      "11411 : Training: loss:  0.018207725\n",
      "11412 : Training: loss:  0.01348129\n",
      "11413 : Training: loss:  0.029644338\n",
      "11414 : Training: loss:  0.020935547\n",
      "11415 : Training: loss:  0.017535822\n",
      "11416 : Training: loss:  0.009590375\n",
      "11417 : Training: loss:  0.031693257\n",
      "11418 : Training: loss:  0.03349758\n",
      "11419 : Training: loss:  0.026707923\n",
      "11420 : Training: loss:  0.018546592\n",
      "Validation: Loss:  0.052346356  Accuracy:  0.8076923\n",
      "11421 : Training: loss:  0.019814935\n",
      "11422 : Training: loss:  0.006860108\n",
      "11423 : Training: loss:  0.023858396\n",
      "11424 : Training: loss:  0.008170513\n",
      "11425 : Training: loss:  0.025723698\n",
      "11426 : Training: loss:  0.026640223\n",
      "11427 : Training: loss:  0.024144039\n",
      "11428 : Training: loss:  0.010423682\n",
      "11429 : Training: loss:  0.011062877\n",
      "11430 : Training: loss:  0.007990285\n",
      "11431 : Training: loss:  0.02234474\n",
      "11432 : Training: loss:  0.021922158\n",
      "11433 : Training: loss:  0.0060872645\n",
      "11434 : Training: loss:  0.016480733\n",
      "11435 : Training: loss:  0.035819255\n",
      "11436 : Training: loss:  0.00963506\n",
      "11437 : Training: loss:  0.030023\n",
      "11438 : Training: loss:  0.004483865\n",
      "11439 : Training: loss:  0.019348117\n",
      "11440 : Training: loss:  0.013627195\n",
      "Validation: Loss:  0.052694336  Accuracy:  0.8076923\n",
      "11441 : Training: loss:  0.039200287\n",
      "11442 : Training: loss:  0.008043591\n",
      "11443 : Training: loss:  0.030591903\n",
      "11444 : Training: loss:  0.015714392\n",
      "11445 : Training: loss:  0.0077436487\n",
      "11446 : Training: loss:  0.038406864\n",
      "11447 : Training: loss:  0.04933325\n",
      "11448 : Training: loss:  0.0052848374\n",
      "11449 : Training: loss:  0.015305936\n",
      "11450 : Training: loss:  0.02505649\n",
      "11451 : Training: loss:  0.00709104\n",
      "11452 : Training: loss:  0.017799892\n",
      "11453 : Training: loss:  0.018930007\n",
      "11454 : Training: loss:  0.013452035\n",
      "11455 : Training: loss:  0.05128565\n",
      "11456 : Training: loss:  0.025189314\n",
      "11457 : Training: loss:  0.011965877\n",
      "11458 : Training: loss:  0.014895958\n",
      "11459 : Training: loss:  0.029228263\n",
      "11460 : Training: loss:  0.059916705\n",
      "Validation: Loss:  0.052424897  Accuracy:  0.8076923\n",
      "11461 : Training: loss:  0.01549064\n",
      "11462 : Training: loss:  0.016458726\n",
      "11463 : Training: loss:  0.051701855\n",
      "11464 : Training: loss:  0.025938235\n",
      "11465 : Training: loss:  0.014352961\n",
      "11466 : Training: loss:  0.042386625\n",
      "11467 : Training: loss:  0.056682866\n",
      "11468 : Training: loss:  0.030673264\n",
      "11469 : Training: loss:  0.044473913\n",
      "11470 : Training: loss:  0.012787859\n",
      "11471 : Training: loss:  0.045463894\n",
      "11472 : Training: loss:  0.0042458093\n",
      "11473 : Training: loss:  0.040362768\n",
      "11474 : Training: loss:  0.013261962\n",
      "11475 : Training: loss:  0.0092512425\n",
      "11476 : Training: loss:  0.027660474\n",
      "11477 : Training: loss:  0.027137805\n",
      "11478 : Training: loss:  0.025115203\n",
      "11479 : Training: loss:  0.0172842\n",
      "11480 : Training: loss:  0.014997553\n",
      "Validation: Loss:  0.051888432  Accuracy:  0.78846157\n",
      "11481 : Training: loss:  0.024655053\n",
      "11482 : Training: loss:  0.016926944\n",
      "11483 : Training: loss:  0.0021082922\n",
      "11484 : Training: loss:  0.03449212\n",
      "11485 : Training: loss:  0.0055089723\n",
      "11486 : Training: loss:  0.0182595\n",
      "11487 : Training: loss:  0.029266408\n",
      "11488 : Training: loss:  0.021618439\n",
      "11489 : Training: loss:  0.008795126\n",
      "11490 : Training: loss:  0.015471338\n",
      "11491 : Training: loss:  0.046411876\n",
      "11492 : Training: loss:  0.014380797\n",
      "11493 : Training: loss:  0.04047173\n",
      "11494 : Training: loss:  0.016462563\n",
      "11495 : Training: loss:  0.020533947\n",
      "11496 : Training: loss:  0.040288635\n",
      "11497 : Training: loss:  0.0037636887\n",
      "11498 : Training: loss:  0.0047499444\n",
      "11499 : Training: loss:  0.0096637625\n",
      "11500 : Training: loss:  0.02924317\n",
      "Validation: Loss:  0.052176934  Accuracy:  0.78846157\n",
      "11501 : Training: loss:  0.0031630849\n",
      "11502 : Training: loss:  0.019196952\n",
      "11503 : Training: loss:  0.006656569\n",
      "11504 : Training: loss:  0.010834425\n",
      "11505 : Training: loss:  0.02012425\n",
      "11506 : Training: loss:  0.011670603\n",
      "11507 : Training: loss:  0.030400708\n",
      "11508 : Training: loss:  0.0075525404\n",
      "11509 : Training: loss:  0.033732023\n",
      "11510 : Training: loss:  0.015270632\n",
      "11511 : Training: loss:  0.037487958\n",
      "11512 : Training: loss:  0.033285283\n",
      "11513 : Training: loss:  0.0055959974\n",
      "11514 : Training: loss:  0.019256053\n",
      "11515 : Training: loss:  0.0111537855\n",
      "11516 : Training: loss:  0.011853202\n",
      "11517 : Training: loss:  0.03359724\n",
      "11518 : Training: loss:  0.012540686\n",
      "11519 : Training: loss:  0.02644643\n",
      "11520 : Training: loss:  0.01877663\n",
      "Validation: Loss:  0.05219683  Accuracy:  0.78846157\n",
      "11521 : Training: loss:  0.02005249\n",
      "11522 : Training: loss:  0.007052675\n",
      "11523 : Training: loss:  0.010573476\n",
      "11524 : Training: loss:  0.011452582\n",
      "11525 : Training: loss:  0.023033747\n",
      "11526 : Training: loss:  0.036928073\n",
      "11527 : Training: loss:  0.04069152\n",
      "11528 : Training: loss:  0.019694068\n",
      "11529 : Training: loss:  0.052847918\n",
      "11530 : Training: loss:  0.005897508\n",
      "11531 : Training: loss:  0.01721073\n",
      "11532 : Training: loss:  0.018125035\n",
      "11533 : Training: loss:  0.008799653\n",
      "11534 : Training: loss:  0.055098277\n",
      "11535 : Training: loss:  0.023153054\n",
      "11536 : Training: loss:  0.008932542\n",
      "11537 : Training: loss:  0.00535798\n",
      "11538 : Training: loss:  0.040739056\n",
      "11539 : Training: loss:  0.01300581\n",
      "11540 : Training: loss:  0.00689024\n",
      "Validation: Loss:  0.05190199  Accuracy:  0.8076923\n",
      "11541 : Training: loss:  0.0067904647\n",
      "11542 : Training: loss:  0.029183313\n",
      "11543 : Training: loss:  0.018788718\n",
      "11544 : Training: loss:  0.025472082\n",
      "11545 : Training: loss:  0.030101385\n",
      "11546 : Training: loss:  0.018646287\n",
      "11547 : Training: loss:  0.041273624\n",
      "11548 : Training: loss:  0.0141775375\n",
      "11549 : Training: loss:  0.033655263\n",
      "11550 : Training: loss:  0.031704664\n",
      "11551 : Training: loss:  0.022392105\n",
      "11552 : Training: loss:  0.03965564\n",
      "11553 : Training: loss:  0.02412981\n",
      "11554 : Training: loss:  0.034011967\n",
      "11555 : Training: loss:  0.029444369\n",
      "11556 : Training: loss:  0.015092301\n",
      "11557 : Training: loss:  0.014976225\n",
      "11558 : Training: loss:  0.021080948\n",
      "11559 : Training: loss:  0.014333777\n",
      "11560 : Training: loss:  0.03270184\n",
      "Validation: Loss:  0.05138308  Accuracy:  0.7692308\n",
      "11561 : Training: loss:  0.022114279\n",
      "11562 : Training: loss:  0.023320548\n",
      "11563 : Training: loss:  0.01629338\n",
      "11564 : Training: loss:  0.0037429999\n",
      "11565 : Training: loss:  0.03277834\n",
      "11566 : Training: loss:  0.02244608\n",
      "11567 : Training: loss:  0.009726582\n",
      "11568 : Training: loss:  0.02598048\n",
      "11569 : Training: loss:  0.016652908\n",
      "11570 : Training: loss:  0.0064660604\n",
      "11571 : Training: loss:  0.014683069\n",
      "11572 : Training: loss:  0.017800214\n",
      "11573 : Training: loss:  0.020484988\n",
      "11574 : Training: loss:  0.017067397\n",
      "11575 : Training: loss:  0.01639179\n",
      "11576 : Training: loss:  0.026592707\n",
      "11577 : Training: loss:  0.015079645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11578 : Training: loss:  0.026163314\n",
      "11579 : Training: loss:  0.02990485\n",
      "11580 : Training: loss:  0.030704223\n",
      "Validation: Loss:  0.051375028  Accuracy:  0.75\n",
      "11581 : Training: loss:  0.019543258\n",
      "11582 : Training: loss:  0.03777456\n",
      "11583 : Training: loss:  0.029756201\n",
      "11584 : Training: loss:  0.011082359\n",
      "11585 : Training: loss:  0.021794591\n",
      "11586 : Training: loss:  0.05751558\n",
      "11587 : Training: loss:  0.023745032\n",
      "11588 : Training: loss:  0.029306622\n",
      "11589 : Training: loss:  0.017463224\n",
      "11590 : Training: loss:  0.03287536\n",
      "11591 : Training: loss:  0.03208247\n",
      "11592 : Training: loss:  0.0103540085\n",
      "11593 : Training: loss:  0.014858192\n",
      "11594 : Training: loss:  0.016352715\n",
      "11595 : Training: loss:  0.015011812\n",
      "11596 : Training: loss:  0.010914676\n",
      "11597 : Training: loss:  0.02052107\n",
      "11598 : Training: loss:  0.015550422\n",
      "11599 : Training: loss:  0.017002478\n",
      "11600 : Training: loss:  0.050709113\n",
      "Validation: Loss:  0.051427696  Accuracy:  0.7692308\n",
      "11601 : Training: loss:  0.018374978\n",
      "11602 : Training: loss:  0.0032917112\n",
      "11603 : Training: loss:  0.030020403\n",
      "11604 : Training: loss:  0.028393362\n",
      "11605 : Training: loss:  0.029237162\n",
      "11606 : Training: loss:  0.035144906\n",
      "11607 : Training: loss:  0.025310306\n",
      "11608 : Training: loss:  0.007982235\n",
      "11609 : Training: loss:  0.014196921\n",
      "11610 : Training: loss:  0.03207937\n",
      "11611 : Training: loss:  0.030094977\n",
      "11612 : Training: loss:  0.033006947\n",
      "11613 : Training: loss:  0.047326997\n",
      "11614 : Training: loss:  0.035607744\n",
      "11615 : Training: loss:  0.026964806\n",
      "11616 : Training: loss:  0.00966352\n",
      "11617 : Training: loss:  0.008824735\n",
      "11618 : Training: loss:  0.01944416\n",
      "11619 : Training: loss:  0.01457647\n",
      "11620 : Training: loss:  0.020428251\n",
      "Validation: Loss:  0.05136405  Accuracy:  0.75\n",
      "11621 : Training: loss:  0.022646252\n",
      "11622 : Training: loss:  0.051534846\n",
      "11623 : Training: loss:  0.011074912\n",
      "11624 : Training: loss:  0.017371997\n",
      "11625 : Training: loss:  0.04896974\n",
      "11626 : Training: loss:  0.02262021\n",
      "11627 : Training: loss:  0.0030470248\n",
      "11628 : Training: loss:  0.033688396\n",
      "11629 : Training: loss:  0.012748592\n",
      "11630 : Training: loss:  0.024230314\n",
      "11631 : Training: loss:  0.023020647\n",
      "11632 : Training: loss:  0.0049102185\n",
      "11633 : Training: loss:  0.0096865315\n",
      "11634 : Training: loss:  0.028994907\n",
      "11635 : Training: loss:  0.0031736915\n",
      "11636 : Training: loss:  0.02502065\n",
      "11637 : Training: loss:  0.01885272\n",
      "11638 : Training: loss:  0.034578346\n",
      "11639 : Training: loss:  0.014105947\n",
      "11640 : Training: loss:  0.016730295\n",
      "Validation: Loss:  0.05151412  Accuracy:  0.75\n",
      "11641 : Training: loss:  0.03115352\n",
      "11642 : Training: loss:  0.018474024\n",
      "11643 : Training: loss:  0.023287985\n",
      "11644 : Training: loss:  0.0049773157\n",
      "11645 : Training: loss:  0.03548463\n",
      "11646 : Training: loss:  0.03352913\n",
      "11647 : Training: loss:  0.037248842\n",
      "11648 : Training: loss:  0.039002173\n",
      "11649 : Training: loss:  0.00765001\n",
      "11650 : Training: loss:  0.021920562\n",
      "11651 : Training: loss:  0.028248204\n",
      "11652 : Training: loss:  0.020810338\n",
      "11653 : Training: loss:  0.02927213\n",
      "11654 : Training: loss:  0.028098656\n",
      "11655 : Training: loss:  0.033832364\n",
      "11656 : Training: loss:  0.030828228\n",
      "11657 : Training: loss:  0.0119031025\n",
      "11658 : Training: loss:  0.011822003\n",
      "11659 : Training: loss:  0.021245984\n",
      "11660 : Training: loss:  0.020329637\n",
      "Validation: Loss:  0.052003946  Accuracy:  0.7692308\n",
      "11661 : Training: loss:  0.01069142\n",
      "11662 : Training: loss:  0.025253834\n",
      "11663 : Training: loss:  0.01127172\n",
      "11664 : Training: loss:  0.036772564\n",
      "11665 : Training: loss:  0.058448855\n",
      "11666 : Training: loss:  0.023644138\n",
      "11667 : Training: loss:  0.011026983\n",
      "11668 : Training: loss:  0.014746535\n",
      "11669 : Training: loss:  0.039239496\n",
      "11670 : Training: loss:  0.0063505\n",
      "11671 : Training: loss:  0.037946817\n",
      "11672 : Training: loss:  0.02186862\n",
      "11673 : Training: loss:  0.0184867\n",
      "11674 : Training: loss:  0.019215604\n",
      "11675 : Training: loss:  0.018166142\n",
      "11676 : Training: loss:  0.03223033\n",
      "11677 : Training: loss:  0.025788972\n",
      "11678 : Training: loss:  0.039910395\n",
      "11679 : Training: loss:  0.02490808\n",
      "11680 : Training: loss:  0.008602283\n",
      "Validation: Loss:  0.052032992  Accuracy:  0.7692308\n",
      "11681 : Training: loss:  0.040538497\n",
      "11682 : Training: loss:  0.027167832\n",
      "11683 : Training: loss:  0.013969517\n",
      "11684 : Training: loss:  0.039673325\n",
      "11685 : Training: loss:  0.046254873\n",
      "11686 : Training: loss:  0.0366659\n",
      "11687 : Training: loss:  0.003925966\n",
      "11688 : Training: loss:  0.019919166\n",
      "11689 : Training: loss:  0.018223379\n",
      "11690 : Training: loss:  0.03996672\n",
      "11691 : Training: loss:  0.01366099\n",
      "11692 : Training: loss:  0.011472149\n",
      "11693 : Training: loss:  0.048034597\n",
      "11694 : Training: loss:  0.025522413\n",
      "11695 : Training: loss:  0.011268022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eb8110e16071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mremove_directory_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries_folder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-fd99206fb047>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             _, l = session.run([train_op, loss], {visual: inputs_visual,\n\u001b[1;32m     32\u001b[0m            \u001b[0;31m#                                textual: inputs_textual,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                            target: correct_classes})\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\": Training: loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0msumm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mloss_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "remove_directory_content(summaries_folder_name)\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
